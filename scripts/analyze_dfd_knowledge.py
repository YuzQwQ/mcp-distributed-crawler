#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DFDçŸ¥è¯†åº“åˆ†æå·¥å…·

å¿«é€Ÿåˆ†æå½“å‰DFDçŸ¥è¯†åº“çš„å†…å®¹å’Œè´¨é‡çŠ¶å†µ
"""

import json
import os
from pathlib import Path
from collections import defaultdict, Counter

def analyze_dfd_knowledge_base():
    """åˆ†æDFDçŸ¥è¯†åº“"""
    knowledge_base_dir = Path("shared_data/knowledge_base")
    
    if not knowledge_base_dir.exists():
        print(f"é”™è¯¯ï¼šçŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨ - {knowledge_base_dir}")
        return
    
    print("DFDçŸ¥è¯†åº“åˆ†ææŠ¥å‘Š")
    print("=" * 50)
    
    # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
    dfd_files = list(knowledge_base_dir.glob("*DFD*.json"))
    summary_files = [f for f in dfd_files if f.name.endswith('_summary.json')]
    content_files = [f for f in dfd_files if not f.name.endswith('_summary.json')]
    
    print(f"\nğŸ“Š æ–‡ä»¶ç»Ÿè®¡ï¼š")
    print(f"  æ€»DFDç›¸å…³æ–‡ä»¶ï¼š{len(dfd_files)}")
    print(f"  å†…å®¹æ–‡ä»¶ï¼š{len(content_files)}")
    print(f"  æ±‡æ€»æ–‡ä»¶ï¼š{len(summary_files)}")
    
    # åˆ†ææ–‡ä»¶ç±»å‹åˆ†å¸ƒ
    file_types = defaultdict(int)
    for file in content_files:
        if '_concepts.json' in file.name:
            file_types['concepts'] += 1
        elif '_rules.json' in file.name:
            file_types['rules'] += 1
        elif '_cases.json' in file.name:
            file_types['cases'] += 1
        elif '_patterns.json' in file.name:
            file_types['patterns'] += 1
        elif '_nlp_mappings.json' in file.name:
            file_types['nlp_mappings'] += 1
    
    print(f"\nğŸ“‹ çŸ¥è¯†ç±»å‹åˆ†å¸ƒï¼š")
    for file_type, count in file_types.items():
        print(f"  {file_type}: {count} ä¸ªæ–‡ä»¶")
    
    # åˆ†æçŸ¥è¯†æ¥æº
    sources = set()
    total_concepts = 0
    total_rules = 0
    total_cases = 0
    
    concept_types = Counter()
    rule_categories = Counter()
    case_types = Counter()
    
    print(f"\nğŸ” å†…å®¹åˆ†æï¼š")
    
    # åˆ†ææ¦‚å¿µæ–‡ä»¶
    concept_files = [f for f in content_files if '_concepts.json' in f.name]
    for file in concept_files:
        try:
            with open(file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # æå–æ¥æºä¿¡æ¯
            metadata = data.get('metadata', {})
            source_title = metadata.get('source_title', 'æœªçŸ¥æ¥æº')
            sources.add(source_title)
            
            # ç»Ÿè®¡æ¦‚å¿µ
            concepts = data.get('data', [])
            total_concepts += len(concepts)
            
            for concept in concepts:
                concept_type = concept.get('type', 'æœªåˆ†ç±»')
                concept_types[concept_type] += 1
                
        except Exception as e:
            print(f"  âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {file.name}: {e}")
    
    # åˆ†æè§„åˆ™æ–‡ä»¶
    rule_files = [f for f in content_files if '_rules.json' in f.name]
    for file in rule_files:
        try:
            with open(file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            rules = data.get('data', [])
            total_rules += len(rules)
            
            for rule in rules:
                category = rule.get('category', 'æœªåˆ†ç±»')
                rule_categories[category] += 1
                
        except Exception as e:
            print(f"  âš ï¸ è¯»å–è§„åˆ™æ–‡ä»¶å¤±è´¥ {file.name}: {e}")
    
    # åˆ†ææ¡ˆä¾‹æ–‡ä»¶
    case_files = [f for f in content_files if '_cases.json' in f.name]
    for file in case_files:
        try:
            with open(file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            cases = data.get('data', [])
            total_cases += len(cases)
            
            for case in cases:
                case_type = case.get('type', 'æœªåˆ†ç±»')
                case_types[case_type] += 1
                
        except Exception as e:
            print(f"  âš ï¸ è¯»å–æ¡ˆä¾‹æ–‡ä»¶å¤±è´¥ {file.name}: {e}")
    
    print(f"  æ€»æ¦‚å¿µæ•°ï¼š{total_concepts}")
    print(f"  æ€»è§„åˆ™æ•°ï¼š{total_rules}")
    print(f"  æ€»æ¡ˆä¾‹æ•°ï¼š{total_cases}")
    
    print(f"\nğŸ“š çŸ¥è¯†æ¥æº ({len(sources)} ä¸ª)ï¼š")
    for i, source in enumerate(sorted(sources), 1):
        print(f"  {i}. {source}")
    
    print(f"\nğŸ·ï¸ æ¦‚å¿µç±»å‹åˆ†å¸ƒï¼š")
    for concept_type, count in concept_types.most_common():
        print(f"  {concept_type}: {count}")
    
    print(f"\nğŸ“ è§„åˆ™ç±»åˆ«åˆ†å¸ƒï¼š")
    for category, count in rule_categories.most_common():
        print(f"  {category}: {count}")
    
    print(f"\nğŸ“ æ¡ˆä¾‹ç±»å‹åˆ†å¸ƒï¼š")
    for case_type, count in case_types.most_common():
        print(f"  {case_type}: {count}")
    
    # è´¨é‡è¯„ä¼°
    print(f"\nâ­ è´¨é‡è¯„ä¼°ï¼š")
    
    # æ£€æŸ¥æ ¸å¿ƒæ¦‚å¿µè¦†ç›–åº¦
    required_concepts = {'process', 'entity', 'data_store', 'data_flow'}
    found_concepts = set(concept_types.keys())
    concept_coverage = len(found_concepts & required_concepts) / len(required_concepts) * 100
    
    print(f"  æ ¸å¿ƒæ¦‚å¿µè¦†ç›–åº¦ï¼š{concept_coverage:.1f}%")
    missing_concepts = required_concepts - found_concepts
    if missing_concepts:
        print(f"  ç¼ºå¤±æ ¸å¿ƒæ¦‚å¿µï¼š{', '.join(missing_concepts)}")
    
    # æ£€æŸ¥è§„åˆ™åˆ†ç±»è¦†ç›–åº¦
    expected_rule_categories = {'hierarchy', 'connection', 'naming'}
    found_categories = set(rule_categories.keys())
    rule_coverage = len(found_categories & expected_rule_categories) / len(expected_rule_categories) * 100
    
    print(f"  è§„åˆ™åˆ†ç±»è¦†ç›–åº¦ï¼š{rule_coverage:.1f}%")
    missing_categories = expected_rule_categories - found_categories
    if missing_categories:
        print(f"  ç¼ºå¤±è§„åˆ™åˆ†ç±»ï¼š{', '.join(missing_categories)}")
    
    # æ•°æ®è´¨é‡æŒ‡æ ‡
    avg_concepts_per_source = total_concepts / len(sources) if sources else 0
    avg_rules_per_source = total_rules / len(rule_files) if rule_files else 0
    
    print(f"  å¹³å‡æ¯æ¥æºæ¦‚å¿µæ•°ï¼š{avg_concepts_per_source:.1f}")
    print(f"  å¹³å‡æ¯æ–‡ä»¶è§„åˆ™æ•°ï¼š{avg_rules_per_source:.1f}")
    
    # ç”Ÿæˆæ”¹è¿›å»ºè®®
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®ï¼š")
    
    if concept_coverage < 100:
        print(f"  1. è¡¥å……ç¼ºå¤±çš„æ ¸å¿ƒæ¦‚å¿µï¼š{', '.join(missing_concepts)}")
    
    if rule_coverage < 100:
        print(f"  2. å®Œå–„è§„åˆ™åˆ†ç±»ï¼š{', '.join(missing_categories)}")
    
    if total_cases < 10:
        print(f"  3. å¢åŠ æ›´å¤šå®è·µæ¡ˆä¾‹ï¼ˆå½“å‰ä»…{total_cases}ä¸ªï¼‰")
    
    if len(sources) < 5:
        print(f"  4. æ‰©å±•çŸ¥è¯†æ¥æºï¼ˆå½“å‰ä»…{len(sources)}ä¸ªæ¥æºï¼‰")
    
    if avg_concepts_per_source < 5:
        print(f"  5. æ·±åŒ–æ¯ä¸ªæ¥æºçš„æ¦‚å¿µæå–")
    
    print(f"\nğŸ“‹ æ¨èä½¿ç”¨çš„æç¤ºè¯ç±»å‹ï¼š")
    
    if concept_coverage < 80:
        print(f"  â€¢ æ¦‚å¿µåº“æ‰©å±•æç¤ºè¯ - è¡¥å……æ ¸å¿ƒæ¦‚å¿µ")
    
    if rule_coverage < 80:
        print(f"  â€¢ è§„åˆ™åº“å®Œå–„æç¤ºè¯ - å®Œå–„è§„åˆ™ä½“ç³»")
    
    if total_cases < 20:
        print(f"  â€¢ æ¡ˆä¾‹åº“ä¸°å¯Œæç¤ºè¯ - å¢åŠ å®è·µæ¡ˆä¾‹")
    
    if len(sources) < 10:
        print(f"  â€¢ é¢†åŸŸç‰¹å®šçŸ¥è¯†æ‰©å±•æç¤ºè¯ - æ‰©å±•åº”ç”¨é¢†åŸŸ")
    
    print(f"\nğŸ“– è¯¦ç»†æç¤ºè¯è¯·æŸ¥çœ‹ï¼šdfd_knowledge_enhancement_prompts.md")
    
    return {
        'total_files': len(content_files),
        'total_concepts': total_concepts,
        'total_rules': total_rules,
        'total_cases': total_cases,
        'sources_count': len(sources),
        'concept_coverage': concept_coverage,
        'rule_coverage': rule_coverage
    }

if __name__ == "__main__":
    analyze_dfd_knowledge_base()