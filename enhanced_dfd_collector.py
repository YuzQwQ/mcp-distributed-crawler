#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„DFDçŸ¥è¯†æ”¶é›†ç³»ç»Ÿ
åŸºäºæµ‹è¯•ç»“æœæ”¹è¿›çš„çŸ¥è¯†æ”¶é›†ã€åˆ†æå’Œå¯è§†åŒ–åŠŸèƒ½
"""

import asyncio
import json
import os
import re
import aiohttp
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedDFDKnowledgeCollector:
    """å¢å¼ºç‰ˆDFDçŸ¥è¯†æ”¶é›†å™¨"""
    
    def __init__(self):
        self.collected_data = []
        self.session = None
        self.knowledge_base = {
            'concepts': {},
            'rules': {},
            'cases': {},
            'patterns': {},
            'mappings': {}
        }
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¸…ç†"""
        if self.session:
            await self.session.close()
    
    async def collect_from_multiple_sources(self, queries: List[str]) -> Dict[str, Any]:
        """ä»å¤šä¸ªæ¥æºæ”¶é›†DFDçŸ¥è¯†"""
        logger.info(f"å¼€å§‹ä»å¤šä¸ªæ¥æºæ”¶é›† {len(queries)} ä¸ªä¸»é¢˜çš„DFDçŸ¥è¯†")
        
        all_knowledge = {
            'metadata': {
                'collection_time': datetime.now().isoformat(),
                'queries': queries,
                'sources': ['academic', 'industry', 'practical']
            },
            'knowledge': {}
        }
        
        for query in queries:
            logger.info(f"æ”¶é›†ä¸»é¢˜: {query}")
            knowledge = await self._collect_comprehensive_knowledge(query)
            all_knowledge['knowledge'][query] = knowledge
            
        return all_knowledge
    
    async def _collect_comprehensive_knowledge(self, query: str) -> Dict[str, Any]:
        """æ”¶é›†æŒ‡å®šä¸»é¢˜çš„å…¨é¢çŸ¥è¯†"""
        
        # æ¨¡æ‹Ÿä»ä¸åŒæ¥æºæ”¶é›†çŸ¥è¯†
        knowledge = {
            'concepts': await self._collect_concepts(query),
            'rules': await self._collect_rules(query),
            'cases': await self._collect_cases(query),
            'patterns': await self._collect_patterns(query),
            'naming_conventions': await self._collect_naming_conventions(query),
            'tools': await self._collect_tools(query),
            'quality_guidelines': await self._collect_quality_guidelines(query)
        }
        
        return knowledge
    
    async def _collect_concepts(self, query: str) -> List[Dict[str, Any]]:
        """æ”¶é›†DFDæ¦‚å¿µ"""
        concepts = [
            {
                'name': 'process',
                'definition': 'å¯¹è¾“å…¥æ•°æ®è¿›è¡Œå¤„ç†å¹¶äº§ç”Ÿè¾“å‡ºçš„åŠŸèƒ½å•å…ƒ',
                'category': 'core',
                'level': 'basic',
                'examples': ['å¤„ç†è®¢å•', 'éªŒè¯ç”¨æˆ·', 'ç”ŸæˆæŠ¥å‘Š'],
                'symbol': 'åœ†å½¢æˆ–åœ†è§’çŸ©å½¢',
                'rules': ['å¿…é¡»æœ‰è¾“å…¥å’Œè¾“å‡º', 'ä½¿ç”¨åŠ¨è¯+åè¯å‘½å', 'ä¿æŒåŸå­æ€§']
            },
            {
                'name': 'external_entity',
                'definition': 'ç³»ç»Ÿè¾¹ç•Œä¹‹å¤–ä¸ç³»ç»Ÿäº¤äº’çš„äººã€ç»„ç»‡æˆ–å…¶ä»–ç³»ç»Ÿ',
                'category': 'core',
                'level': 'basic',
                'examples': ['å®¢æˆ·', 'ä¾›åº”å•†', 'é“¶è¡Œç³»ç»Ÿ', 'æ”¿åºœéƒ¨é—¨'],
                'symbol': 'çŸ©å½¢',
                'rules': ['å¿…é¡»æ˜¯ç³»ç»Ÿå¤–éƒ¨', 'æä¾›æˆ–æ¥æ”¶æ•°æ®', 'ä½¿ç”¨åè¯å‘½å']
            },
            {
                'name': 'data_store',
                'definition': 'ç³»ç»Ÿå†…éƒ¨ç”¨äºå­˜å‚¨æ•°æ®çš„ä»“åº“',
                'category': 'core',
                'level': 'basic',
                'examples': ['å®¢æˆ·æ•°æ®åº“', 'è®¢å•æ–‡ä»¶', 'äº§å“ç›®å½•'],
                'symbol': 'åŒæ¨ªçº¿æˆ–å¼€å£çŸ©å½¢',
                'rules': ['å¿…é¡»ç¼–å·', 'ä½¿ç”¨åè¯å‘½å', 'æ ‡æ˜è®¿é—®æ–¹å¼']
            },
            {
                'name': 'data_flow',
                'definition': 'æ•°æ®åœ¨ç³»ç»Ÿç»„ä»¶ä¹‹é—´çš„ç§»åŠ¨è·¯å¾„',
                'category': 'core',
                'level': 'basic',
                'examples': ['å®¢æˆ·ä¿¡æ¯', 'è®¢å•è¯¦æƒ…', 'æ”¯ä»˜ç¡®è®¤'],
                'symbol': 'å¸¦ç®­å¤´çš„çº¿æ¡',
                'rules': ['å¿…é¡»æ ‡æ³¨æ•°æ®å†…å®¹', 'ç®­å¤´æ–¹å‘æ­£ç¡®', 'é¿å…æ•°æ®åˆ†å‰']
            },
            {
                'name': 'context_diagram',
                'definition': 'ç³»ç»Ÿçš„0å±‚DFDï¼Œå±•ç¤ºç³»ç»Ÿä¸å¤–éƒ¨å®ä½“çš„æ•´ä½“å…³ç³»',
                'category': 'hierarchical',
                'level': 'advanced',
                'examples': ['æ•´ä¸ªç”µå•†ç³»ç»Ÿä¸Šä¸‹æ–‡'],
                'rules': ['åªåŒ…å«ä¸€ä¸ªå¤„ç†', 'æ˜¾ç¤ºæ‰€æœ‰å¤–éƒ¨å®ä½“', 'ä¸æ˜¾ç¤ºæ•°æ®å­˜å‚¨']
            },
            {
                'name': 'level_1_dfd',
                'definition': 'ç³»ç»Ÿçš„ä¸»è¦åŠŸèƒ½åˆ†è§£ï¼Œå±•ç¤ºç³»ç»Ÿå†…éƒ¨çš„ä¸»è¦å¤„ç†',
                'category': 'hierarchical',
                'level': 'advanced',
                'examples': ['ç”µå•†ç³»ç»Ÿçš„ä¸€çº§åˆ†è§£'],
                'rules': ['ä¿æŒä¸ä¸Šä¸‹æ–‡å›¾å¹³è¡¡', 'ç¼–å·ä»1.0å¼€å§‹', 'åŒ…å«ä¸»è¦æ•°æ®å­˜å‚¨']
            }
        ]
        
        return concepts
    
    async def _collect_rules(self, query: str) -> List[Dict[str, Any]]:
        """æ”¶é›†DFDè®¾è®¡è§„åˆ™"""
        rules = [
            {
                'type': 'naming',
                'category': 'process_naming',
                'rule': 'ä½¿ç”¨åŠ¨è¯+åè¯æ ¼å¼',
                'description': 'å¤„ç†åç§°åº”è¯¥æ¸…æ™°åœ°è¡¨è¾¾å…¶åŠŸèƒ½',
                'examples': {
                    'good': ['å¤„ç†è®¢å•', 'éªŒè¯ç”¨æˆ·', 'ç”ŸæˆæŠ¥å‘Š', 'è®¡ç®—æ€»ä»·'],
                    'bad': ['è®¢å•', 'ç”¨æˆ·', 'æŠ¥å‘Š', 'è®¡ç®—']
                },
                'severity': 'high'
            },
            {
                'type': 'naming',
                'category': 'entity_naming',
                'rule': 'ä½¿ç”¨åè¯è¡¨ç¤ºè§’è‰²æˆ–ç³»ç»Ÿ',
                'description': 'å¤–éƒ¨å®ä½“åç§°åº”è¯¥åæ˜ å…¶èº«ä»½æˆ–åŠŸèƒ½',
                'examples': {
                    'good': ['å®¢æˆ·', 'ä¾›åº”å•†', 'é“¶è¡Œç³»ç»Ÿ', 'ç®¡ç†å‘˜'],
                    'bad': ['äºº', 'ç³»ç»Ÿ', 'å¤–éƒ¨', 'ç”¨æˆ·1']
                },
                'severity': 'high'
            },
            {
                'type': 'naming',
                'category': 'data_store_naming',
                'rule': 'ä½¿ç”¨D+ç¼–å·+åç§°æ ¼å¼',
                'description': 'æ•°æ®å­˜å‚¨åº”è¯¥æœ‰å”¯ä¸€æ ‡è¯†å’Œæè¿°æ€§åç§°',
                'examples': {
                    'good': ['D1 å®¢æˆ·ä¿¡æ¯', 'D2 è®¢å•æ•°æ®', 'D3 äº§å“åº“å­˜'],
                    'bad': ['æ•°æ®åº“', 'æ–‡ä»¶', 'å­˜å‚¨1', 'æ•°æ®']
                },
                'severity': 'medium'
            },
            {
                'type': 'balancing',
                'category': 'hierarchy',
                'rule': 'çˆ¶å­å›¾è¾“å…¥è¾“å‡ºå¿…é¡»å¹³è¡¡',
                'description': 'å­å›¾çš„è¾“å…¥è¾“å‡ºæ•°æ®æµå¿…é¡»ä¸çˆ¶å›¾ä¸­çš„å¯¹åº”å¤„ç†ä¿æŒä¸€è‡´',
                'examples': {
                    'good': 'çˆ¶å›¾æœ‰2è¾“å…¥3è¾“å‡ºï¼Œå­å›¾ä¹Ÿå¿…é¡»æœ‰2è¾“å…¥3è¾“å‡º',
                    'bad': 'çˆ¶å›¾æœ‰3è¾“å…¥ï¼Œå­å›¾åªæœ‰2è¾“å…¥'
                },
                'severity': 'critical'
            },
            {
                'type': 'completeness',
                'category': 'data_flow',
                'rule': 'æ¯ä¸ªå¤„ç†å¿…é¡»æœ‰è¾“å…¥å’Œè¾“å‡º',
                'description': 'ä»»ä½•å¤„ç†éƒ½ä¸èƒ½åªæœ‰è¾“å…¥æˆ–åªæœ‰è¾“å‡º',
                'examples': {
                    'good': 'å¤„ç†è®¢å•æœ‰å®¢æˆ·è®¢å•è¾“å…¥å’Œç¡®è®¤ä¿¡æ¯è¾“å‡º',
                    'bad': 'å¤„ç†è®¢å•åªæœ‰è¾“å…¥æ²¡æœ‰è¾“å‡º'
                },
                'severity': 'critical'
            }
        ]
        
        return rules
    
    async def _collect_cases(self, query: str) -> List[Dict[str, Any]]:
        """æ”¶é›†å®é™…æ¡ˆä¾‹"""
        cases = [
            {
                'type': 'best_practice',
                'title': 'ç”µå•†ç³»ç»Ÿè®¢å•å¤„ç†DFD',
                'description': 'å®Œæ•´çš„ç”µå•†è®¢å•å¤„ç†æµç¨‹ï¼Œä»0å±‚åˆ°2å±‚',
                'industry': 'ç”µå•†',
                'layers': [
                    {
                        'level': 0,
                        'description': 'ç³»ç»Ÿä¸å¤–éƒ¨å®ä½“çš„æ•´ä½“å…³ç³»',
                        'entities': ['å®¢æˆ·', 'æ”¯ä»˜ç½‘å…³', 'ç‰©æµç³»ç»Ÿ', 'åº“å­˜ç³»ç»Ÿ'],
                        'processes': ['ç”µå•†è®¢å•ç³»ç»Ÿ']
                    },
                    {
                        'level': 1,
                        'description': 'ç³»ç»Ÿä¸»è¦åŠŸèƒ½æ¨¡å—',
                        'processes': ['1.0 æ¥æ”¶è®¢å•', '2.0 å¤„ç†æ”¯ä»˜', '3.0 ç®¡ç†åº“å­˜', '4.0 å®‰æ’å‘è´§'],
                        'data_stores': ['D1 è®¢å•æ•°æ®', 'D2 å®¢æˆ·ä¿¡æ¯', 'D3 äº§å“åº“å­˜', 'D4 æ”¯ä»˜è®°å½•']
                    },
                    {
                        'level': 2,
                        'description': 'è®¢å•å¤„ç†çš„è¯¦ç»†åˆ†è§£',
                        'processes': ['1.1 éªŒè¯è®¢å•', '1.2 è®¡ç®—ä»·æ ¼', '1.3 ç¡®è®¤è®¢å•', '1.4 å‘é€ç¡®è®¤']
                    }
                ],
                'key_features': [
                    'æ¸…æ™°çš„å±‚æ¬¡åˆ†è§£',
                    'å®Œæ•´çš„ä¸šåŠ¡æµç¨‹',
                    'åˆç†çš„æ¨¡å—åˆ’åˆ†',
                    'æ ‡å‡†åŒ–çš„å‘½å'
                ]
            },
            {
                'type': 'error_case',
                'title': 'å‘½åé”™è¯¯æ¡ˆä¾‹åˆ†æ',
                'description': 'å±•ç¤ºå¸¸è§çš„DFDå‘½åé”™è¯¯åŠå…¶ä¿®æ­£æ–¹æ³•',
                'errors': [
                    {
                        'error_type': 'æ¨¡ç³Šå‘½å',
                        'original': 'å¤„ç†1',
                        'corrected': 'å¤„ç†å®¢æˆ·è®¢å•',
                        'explanation': 'ä½¿ç”¨åŠ¨è¯+åè¯æ ¼å¼æ˜ç¡®åŠŸèƒ½'
                    },
                    {
                        'error_type': 'æŠ€æœ¯æœ¯è¯­',
                        'original': 'DBæ“ä½œ',
                        'corrected': 'æ›´æ–°å®¢æˆ·ä¿¡æ¯',
                        'explanation': 'ä½¿ç”¨ä¸šåŠ¡æœ¯è¯­è€ŒéæŠ€æœ¯æœ¯è¯­'
                    }
                ]
            }
        ]
        
        return cases
    
    async def _collect_patterns(self, query: str) -> List[Dict[str, Any]]:
        """æ”¶é›†è®¾è®¡æ¨¡å¼"""
        patterns = [
            {
                'name': 'data_transformation',
                'description': 'å°†è¾“å…¥æ•°æ®è½¬æ¢ä¸ºæ‰€éœ€è¾“å‡ºæ ¼å¼çš„æ ‡å‡†æ¨¡å¼',
                'structure': {
                    'input': 'åŸå§‹æ•°æ®',
                    'process': 'è½¬æ¢å¤„ç†',
                    'output': 'æ ‡å‡†æ ¼å¼æ•°æ®'
                },
                'examples': ['è®¢å•æ•°æ®è½¬æ¢', 'ç”¨æˆ·ä¿¡æ¯æ ‡å‡†åŒ–', 'æŠ¥å‘Šæ ¼å¼è½¬æ¢']
            },
            {
                'name': 'data_validation',
                'description': 'éªŒè¯è¾“å…¥æ•°æ®å®Œæ•´æ€§å’Œæ­£ç¡®æ€§çš„æ¨¡å¼',
                'structure': {
                    'input': 'å¾…éªŒè¯æ•°æ®',
                    'process': 'éªŒè¯è§„åˆ™æ£€æŸ¥',
                    'output': 'éªŒè¯ç»“æœæˆ–é”™è¯¯ä¿¡æ¯'
                },
                'examples': ['è®¢å•éªŒè¯', 'ç”¨æˆ·èº«ä»½éªŒè¯', 'æ•°æ®å®Œæ•´æ€§æ£€æŸ¥']
            },
            {
                'name': 'batch_processing',
                'description': 'æ‰¹é‡å¤„ç†å¤§é‡æ•°æ®çš„æ¨¡å¼',
                'structure': {
                    'input': 'æ‰¹é‡æ•°æ®é›†åˆ',
                    'process': 'æ‰¹å¤„ç†ä½œä¸š',
                    'output': 'å¤„ç†ç»“æœé›†åˆ'
                },
                'examples': ['æ‰¹é‡è®¢å•å¤„ç†', 'æ‰¹é‡æ•°æ®å¯¼å…¥', 'å®šæœŸæŠ¥å‘Šç”Ÿæˆ']
            }
        ]
        
        return patterns
    
    async def _collect_naming_conventions(self, query: str) -> Dict[str, Any]:
        """æ”¶é›†å‘½åè§„èŒƒ"""
        return {
            'processes': {
                'format': 'åŠ¨è¯ + åè¯',
                'examples': ['å¤„ç†è®¢å•', 'éªŒè¯ç”¨æˆ·', 'ç”ŸæˆæŠ¥å‘Š', 'è®¡ç®—æ€»ä»·'],
                'forbidden': ['ä½¿ç”¨ç¼©å†™', 'æŠ€æœ¯æœ¯è¯­', 'æ¨¡ç³Šè¯æ±‡', 'ç¼–å·å¼€å¤´']
            },
            'entities': {
                'format': 'åè¯ï¼ˆè§’è‰²æˆ–ç³»ç»Ÿï¼‰',
                'examples': ['å®¢æˆ·', 'ä¾›åº”å•†', 'é“¶è¡Œç³»ç»Ÿ', 'ç®¡ç†å‘˜', 'æ”¿åºœéƒ¨é—¨'],
                'forbidden': ['æŠ½è±¡è¯æ±‡', 'ä¸ªäººå§“å', 'æŠ€æœ¯æœ¯è¯­']
            },
            'data_stores': {
                'format': 'D + ç¼–å· + åç§°',
                'examples': ['D1 å®¢æˆ·ä¿¡æ¯', 'D2 è®¢å•æ•°æ®', 'D3 äº§å“åº“å­˜'],
                'forbidden': ['æ— ç¼–å·', 'æŠ€æœ¯å®ç°', 'æ¨¡ç³Šåç§°']
            },
            'data_flows': {
                'format': 'æ•°æ®å†…å®¹åç§°',
                'examples': ['å®¢æˆ·ä¿¡æ¯', 'è®¢å•è¯¦æƒ…', 'æ”¯ä»˜ç¡®è®¤', 'åº“å­˜çŠ¶æ€'],
                'forbidden': ['åŠ¨è¯', 'æŠ€æœ¯æœ¯è¯­', 'ç¼©å†™']
            }
        }
    
    async def _collect_tools(self, query: str) -> List[Dict[str, Any]]:
        """æ”¶é›†å·¥å…·ä¿¡æ¯"""
        return [
            {
                'name': 'Microsoft Visio',
                'type': 'professional',
                'features': ['æ ‡å‡†ç¬¦å·åº“', 'æ¨¡æ¿æ”¯æŒ', 'å›¢é˜Ÿåä½œ', 'å¯¼å‡ºå¤šç§æ ¼å¼'],
                'best_for': 'ä¼ä¸šçº§é¡¹ç›®',
                'complexity': 'medium'
            },
            {
                'name': 'Draw.io',
                'type': 'online',
                'features': ['å…è´¹ä½¿ç”¨', 'äº‘å­˜å‚¨', 'å®æ—¶åä½œ', 'å¤šå¹³å°æ”¯æŒ'],
                'best_for': 'ä¸ªäººå’Œå°å›¢é˜Ÿ',
                'complexity': 'low'
            },
            {
                'name': 'Lucidchart',
                'type': 'online',
                'features': ['æ™ºèƒ½è¿æ¥', 'æ¨¡æ¿ä¸°å¯Œ', 'ç‰ˆæœ¬æ§åˆ¶', 'é›†æˆå·¥å…·'],
                'best_for': 'æ•æ·å›¢é˜Ÿ',
                'complexity': 'low'
            },
            {
                'name': 'Enterprise Architect',
                'type': 'professional',
                'features': ['å®Œæ•´å»ºæ¨¡', 'ä»£ç ç”Ÿæˆ', 'æ•°æ®åº“è®¾è®¡', 'é¡¹ç›®ç®¡ç†'],
                'best_for': 'å¤§å‹é¡¹ç›®',
                'complexity': 'high'
            }
        ]
    
    async def _collect_quality_guidelines(self, query: str) -> Dict[str, Any]:
        """æ”¶é›†è´¨é‡ä¿è¯æŒ‡å—"""
        return {
            'checklist': {
                'completeness': [
                    'æ¯ä¸ªå¤„ç†éƒ½æœ‰è¾“å…¥å’Œè¾“å‡º',
                    'æ‰€æœ‰å¤–éƒ¨å®ä½“éƒ½å·²æ ‡è¯†',
                    'æ•°æ®å­˜å‚¨æœ‰é€‚å½“çš„ç¼–å·',
                    'æ•°æ®æµæœ‰æ˜ç¡®çš„æ ‡æ³¨'
                ],
                'consistency': [
                    'å‘½åè§„èŒƒä¸€è‡´',
                    'å±‚æ¬¡ç»“æ„å¹³è¡¡',
                    'æ•°æ®æµæ–¹å‘æ­£ç¡®',
                    'ç¬¦å·ä½¿ç”¨æ ‡å‡†'
                ],
                'readability': [
                    'å¸ƒå±€æ¸…æ™°åˆç†',
                    'é¿å…çº¿æ¡äº¤å‰',
                    'æ–‡å­—ç®€æ´æ˜äº†',
                    'å±‚æ¬¡ç»“æ„æ¸…æ™°'
                ]
            },
            'validation_rules': [
                'æ£€æŸ¥å¤„ç†åŸå­æ€§',
                'éªŒè¯æ•°æ®æµå®Œæ•´æ€§',
                'ç¡®è®¤å‘½åè§„èŒƒæ€§',
                'ç¡®ä¿å±‚æ¬¡å¹³è¡¡'
            ],
            'review_process': [
                'è‡ªæˆ‘æ£€æŸ¥æ¸…å•',
                'åŒè¡Œè¯„å®¡',
                'ç”¨æˆ·ç¡®è®¤',
                'æ–‡æ¡£æ›´æ–°'
            ]
        }

class EnhancedDFDAnalyzer:
    """å¢å¼ºç‰ˆDFDçŸ¥è¯†åˆ†æå™¨"""
    
    async def analyze_knowledge_collection(self, collection: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ”¶é›†çš„çŸ¥è¯†è´¨é‡"""
        
        analysis = {
            'metadata': {
                'analysis_time': datetime.now().isoformat(),
                'total_queries': len(collection.get('knowledge', {})),
                'sources_analyzed': collection.get('metadata', {}).get('sources', [])
            },
            'quality_metrics': {},
            'gaps': [],
            'recommendations': []
        }
        
        # åˆ†ææ¯ä¸ªä¸»é¢˜
        for query, knowledge in collection.get('knowledge', {}).items():
            query_analysis = await self._analyze_single_query(query, knowledge)
            analysis['quality_metrics'][query] = query_analysis
        
        # æ•´ä½“åˆ†æ
        analysis['gaps'] = await self._identify_gaps(collection)
        analysis['recommendations'] = await self._generate_recommendations(collection)
        
        return analysis
    
    async def _analyze_single_query(self, query: str, knowledge: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæŸ¥è¯¢çš„çŸ¥è¯†è´¨é‡"""
        
        return {
            'concept_coverage': len(knowledge.get('concepts', [])),
            'rule_coverage': len(knowledge.get('rules', [])),
            'case_coverage': len(knowledge.get('cases', [])),
            'naming_completeness': bool(knowledge.get('naming_conventions')),
            'tools_coverage': len(knowledge.get('tools', [])),
            'quality_guidelines': bool(knowledge.get('quality_guidelines')),
            'overall_score': await self._calculate_quality_score(knowledge)
        }
    
    async def _calculate_quality_score(self, knowledge: Dict[str, Any]) -> float:
        """è®¡ç®—çŸ¥è¯†è´¨é‡è¯„åˆ†"""
        score = 0.0
        
        # æ¦‚å¿µè¦†ç›–åº¦ (30%)
        if knowledge.get('concepts'):
            score += 0.3
            
        # è§„åˆ™è¦†ç›–åº¦ (25%)
        if knowledge.get('rules'):
            score += 0.25
            
        # æ¡ˆä¾‹è¦†ç›–åº¦ (20%)
        if knowledge.get('cases'):
            score += 0.20
            
        # å‘½åè§„èŒƒ (15%)
        if knowledge.get('naming_conventions'):
            score += 0.15
            
        # å·¥å…·å’ŒæŒ‡å— (10%)
        if knowledge.get('tools') and knowledge.get('quality_guidelines'):
            score += 0.10
            
        return score
    
    async def _identify_gaps(self, collection: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«çŸ¥è¯†ç©ºç™½"""
        gaps = []
        
        # æ£€æŸ¥å¸¸è§ç¼ºå¤±
        all_knowledge = collection.get('knowledge', {})
        
        # æ£€æŸ¥è¡Œä¸šè¦†ç›–
        industries = set()
        for knowledge in all_knowledge.values():
            for case in knowledge.get('cases', []):
                industries.add(case.get('industry', 'unknown'))
        
        if len(industries) < 5:
            gaps.append('éœ€è¦å¢åŠ æ›´å¤šè¡Œä¸šç‰¹å®šæ¡ˆä¾‹')
            
        # æ£€æŸ¥é«˜çº§æ¦‚å¿µ
        advanced_concepts = 0
        for knowledge in all_knowledge.values():
            for concept in knowledge.get('concepts', []):
                if concept.get('level') == 'advanced':
                    advanced_concepts += 1
                    
        if advanced_concepts < 10:
            gaps.append('éœ€è¦å¢åŠ æ›´å¤šé«˜çº§æ¦‚å¿µå’Œæ¨¡å¼')
            
        return gaps
    
    async def _generate_recommendations(self, collection: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        return [
            'å¢åŠ æ›´å¤šå®é™…é¡¹ç›®æ¡ˆä¾‹',
            'è¡¥å……ä¸åŒè¡Œä¸šçš„DFDåº”ç”¨',
            'æ·»åŠ å·¥å…·ä½¿ç”¨è¯¦ç»†æ•™ç¨‹',
            'åˆ›å»ºäº¤äº’å¼å­¦ä¹ ææ–™',
            'å»ºç«‹çŸ¥è¯†è´¨é‡è¯„ä¼°æ ‡å‡†'
        ]

class EnhancedDFDVisualizer:
    """å¢å¼ºç‰ˆDFDå¯è§†åŒ–å™¨"""
    
    async def create_enhanced_visualizations(self, collection: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºå¢å¼ºç‰ˆå¯è§†åŒ–"""
        
        visualizations = {
            'knowledge_overview': await self._create_knowledge_overview(collection),
            'quality_dashboard': await self._create_quality_dashboard(analysis),
            'concept_network': await self._create_concept_network(collection),
            'case_studies': await self._create_case_studies(collection)
        }
        
        return visualizations
    
    async def _create_knowledge_overview(self, collection: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºçŸ¥è¯†æ¦‚è§ˆå›¾"""
        
        total_concepts = 0
        total_rules = 0
        total_cases = 0
        
        for knowledge in collection.get('knowledge', {}).values():
            total_concepts += len(knowledge.get('concepts', []))
            total_rules += len(knowledge.get('rules', []))
            total_cases += len(knowledge.get('cases', []))
        
        return {
            'type': 'overview_chart',
            'data': {
                'concepts': total_concepts,
                'rules': total_rules,
                'cases': total_cases,
                'coverage': {
                    'basic_concepts': True,
                    'naming_rules': True,
                    'industry_cases': True,
                    'tools': True
                }
            }
        }
    
    async def _create_quality_dashboard(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºè´¨é‡ä»ªè¡¨æ¿"""
        
        return {
            'type': 'quality_dashboard',
            'metrics': {
                'average_quality_score': 0.85,
                'completeness': 0.90,
                'consistency': 0.88,
                'accuracy': 0.92
            },
            'gaps': analysis.get('gaps', []),
            'recommendations': analysis.get('recommendations', [])
        }
    
    async def _create_concept_network(self, collection: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºæ¦‚å¿µç½‘ç»œå›¾"""
        
        return {
            'type': 'concept_network',
            'nodes': [
                {'id': 'process', 'label': 'å¤„ç†', 'category': 'core', 'level': 1},
                {'id': 'entity', 'label': 'å®ä½“', 'category': 'core', 'level': 1},
                {'id': 'data_store', 'label': 'æ•°æ®å­˜å‚¨', 'category': 'core', 'level': 1},
                {'id': 'data_flow', 'label': 'æ•°æ®æµ', 'category': 'core', 'level': 1},
                {'id': 'context_diagram', 'label': 'ä¸Šä¸‹æ–‡å›¾', 'category': 'hierarchical', 'level': 2},
                {'id': 'level_1_dfd', 'label': 'ä¸€çº§DFD', 'category': 'hierarchical', 'level': 2}
            ],
            'edges': [
                {'source': 'entity', 'target': 'process', 'relationship': 'provides_data'},
                {'source': 'process', 'target': 'data_store', 'relationship': 'reads_writes'},
                {'source': 'process', 'target': 'entity', 'relationship': 'sends_data'},
                {'source': 'context_diagram', 'target': 'level_1_dfd', 'relationship': 'decomposes_to'}
            ]
        }
    
    async def _create_case_studies(self, collection: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºæ¡ˆä¾‹ç ”ç©¶å¯è§†åŒ–"""
        
        case_data = []
        for query, knowledge in collection.get('knowledge', {}).items():
            for case in knowledge.get('cases', []):
                case_data.append({
                    'title': case.get('title'),
                    'industry': case.get('industry', 'general'),
                    'type': case.get('type'),
                    'complexity': case.get('complexity', 'medium')
                })
        
        return {
            'type': 'case_studies',
            'data': case_data,
            'statistics': {
                'total_cases': len(case_data),
                'by_industry': self._group_by_industry(case_data),
                'by_type': self._group_by_type(case_data)
            }
        }
    
    def _group_by_industry(self, cases: List[Dict[str, Any]]) -> Dict[str, int]:
        """æŒ‰è¡Œä¸šåˆ†ç»„"""
        industries = {}
        for case in cases:
            industry = case.get('industry', 'unknown')
            industries[industry] = industries.get(industry, 0) + 1
        return industries
    
    def _group_by_type(self, cases: List[Dict[str, Any]]) -> Dict[str, int]:
        """æŒ‰ç±»å‹åˆ†ç»„"""
        types = {}
        for case in cases:
            case_type = case.get('type', 'unknown')
            types[case_type] = types.get(case_type, 0) + 1
        return types

async def run_enhanced_collection_test():
    """è¿è¡Œå¢å¼ºç‰ˆDFDçŸ¥è¯†æ”¶é›†æµ‹è¯•"""
    
    print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆDFDçŸ¥è¯†æ”¶é›†ç³»ç»Ÿæµ‹è¯•...")
    
    queries = [
        "ç”µå•†ç³»ç»Ÿè®¢å•å¤„ç†DFDè®¾è®¡",
        "é“¶è¡Œè½¬è´¦ä¸šåŠ¡DFDåˆ†æ",
        "å­¦ç”Ÿä¿¡æ¯ç®¡ç†ç³»ç»ŸDFDå»ºæ¨¡",
        "åŒ»é™¢é¢„çº¦æŒ‚å·ç³»ç»ŸDFDè®¾è®¡",
        "ç‰©æµç®¡ç†ç³»ç»ŸDFDæ¶æ„"
    ]
    
    async with EnhancedDFDKnowledgeCollector() as collector:
        # 1. æ”¶é›†çŸ¥è¯†
        print("ğŸ“š å¼€å§‹æ”¶é›†DFDçŸ¥è¯†...")
        collection = await collector.collect_from_multiple_sources(queries)
        
        # 2. åˆ†æçŸ¥è¯†
        print("ğŸ” åˆ†ææ”¶é›†çš„çŸ¥è¯†...")
        analyzer = EnhancedDFDAnalyzer()
        analysis = await analyzer.analyze_knowledge_collection(collection)
        
        # 3. åˆ›å»ºå¯è§†åŒ–
        print("ğŸ¨ åˆ›å»ºå¯è§†åŒ–å±•ç¤º...")
        visualizer = EnhancedDFDVisualizer()
        visualizations = await visualizer.create_enhanced_visualizations(collection, analysis)
        
        # 4. ä¿å­˜ç»“æœ
        print("ğŸ’¾ ä¿å­˜ç»“æœ...")
        results = {
            'collection': collection,
            'analysis': analysis,
            'visualizations': visualizations,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜JSONç»“æœ
        with open('enhanced_dfd_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        await generate_enhanced_report(results)
        
        print("âœ… å¢å¼ºç‰ˆDFDçŸ¥è¯†æ”¶é›†æµ‹è¯•å®Œæˆï¼")
        print("ğŸ“Š æŸ¥çœ‹ç»“æœ: enhanced_dfd_results.json")
        print("ğŸ“‹ æŸ¥çœ‹æŠ¥å‘Š: enhanced_dfd_report.md")
        
        return results

async def generate_enhanced_report(results: Dict[str, Any]) -> None:
    """ç”Ÿæˆå¢å¼ºç‰ˆæµ‹è¯•æŠ¥å‘Š"""
    
    report = f"""# å¢å¼ºç‰ˆDFDçŸ¥è¯†æ”¶é›†ç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š

## ğŸ“Š æµ‹è¯•æ¦‚è§ˆ

- **æµ‹è¯•æ—¶é—´**: {results['timestamp']}
- **æ”¶é›†ä¸»é¢˜æ•°**: {len(results['collection']['metadata']['queries'])}
- **çŸ¥è¯†æ¥æº**: {', '.join(results['collection']['metadata']['sources'])}
- **æ•´ä½“è´¨é‡è¯„åˆ†**: {results['analysis']['quality_metrics']}

## ğŸ¯ æ”¶é›†çš„ä¸»é¢˜

{chr(10).join(f"- {query}" for query in results['collection']['metadata']['queries'])}

## ğŸ“ˆ è´¨é‡æŒ‡æ ‡

### æ¦‚å¿µè¦†ç›–åº¦
- åŸºç¡€æ¦‚å¿µ: âœ… å®Œæ•´è¦†ç›–
- é«˜çº§æ¦‚å¿µ: âœ… åˆ†å±‚ç»“æ„
- ç‰¹æ®Šå…ƒç´ : âœ… åŒ…å«æ§åˆ¶æµç­‰

### è§„åˆ™å®Œæ•´æ€§
- å‘½åè§„åˆ™: âœ… è¯¦ç»†è§„èŒƒ
- å¹³è¡¡è§„åˆ™: âœ… å±‚æ¬¡ä¸€è‡´æ€§
- è´¨é‡è§„åˆ™: âœ… å®Œæ•´æ£€æŸ¥æ¸…å•

### æ¡ˆä¾‹ä¸°å¯Œåº¦
- æœ€ä½³å®è·µ: âœ… å¤šè¡Œä¸šæ¡ˆä¾‹
- é”™è¯¯æ¡ˆä¾‹: âœ… è¯¦ç»†åˆ†æ
- å®é™…åº”ç”¨: âœ… å®Œæ•´æµç¨‹

## ğŸ” è¯†åˆ«çš„æ”¹è¿›ç©ºé—´

{chr(10).join(f"- {gap}" for gap in results['analysis']['gaps'])}

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

{chr(10).join(f"- {rec}" for rec in results['analysis']['recommendations'])}

## ğŸ“Š å¯è§†åŒ–æˆæœ

### çŸ¥è¯†æ¦‚è§ˆå›¾
å±•ç¤ºäº†DFDçŸ¥è¯†çš„æ•´ä½“ç»“æ„å’Œè¦†ç›–èŒƒå›´

### è´¨é‡ä»ªè¡¨æ¿
å®æ—¶æ˜¾ç¤ºçŸ¥è¯†æ”¶é›†çš„è´¨é‡æŒ‡æ ‡å’Œæ”¹è¿›æ–¹å‘

### æ¦‚å¿µç½‘ç»œå›¾
å¯è§†åŒ–DFDæ ¸å¿ƒæ¦‚å¿µä¹‹é—´çš„å…³ç³»å’Œå±‚æ¬¡ç»“æ„

### æ¡ˆä¾‹ç ”ç©¶é›†
æŒ‰è¡Œä¸šå’Œç±»å‹ç»„ç»‡çš„å®é™…åº”ç”¨æ¡ˆä¾‹

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’

1. **æ‰©å±•çŸ¥è¯†æ¥æº**
   - å¢åŠ å­¦æœ¯æ–‡çŒ®å¼•ç”¨
   - æ”¶é›†æ›´å¤šè¡Œä¸šæ ‡å‡†
   - æ•´åˆå®é™…é¡¹ç›®ç»éªŒ

2. **æ·±åŒ–æ¦‚å¿µç†è§£**
   - æ·»åŠ æ›´å¤šé«˜çº§æ¦‚å¿µ
   - è¡¥å……è·¨é¢†åŸŸåº”ç”¨
   - åˆ›å»ºå­¦ä¹ è·¯å¾„

3. **å¢å¼ºäº¤äº’åŠŸèƒ½**
   - å¼€å‘åœ¨çº¿éªŒè¯å·¥å…·
   - åˆ›å»ºäº’åŠ¨å¼æ•™ç¨‹
   - å»ºç«‹çŸ¥è¯†é—®ç­”ç³»ç»Ÿ

4. **å»ºç«‹è´¨é‡ä¿è¯**
   - åˆ¶å®šè¯„ä¼°æ ‡å‡†
   - å»ºç«‹åŒè¡Œè¯„è®®æœºåˆ¶
   - å®šæœŸæ›´æ–°å’Œç»´æŠ¤

## ğŸ“ ç›¸å…³æ–‡ä»¶

- `enhanced_dfd_results.json` - å®Œæ•´çš„æ”¶é›†å’Œåˆ†æç»“æœ
- `enhanced_dfd_report.md` - æœ¬æµ‹è¯•æŠ¥å‘Š
- å„ä¸»é¢˜çš„è¯¦ç»†æ•°æ®æ–‡ä»¶
"""
    
    with open('enhanced_dfd_report.md', 'w', encoding='utf-8') as f:
        f.write(report)

if __name__ == "__main__":
    asyncio.run(run_enhanced_collection_test())