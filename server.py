# -*- coding: utf-8 -*-
import base64
from io import BytesIO
from PIL import Image
import json
import os
from dotenv import load_dotenv
import httpx
from typing import Any, List, Dict
from mcp.server.fastmcp import FastMCP
import logging

# åœ¨å¯¼å…¥ä»»ä½•è‡ªå®šä¹‰æ¨¡å—ä¹‹å‰é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('mcp_server.log'),
        # Remove StreamHandler to avoid stdout pollution
    ],
    force=True  # å¼ºåˆ¶é‡æ–°é…ç½®ï¼Œè¦†ç›–ä»»ä½•å·²å­˜åœ¨çš„é…ç½®
)

# ç¡®ä¿æ‰€æœ‰å·²å­˜åœ¨çš„ logger éƒ½ä½¿ç”¨æ–°é…ç½®
for name in logging.Logger.manager.loggerDict:
    logger_obj = logging.getLogger(name)
    logger_obj.handlers.clear()
    logger_obj.propagate = True

# ç°åœ¨å¯ä»¥å®‰å…¨åœ°å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from scripts.format_processor import FormatProcessor
from utils.web_deduplication import get_deduplication_instance, check_and_cache, clean_cache, get_stats
from utils.webpage_storage import get_storage_instance
try:
    from httpx_socks import AsyncProxyTransport
    SOCKS_AVAILABLE = True
except ImportError:
    SOCKS_AVAILABLE = False
    # Warning: httpx-socks not installed, SOCKS proxy functionality will be unavailable
from openai import OpenAI
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
import asyncio
from urllib.parse import urljoin, urlparse
import time
import requests  # æ·»åŠ requestsåº“ç”¨äºSerpAPIè¯·æ±‚
import datetime
import re
from pathlib import Path
import subprocess
import json
import requests

import random
import threading

# å¯¼å…¥MySQLæ•°æ®åº“å·¥å…·
# from mysql_db_utils import init_db, save_to_db, close_pool

load_dotenv()
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"), base_url=os.getenv("BASE_URL"))
model = os.getenv("MODEL")

mcp = FastMCP("WebScrapingServer")

# åˆå§‹åŒ–æ ¼å¼å¤„ç†å™¨
format_processor = FormatProcessor()

# åŠ è½½ç³»ç»Ÿæç¤ºè¯é…ç½®
def load_system_prompts():
    """åŠ è½½ç³»ç»Ÿæç¤ºè¯é…ç½®æ–‡ä»¶"""
    try:
        config_path = Path("config/system_prompts.json")
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            logger.warning(f"ç³»ç»Ÿæç¤ºè¯é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return None
    except Exception as e:
        logger.error(f"åŠ è½½ç³»ç»Ÿæç¤ºè¯é…ç½®å¤±è´¥: {str(e)}")
        return None

def get_system_prompt(prompt_type=None):
    """è·å–æŒ‡å®šç±»å‹çš„ç³»ç»Ÿæç¤ºè¯"""
    prompts_config = load_system_prompts()
    if not prompts_config:
        # å¦‚æœé…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œè¿”å›é»˜è®¤çš„é€šç”¨æç¤ºè¯
        return (
            "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç½‘é¡µå†…å®¹åˆ†æä¸“å®¶ã€‚ä½ çš„ä¸»è¦èŒè´£æ˜¯ä»ç½‘é¡µå†…å®¹ä¸­æå–å’Œåˆ†ææœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚\n\n"
            "è¯·é‡ç‚¹å…³æ³¨ä»¥ä¸‹å†…å®¹ï¼š\n"
            "1. æ ¸å¿ƒæ¦‚å¿µã€å®šä¹‰å’Œå…³é”®æœ¯è¯­\n"
            "2. é‡è¦çš„æ–¹æ³•ã€æ­¥éª¤å’ŒæŠ€å·§\n"
            "3. å®é™…æ¡ˆä¾‹å’Œåº”ç”¨åœºæ™¯\n"
            "4. æœ€ä½³å®è·µå’Œæ³¨æ„äº‹é¡¹\n"
            "5. ç›¸å…³å·¥å…·å’Œèµ„æºæ¨è\n"
            "6. æŠ€æœ¯è§„èŒƒå’Œæ ‡å‡†\n"
            "7. å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ\n"
            "8. è¡Œä¸šè¶‹åŠ¿å’Œå‘å±•æ–¹å‘\n\n"
            "è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€æ€»ç»“å†…å®¹ï¼Œçªå‡ºæ ¸å¿ƒçŸ¥è¯†ç‚¹å’Œå®ç”¨ä»·å€¼ã€‚"
        )
    
    # ä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è·å–æç¤ºè¯ç±»å‹
    if not prompt_type:
        prompt_type = os.getenv("SYSTEM_PROMPT_TYPE", prompts_config.get("default_prompt_type", "general"))
    
    # è·å–æŒ‡å®šç±»å‹çš„æç¤ºè¯
    prompts = prompts_config.get("prompts", {})
    if prompt_type in prompts:
        return prompts[prompt_type]["system_prompt"]
    else:
        logger.warning(f"æœªæ‰¾åˆ°æç¤ºè¯ç±»å‹ '{prompt_type}'ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹")
        default_type = prompts_config.get("default_prompt_type", "general")
        if default_type in prompts:
            return prompts[default_type]["system_prompt"]
        else:
            # å¦‚æœé»˜è®¤ç±»å‹ä¹Ÿä¸å­˜åœ¨ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„æç¤ºè¯
            if prompts:
                first_key = list(prompts.keys())[0]
                return prompts[first_key]["system_prompt"]
            else:
                # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰ä»»ä½•æç¤ºè¯ï¼Œè¿”å›ç¡¬ç¼–ç çš„é»˜è®¤æç¤ºè¯
                return get_system_prompt()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
BASE_URL = os.getenv("BASE_URL")
MODEL = os.getenv("MODEL")

SERPAPI_API_KEY = os.getenv("SERPAPI_API_KEY")  # SerpAPIçš„APIå¯†é’¥

# Torä»£ç†é…ç½®
USE_TOR = os.getenv("USE_TOR", "false").lower() == "true"
TOR_SOCKS_PORT = int(os.getenv("TOR_SOCKS_PORT", "9050"))
TOR_CONTROL_PORT = int(os.getenv("TOR_CONTROL_PORT", "9051"))
TOR_PASSWORD = os.getenv("TOR_PASSWORD", "")
TOR_EXECUTABLE_PATH = os.getenv("TOR_EXECUTABLE_PATH", "tor")  # Torå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„

# å…¨å±€è‡ªå®šä¹‰headerså’Œå¯é€‰Cookie
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Accept-Encoding": "gzip, deflate, br",
    "Referer": "https://www.zhihu.com/",
    "Connection": "keep-alive",
}
DEFAULT_COOKIES = os.getenv("SCRAPER_COOKIES", "")  # å¯åœ¨.envä¸­é…ç½®Cookieå­—ç¬¦ä¸²

def parse_cookies(cookie_str):
    cookies = {}
    for item in cookie_str.split(';'):
        if '=' in item:
            k, v = item.split('=', 1)
            cookies[k.strip()] = v.strip()
    return cookies


class TorManager:
    """Torä»£ç†ç®¡ç†å™¨ï¼ˆä½¿ç”¨subprocessè‡ªåŠ¨å¯åŠ¨ï¼‰"""
    
    def __init__(self):
        self.tor_process = None
        self.is_running = False
        self.lock = threading.Lock()
        self.log_file = None
    
    def start_tor(self):
        """ä½¿ç”¨subprocesså¯åŠ¨Torè¿›ç¨‹"""
        if self.is_running:
            return True
            
        try:
            with self.lock:
                if self.is_running:
                    return True
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¤–éƒ¨Torè¿›ç¨‹åœ¨è¿è¡Œ
                if self._check_existing_tor():
                    logger.info("Detected existing Tor process, using it")
                    self.is_running = True
                    return True
                    
                # Starting Tor process...
                
                # æ£€æŸ¥Torå¯æ‰§è¡Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                tor_cmd = TOR_EXECUTABLE_PATH
                if not self._check_tor_executable(tor_cmd):
                    # ERROR: Tor executable not found
                    return False
                
                # æ„å»ºTorå¯åŠ¨å‘½ä»¤
                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰é…ç½®æ–‡ä»¶
                torrc_path = "./torrc"
                if os.path.exists(torrc_path):
                    cmd = [tor_cmd, "-f", torrc_path]
                    # å¦‚æœè®¾ç½®äº†å¯†ç ï¼Œæ·»åŠ å¯†ç é…ç½®
                    if TOR_PASSWORD:
                        cmd.extend(["--HashedControlPassword", self._hash_password(TOR_PASSWORD)])
                else:
                    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
                    cmd = [
                        tor_cmd,
                        "--SocksPort", str(TOR_SOCKS_PORT),
                        "--ControlPort", str(TOR_CONTROL_PORT),
                        "--DataDirectory", "./tor_data",
                        "--Log", "notice file ./tor_data/tor.log",
                        "--NumEntryGuards", "8",
                        "--CircuitBuildTimeout", "30",
                        "--MaxClientCircuitsPending", "32"
                    ]
                    
                    # å¦‚æœè®¾ç½®äº†å¯†ç ï¼Œæ·»åŠ å¯†ç é…ç½®
                    if TOR_PASSWORD:
                        cmd.extend(["--HashedControlPassword", self._hash_password(TOR_PASSWORD)])
                
                # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
                data_dir = "./tor_data"
                if not os.path.exists(data_dir):
                    os.makedirs(data_dir)
                
                # å¯åŠ¨Torè¿›ç¨‹ï¼Œé‡å®šå‘è¾“å‡ºåˆ°æ–‡ä»¶é¿å…ç¼–ç é—®é¢˜
                log_file_path = os.path.join(data_dir, 'tor.log')
                try:
                    log_file = open(log_file_path, 'w', encoding='utf-8', errors='ignore')
                    
                    # è®°å½•å¯åŠ¨å‘½ä»¤ç”¨äºè°ƒè¯•
                    logger.info(f"Starting Tor with command: {' '.join(cmd)}")
                    
                    self.tor_process = subprocess.Popen(
                        cmd,
                        stdout=log_file,
                        stderr=subprocess.STDOUT,
                        creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                    )
                    self.log_file = log_file
                    
                    # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦ç«‹å³é€€å‡º
                    time.sleep(2)
                    if self.tor_process.poll() is not None:
                        # è¿›ç¨‹å·²é€€å‡ºï¼Œè¯»å–æ—¥å¿—æŸ¥çœ‹é”™è¯¯
                        log_file.close()
                        try:
                            with open(log_file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                log_content = f.read()
                                logger.error(f"Tor process exited with code {self.tor_process.returncode}")
                                logger.error(f"Log content: {log_content[:500]}")
                        except:
                            pass
                        return False
                    
                    # ç­‰å¾…Torå¯åŠ¨
                    if self._wait_for_tor_ready():
                        self.is_running = True
                        # SUCCESS: Tor started, SOCKS proxy port ready
                        return True
                    else:
                        # ERROR: Tor startup timeout
                        self.cleanup()
                        return False
                        
                except Exception as e:
                    logger.error(f"Failed to start Tor process: {e}")
                    if 'log_file' in locals():
                        try:
                            log_file.close()
                        except:
                            pass
                    return False
                    
        except Exception as e:
            # ERROR: Failed to start Tor
            self.cleanup()
            return False
    
    def _check_tor_executable(self, tor_cmd):
        """æ£€æŸ¥Torå¯æ‰§è¡Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        try:
            result = subprocess.run([tor_cmd, "--version"], 
                                  capture_output=True, 
                                  timeout=5,
                                  creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0)
            return result.returncode == 0
        except:
            return False
    
    def _check_existing_tor(self):
        """æ£€æŸ¥æ˜¯å¦å·²æœ‰Torè¿›ç¨‹åœ¨è¿è¡Œ"""
        import socket
        try:
            # æ£€æŸ¥SOCKSç«¯å£æ˜¯å¦å¯ç”¨
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2)
            result = sock.connect_ex(('127.0.0.1', TOR_SOCKS_PORT))
            sock.close()
            
            if result == 0:
                # SOCKSç«¯å£å¯ç”¨ï¼Œå†æ£€æŸ¥æ§åˆ¶ç«¯å£
                try:
                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    sock.settimeout(2)
                    result = sock.connect_ex(('127.0.0.1', TOR_CONTROL_PORT))
                    sock.close()
                    
                    if result == 0:
                        # ä¸¤ä¸ªç«¯å£éƒ½å¯ç”¨ï¼Œè¯´æ˜æœ‰Toråœ¨è¿è¡Œ
                        return True
                except:
                    pass
        except:
            pass
        
        return False
    
    def _hash_password(self, password):
        """ç”ŸæˆTorå¯†ç å“ˆå¸Œï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            import hashlib
            import base64
            salt = os.urandom(8)
            key = hashlib.pbkdf2_hmac('sha1', password.encode(), salt, 1000, 20)
            return base64.b64encode(salt + key).decode()
        except:
            return None
    
    def _wait_for_tor_ready(self, timeout=30):
        """ç­‰å¾…Torå‡†å¤‡å°±ç»ªï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œåªæ£€æŸ¥ç«¯å£å¯ç”¨æ€§ï¼‰"""
        import socket
        start_time = time.time()
        
        # ç­‰å¾…SOCKSç«¯å£å¯ç”¨
        while time.time() - start_time < timeout:
            try:
                # å°è¯•è¿æ¥SOCKSç«¯å£
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(2)
                result = sock.connect_ex(('127.0.0.1', TOR_SOCKS_PORT))
                sock.close()
                
                if result == 0:
                    # SOCKSç«¯å£å¯ç”¨ï¼Œå†æ£€æŸ¥æ§åˆ¶ç«¯å£
                    try:
                        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                        sock.settimeout(2)
                        result = sock.connect_ex(('127.0.0.1', TOR_CONTROL_PORT))
                        sock.close()
                        
                        if result == 0:
                            # ä¸¤ä¸ªç«¯å£éƒ½å¯ç”¨ï¼Œè®¤ä¸ºTorå·²å‡†å¤‡å°±ç»ª
                            logger.info("Tor ports are ready")
                            return True
                    except:
                        pass
                    
            except:
                pass
                
            time.sleep(1)
            
        logger.warning(f"Tor startup timeout after {timeout} seconds")
        return False
    
    def new_identity(self):
        """è¯·æ±‚æ–°çš„Torèº«ä»½ï¼ˆé€šè¿‡é‡å¯å®ç°ï¼‰"""
        if not self.is_running:
            return False
            
        try:
            # Changing Tor identity...
            self.cleanup()
            time.sleep(2)
            return self.start_tor()
        except Exception as e:
            # Failed to change Tor identity
            return False
    
    def get_proxy_config(self):
        """è·å–ä»£ç†é…ç½®"""
        if not self.is_running:
            return None
        return f"socks5://127.0.0.1:{TOR_SOCKS_PORT}"
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.tor_process:
                self.tor_process.terminate()
                try:
                    self.tor_process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    self.tor_process.kill()
                self.tor_process = None
                
            self.is_running = False
            
            # å…³é—­æ—¥å¿—æ–‡ä»¶
            if hasattr(self, 'log_file') and self.log_file:
                try:
                    self.log_file.close()
                    self.log_file = None
                except Exception as e:
                    # Error closing log file
                    pass
                    
            # Tor process stopped
        except Exception as e:
            # Error cleaning up Tor resources
            pass


# å…¨å±€Torç®¡ç†å™¨å®ä¾‹
tor_manager = TorManager() if USE_TOR else None


def get_http_client_config():
    """è·å–HTTPå®¢æˆ·ç«¯é…ç½®"""
    config = {
        "timeout": 30.0,
        "follow_redirects": True,
    }
    
    if USE_TOR and tor_manager and tor_manager.is_running and SOCKS_AVAILABLE:
        proxy_url = tor_manager.get_proxy_config()
        if proxy_url:
            try:
                # ä½¿ç”¨httpx-socksçš„AsyncProxyTransportæ¥æ”¯æŒSOCKS5ä»£ç†
                transport = AsyncProxyTransport.from_url(proxy_url)
                config["transport"] = transport
                logger.info(f"ä½¿ç”¨Torä»£ç† (transport): {proxy_url}")
            except Exception as e:
                logger.warning(f"ä»£ç†é…ç½®é”™è¯¯: {e}")
                logger.info("å°†ä½¿ç”¨æ™®é€šç½‘ç»œè¿æ¥")
    elif USE_TOR and tor_manager and tor_manager.is_running and not SOCKS_AVAILABLE:
        logger.warning("æ£€æµ‹åˆ°Torä»£ç†å·²å¯ç”¨ï¼Œä½†httpx-socksæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨SOCKSä»£ç†")
        logger.info("è¯·è¿è¡Œ: pip install httpx-socks[asyncio]")
    
    return config


# å¯¼å…¥crawler_framework
# å¯¼å…¥çˆ¬è™«æ¡†æ¶ï¼ˆåœ¨æ—¥å¿—é…ç½®ä¹‹åï¼‰
from utils.crawler_framework import CrawlerFramework

# åˆå§‹åŒ–çˆ¬è™«æ¡†æ¶
crawler = CrawlerFramework()

# è·å–å½“å‰æ¨¡å—çš„ logger
logger = logging.getLogger(__name__)

def search_web(keyword: str, max_results=12):
    """ä½¿ç”¨SerpAPIæœç´¢ç½‘é¡µ - å…¼å®¹æ€§ä¿ç•™å‡½æ•°"""
    logger.info("æ³¨æ„: search_webå‡½æ•°å·²ç»è¿‡æ—¶ï¼Œå»ºè®®ä½¿ç”¨æ–°çš„é€šç”¨çˆ¬è™«æ¡†æ¶ crawler.search_and_parse")
    try:
        # ä½¿ç”¨æ–°æ¡†æ¶çš„Googleæœç´¢
        result = crawler.search_and_parse("google", keyword, max_results)
        
        if result["parsed_response"]["success"]:
            # è¿”å›URLåˆ—è¡¨ä»¥ä¿æŒå‘åå…¼å®¹
            urls = [item["url"] for item in result["parsed_response"]["results"] if "url" in item]
            return urls
        else:
            logger.error(f"æœç´¢å¤±è´¥: {result['parsed_response'].get('error', 'æœªçŸ¥é”™è¯¯')}")
            return []
            
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
        return []

@mcp.tool()
def fetch_raw_data(engine: str, keyword: str, max_results: int = 10) -> str:
    """
    ä»æŒ‡å®šæœç´¢å¼•æ“è·å–åŸå§‹æ•°æ®
    
    Args:
        engine: æœç´¢å¼•æ“åç§° (google, bing, baidu, duckduckgo)
        keyword: æœç´¢å…³é”®è¯
        max_results: æœ€å¤§ç»“æœæ•°
    
    Returns:
        åŒ…å«åŸå§‹æ•°æ®ã€å…ƒæ•°æ®å’Œè°ƒè¯•ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
    """
    try:
        result = crawler.fetch_raw_data(engine, keyword, max_results)
        return json.dumps(result, ensure_ascii=False, indent=2)
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "engine": engine,
            "keyword": keyword
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool()
def parse_search_results(raw_response_json: str, engine: str = None, custom_rules: str = None) -> str:
    """
    æ ¹æ®é…ç½®è§„åˆ™è§£æåŸå§‹æœç´¢æ•°æ®
    
    Args:
        raw_response_json: fetch_raw_dataè¿”å›çš„JSONå­—ç¬¦ä¸²
        engine: æœç´¢å¼•æ“åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœraw_responseä¸­æœ‰ï¼‰
        custom_rules: è‡ªå®šä¹‰è§£æè§„åˆ™çš„JSONå­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        è§£æåçš„ç»“æ„åŒ–æ•°æ®JSONå­—ç¬¦ä¸²
    """
    try:
        # è§£æè¾“å…¥å‚æ•°
        raw_response = json.loads(raw_response_json)
        custom_rules_dict = json.loads(custom_rules) if custom_rules else None
        
        # æ‰§è¡Œè§£æ
        result = crawler.parse_results(raw_response, engine, custom_rules_dict)
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except json.JSONDecodeError as e:
        error_result = {
            "success": False,
            "error": f"JSONè§£æå¤±è´¥: {str(e)}"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e)
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool()
def search_and_parse_universal(engine: str, keyword: str, max_results: int = 10, custom_rules: str = None) -> str:
    """
    é€šç”¨æœç´¢å’Œè§£æå·¥å…· - ä¸€ç«™å¼æœç´¢å’Œè§£æ
    
    Args:
        engine: æœç´¢å¼•æ“åç§° (google, bing, baidu, duckduckgo)
        keyword: æœç´¢å…³é”®è¯
        max_results: æœ€å¤§ç»“æœæ•°
        custom_rules: è‡ªå®šä¹‰è§£æè§„åˆ™çš„JSONå­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        åŒ…å«åŸå§‹æ•°æ®å’Œè§£ææ•°æ®çš„å®Œæ•´å“åº”JSONå­—ç¬¦ä¸²
    """
    try:
        # è§£æè‡ªå®šä¹‰è§„åˆ™
        custom_rules_dict = json.loads(custom_rules) if custom_rules else None
        
        # æ‰§è¡Œæœç´¢å’Œè§£æ
        result = crawler.search_and_parse(engine, keyword, max_results, custom_rules_dict)
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except json.JSONDecodeError as e:
        error_result = {
            "success": False,
            "error": f"è‡ªå®šä¹‰è§„åˆ™JSONè§£æå¤±è´¥: {str(e)}"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "engine": engine,
            "keyword": keyword
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool()
def get_available_search_engines() -> str:
    """
    è·å–å¯ç”¨çš„æœç´¢å¼•æ“åˆ—è¡¨åŠå…¶é…ç½®ä¿¡æ¯
    
    Returns:
        æœç´¢å¼•æ“ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
    """
    try:
        engines = crawler.get_available_engines()
        engine_details = {}
        
        for engine in engines:
            config = crawler.get_engine_info(engine)
            engine_details[engine] = {
                "api_name": config.get("api_name", "æœªçŸ¥"),
                "supported_parameters": config.get("parameters", {}).get("optional", []),
                "primary_keys": config.get("parsing_rules", {}).get("primary_keys", [])
            }
        
        result = {
            "available_engines": engines,
            "engine_details": engine_details,
            "total_engines": len(engines)
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e)
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool()
def configure_search_engine(engine: str, config_json: str) -> str:
    """
    åŠ¨æ€é…ç½®æœç´¢å¼•æ“è§£æè§„åˆ™ï¼ˆè¿è¡Œæ—¶é…ç½®ï¼‰
    
    Args:
        engine: æœç´¢å¼•æ“åç§°
        config_json: é…ç½®è§„åˆ™çš„JSONå­—ç¬¦ä¸²
        
    Returns:
        é…ç½®ç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        config = json.loads(config_json)
        
        # éªŒè¯é…ç½®æ ¼å¼
        required_fields = ["engine", "parsing_rules"]
        for field in required_fields:
            if field not in config:
                return json.dumps({
                    "success": False,
                    "error": f"é…ç½®ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
                }, ensure_ascii=False, indent=2)
        
        # æ›´æ–°è¿è¡Œæ—¶é…ç½®
        crawler.engine_configs[engine] = config
        
        result = {
            "success": True,
            "engine": engine,
            "message": f"æœç´¢å¼•æ“ {engine} é…ç½®å·²æ›´æ–°",
            "config_summary": {
                "primary_keys": config.get("parsing_rules", {}).get("primary_keys", []),
                "link_fields": config.get("parsing_rules", {}).get("link_fields", []),
                "api_name": config.get("api_name", "æœªæŒ‡å®š")
            }
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except json.JSONDecodeError as e:
        error_result = {
            "success": False,
            "error": f"é…ç½®JSONè§£æå¤±è´¥: {str(e)}"
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e)
        }
        return json.dumps(error_result, ensure_ascii=False, indent=2)

@mcp.tool()
async def check_tor_ip() -> str:
    """Check current IP address through Tor proxy"""
    if not USE_TOR or not tor_manager:
        return "Tor proxy feature is disabled."
    
    if not tor_manager.is_running:
        return "Tor proxy is not running. Please start Tor proxy first."
    
    try:
        # Check IP through Tor proxy
        config = get_http_client_config()
        async with httpx.AsyncClient(**config) as client:
            response = await client.get("https://httpbin.org/ip", timeout=10)
            if response.status_code == 200:
                ip_data = response.json()
                tor_ip = ip_data.get('origin', 'Unknown')
                
                # Also check without proxy for comparison
                async with httpx.AsyncClient() as normal_client:
                    normal_response = await normal_client.get("https://httpbin.org/ip", timeout=10)
                    if normal_response.status_code == 200:
                        normal_ip_data = normal_response.json()
                        normal_ip = normal_ip_data.get('origin', 'Unknown')
                        
                        return f"[SUCCESS] IP check completed\nTor IP: {tor_ip}\nNormal IP: {normal_ip}\nProxy working: {'Yes' if tor_ip != normal_ip else 'No'}"
                    else:
                        return f"[SUCCESS] Tor IP: {tor_ip}\n[WARNING] Could not get normal IP for comparison"
            else:
                return f"[ERROR] Failed to check IP through Tor. Status code: {response.status_code}"
                
    except Exception as e:
        return f"[ERROR] Failed to check Tor IP: {str(e)}"


@mcp.tool()
async def test_tor_connection() -> str:
    """Test Tor proxy connection with multiple endpoints"""
    if not USE_TOR or not tor_manager:
        return "Tor proxy feature is disabled."
    
    if not tor_manager.is_running:
        return "Tor proxy is not running. Please start Tor proxy first."
    
    test_urls = [
        "https://httpbin.org/ip",
        "https://check.torproject.org/api/ip",
        "https://icanhazip.com"
    ]
    
    results = []
    config = get_http_client_config()
    
    async with httpx.AsyncClient(**config) as client:
        for url in test_urls:
            try:
                start_time = time.time()
                response = await client.get(url, timeout=15)
                end_time = time.time()
                
                if response.status_code == 200:
                    response_time = round((end_time - start_time) * 1000, 2)
                    results.append(f"âœ“ {url}: OK ({response_time}ms)")
                else:
                    results.append(f"âœ— {url}: HTTP {response.status_code}")
                    
            except Exception as e:
                results.append(f"âœ— {url}: {str(e)}")
    
    success_count = len([r for r in results if r.startswith('âœ“')])
    total_count = len(results)
    
    status = "[SUCCESS]" if success_count == total_count else "[PARTIAL]" if success_count > 0 else "[ERROR]"
    
    return f"{status} Tor connection test completed ({success_count}/{total_count} passed)\n" + "\n".join(results)


@mcp.tool()
def validate_tor_config() -> str:
    """Validate Tor configuration settings"""
    issues = []
    warnings = []
    
    # Check if Tor is enabled
    if not USE_TOR:
        return "Tor proxy feature is disabled. Set USE_TOR=true in .env to enable."
    
    # Check Tor executable path
    if not TOR_EXECUTABLE_PATH:
        issues.append("TOR_EXECUTABLE_PATH is not set")
    else:
        import os
        if not os.path.exists(TOR_EXECUTABLE_PATH):
            issues.append(f"Tor executable not found at: {TOR_EXECUTABLE_PATH}")
    
    # Check ports
    if TOR_SOCKS_PORT == TOR_CONTROL_PORT:
        issues.append("SOCKS port and Control port cannot be the same")
    
    if TOR_SOCKS_PORT < 1024 or TOR_SOCKS_PORT > 65535:
        issues.append(f"Invalid SOCKS port: {TOR_SOCKS_PORT} (must be 1024-65535)")
    
    if TOR_CONTROL_PORT < 1024 or TOR_CONTROL_PORT > 65535:
        issues.append(f"Invalid Control port: {TOR_CONTROL_PORT} (must be 1024-65535)")
    
    # Check password
    if not TOR_PASSWORD:
        warnings.append("No control password set (recommended for security)")
    
    # Check httpx-socks availability
    if not SOCKS_AVAILABLE:
        issues.append("httpx-socks library not available. Install with: pip install httpx-socks")
    
    # Generate report
    if issues:
        return f"[ERROR] Configuration validation failed:\n" + "\n".join(f"- {issue}" for issue in issues) + \
               (f"\n\nWarnings:\n" + "\n".join(f"- {warning}" for warning in warnings) if warnings else "")
    elif warnings:
        return f"[WARNING] Configuration has warnings:\n" + "\n".join(f"- {warning}" for warning in warnings)
    else:
        return "[SUCCESS] Tor configuration is valid"


@mcp.tool()
def get_tor_bootstrap_status() -> str:
    """Get detailed Tor bootstrap status and progress"""
    if not USE_TOR or not tor_manager:
        return "Tor proxy feature is disabled."
    
    if not tor_manager.is_running:
        return "Tor proxy is not running. Please start Tor proxy first."
    
    try:
        import socket
        import re
        
        # Connect to control port
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(5)
        sock.connect(('127.0.0.1', TOR_CONTROL_PORT))
        
        # Authenticate
        if TOR_PASSWORD:
            auth_cmd = f'AUTHENTICATE "{TOR_PASSWORD}"\r\n'
        else:
            auth_cmd = 'AUTHENTICATE\r\n'
        
        sock.send(auth_cmd.encode())
        response = sock.recv(1024).decode()
        
        if '250 OK' not in response:
            sock.close()
            return "[ERROR] Failed to authenticate with Tor control port"
        
        results = []
        
        # Get bootstrap status
        sock.send(b'GETINFO status/bootstrap-phase\r\n')
        bootstrap_response = sock.recv(1024).decode()
        
        if 'status/bootstrap-phase=' in bootstrap_response:
            # Extract bootstrap info
            lines = bootstrap_response.split('\n')
            for line in lines:
                if 'status/bootstrap-phase=' in line:
                    bootstrap_info = line.split('status/bootstrap-phase=')[1].strip()
                    
                    # Parse progress
                    progress_match = re.search(r'PROGRESS=(\d+)', bootstrap_info)
                    if progress_match:
                        progress = int(progress_match.group(1))
                        results.append(f"Bootstrap Progress: {progress}%")
                        
                        if progress == 100:
                            results.append("Status: âœ… Fully bootstrapped")
                        elif progress >= 80:
                            results.append("Status: ğŸŸ¡ Nearly ready")
                        else:
                            results.append("Status: ğŸ”„ Still bootstrapping")
                    
                    # Parse summary
                    summary_match = re.search(r'SUMMARY="([^"]+)"', bootstrap_info)
                    if summary_match:
                        summary = summary_match.group(1)
                        results.append(f"Summary: {summary}")
                    
                    break
        
        # Get circuit count
        sock.send(b'GETINFO status/circuit-established\r\n')
        circuit_response = sock.recv(1024).decode()
        
        if 'status/circuit-established=' in circuit_response:
            if 'status/circuit-established=1' in circuit_response:
                results.append("Circuits: âœ… Established")
            else:
                results.append("Circuits: âŒ Not established")
        
        # Get version info
        sock.send(b'GETINFO version\r\n')
        version_response = sock.recv(1024).decode()
        
        if 'version=' in version_response:
            lines = version_response.split('\n')
            for line in lines:
                if 'version=' in line:
                    version = line.split('version=')[1].strip()
                    results.append(f"Tor Version: {version}")
                    break
        
        sock.send(b'QUIT\r\n')
        sock.close()
        
        if results:
            return "[SUCCESS] Tor bootstrap status:\n" + "\n".join(results)
        else:
            return "[WARNING] Could not retrieve bootstrap status"
            
    except Exception as e:
        return f"[ERROR] Failed to get bootstrap status: {str(e)}"


@mcp.tool()
async def auto_rotate_tor_identity(interval_seconds: int = 300, max_rotations: int = 10) -> str:
    """Automatically rotate Tor identity at specified intervals"""
    if not USE_TOR or not tor_manager:
        return "Tor proxy feature is disabled."
    
    if not tor_manager.is_running:
        return "Tor proxy is not running. Please start Tor proxy first."
    
    if interval_seconds < 60:
        return "[ERROR] Minimum interval is 60 seconds to avoid overloading Tor network."
    
    if max_rotations < 1 or max_rotations > 100:
        return "[ERROR] Max rotations must be between 1 and 100."
    
    try:
        rotation_count = 0
        results = []
        
        results.append(f"[INFO] Starting automatic Tor identity rotation")
        results.append(f"[INFO] Interval: {interval_seconds} seconds, Max rotations: {max_rotations}")
        
        # Get initial IP
        try:
            config = get_http_client_config()
            async with httpx.AsyncClient(**config) as client:
                response = await client.get("https://httpbin.org/ip", timeout=10)
                if response.status_code == 200:
                    initial_ip = response.json().get('origin', 'Unknown')
                    results.append(f"[INFO] Initial IP: {initial_ip}")
        except:
            results.append(f"[WARNING] Could not get initial IP")
        
        while rotation_count < max_rotations:
            # Wait for the specified interval
            await asyncio.sleep(interval_seconds)
            
            # Rotate identity
            success = tor_manager.new_identity()
            rotation_count += 1
            
            if success:
                # Wait a bit for the new circuit to establish
                await asyncio.sleep(10)
                
                # Check new IP
                try:
                    config = get_http_client_config()
                    async with httpx.AsyncClient(**config) as client:
                        response = await client.get("https://httpbin.org/ip", timeout=10)
                        if response.status_code == 200:
                            new_ip = response.json().get('origin', 'Unknown')
                            results.append(f"[SUCCESS] Rotation {rotation_count}: New IP {new_ip}")
                        else:
                            results.append(f"[WARNING] Rotation {rotation_count}: Could not verify new IP")
                except Exception as e:
                    results.append(f"[WARNING] Rotation {rotation_count}: IP check failed - {str(e)}")
            else:
                results.append(f"[ERROR] Rotation {rotation_count}: Failed to change identity")
        
        results.append(f"[INFO] Automatic rotation completed. Total rotations: {rotation_count}")
        return "\n".join(results)
        
    except Exception as e:
        return f"[ERROR] Auto rotation failed: {str(e)}"


@mcp.tool()
def get_tor_circuit_info() -> str:
    """Get information about current Tor circuit"""
    if not USE_TOR or not tor_manager:
        return "Tor proxy feature is disabled."
    
    if not tor_manager.is_running:
        return "Tor proxy is not running. Please start Tor proxy first."
    
    try:
        import socket
        import struct
        
        # Try to connect to Tor control port
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(5)
        sock.connect(('127.0.0.1', TOR_CONTROL_PORT))
        
        # Authenticate if password is set
        if TOR_PASSWORD:
            auth_cmd = f'AUTHENTICATE "{TOR_PASSWORD}"\r\n'
        else:
            auth_cmd = 'AUTHENTICATE\r\n'
        
        sock.send(auth_cmd.encode())
        response = sock.recv(1024).decode()
        
        if '250 OK' not in response:
            sock.close()
            return "[ERROR] Failed to authenticate with Tor control port"
        
        # Get circuit information
        sock.send(b'GETINFO circuit-status\r\n')
        circuit_response = sock.recv(4096).decode()
        
        sock.send(b'QUIT\r\n')
        sock.close()
        
        if '250 OK' in circuit_response:
            lines = circuit_response.split('\n')
            circuit_lines = [line for line in lines if line.startswith('250-circuit-status=') or (line.startswith('250+circuit-status='))]
            
            if circuit_lines:
                return f"[SUCCESS] Tor circuit information:\n" + "\n".join(circuit_lines)
            else:
                return "[INFO] No active circuits found"
        else:
            return "[ERROR] Failed to get circuit information"
            
    except Exception as e:
        return f"[ERROR] Failed to get circuit info: {str(e)}"


@mcp.tool()
async def scrape_webpage(url: str, headers=None, cookies=None) -> str:
    """
    æŠ“å–ç½‘é¡µæ–‡æœ¬ + å›¾ç‰‡åˆ†æï¼ˆé€šè¿‡è§†è§‰æ¨¡å‹ï¼‰+ ä½¿ç”¨ä¸»æ¨¡å‹æ€»ç»“ã€‚
    """
    # æ£€æŸ¥URLå»é‡
    dedup_instance = get_deduplication_instance()
    is_duplicate, cache_info = dedup_instance.is_url_duplicate(url)
    if is_duplicate:
        return json.dumps({
            "status": "skipped",
            "message": "URLå·²å­˜åœ¨äºç¼“å­˜ä¸­ï¼Œè·³è¿‡é‡å¤æŠ“å–",
            "url": url,
            "cached_info": cache_info
        }, ensure_ascii=False, indent=2)
    
    headers = headers or DEFAULT_HEADERS
    # çŸ¥ä¹åçˆ¬è™«åŠŸèƒ½å·²ç¦ç”¨ - è‡ªåŠ¨åˆ¤æ–­çŸ¥ä¹ç­‰ç«™ç‚¹ï¼Œè‡ªåŠ¨è·å–Cookie
    # cookies = cookies or get_cookies_for_url(url) or (parse_cookies(DEFAULT_COOKIES) if DEFAULT_COOKIES else None)
    cookies = cookies or (parse_cookies(DEFAULT_COOKIES) if DEFAULT_COOKIES else None)

    # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
    SUPPORTED_IMAGE_FORMATS = ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp', '.svg']
    MAX_IMAGE_SIZE = 5 * 1024 * 1024  # 5MB

    def normalize_image_url(base_url: str, img_url: str) -> str:
        if img_url.startswith(('http://', 'https://')):
            return img_url
        elif img_url.startswith('//'):
            return 'https:' + img_url
        elif img_url.startswith('/'):
            return urljoin(base_url, img_url)
        else:
            return urljoin(base_url, img_url)

    async def download_image_with_retry(client, img_url: str, max_retries: int = 3) -> bytes:
        for attempt in range(max_retries):
            try:
                response = await client.get(img_url, timeout=10.0)
                if response.status_code != 200:
                    # Image request failed
                    return None
                response.raise_for_status()
                return response.content
            except Exception as e:
                if attempt == max_retries - 1:
                    # Image download failed
                    return None
                await asyncio.sleep(1)

    async def is_valid_image_size(client, img_url: str) -> bool:
        try:
            async with client.stream('GET', img_url) as response:
                content_length = response.headers.get('content-length')
                if content_length and int(content_length) > MAX_IMAGE_SIZE:
                    return False
                return True
        except:
            return False

    async def is_valid_image_tag(img_tag) -> bool:
        # è¿‡æ»¤æå°å›¾ç‰‡ã€iconã€å¹¿å‘Šç­‰
        src = img_tag.get('src', '')
        if not src:
            return False
        # è¿‡æ»¤å¸¸è§icon/å¹¿å‘Šå…³é”®è¯
        lower_src = src.lower()
        if any(x in lower_src for x in ['logo', 'icon', 'avatar', 'ad', 'ads', 'spacer', 'blank', 'tracker']):
            return False
        # è¿‡æ»¤æå°å›¾ç‰‡ï¼ˆå¦‚å®½é«˜<32pxï¼‰
        try:
            width = int(img_tag.get('width', 0))
            height = int(img_tag.get('height', 0))
            if width and height and (width < 32 or height < 32):
                return False
        except Exception:
            pass
        return True

    async def get_image_description(client, image_data: bytes, max_retries: int = 2) -> str:
        for attempt in range(max_retries):
            try:
                # å¤„ç†å›¾ç‰‡æ•°æ®
                image = Image.open(BytesIO(image_data)).convert("RGB")
                buffer = BytesIO()
                image.save(buffer, format="JPEG", quality=85)  # é™ä½è´¨é‡ä»¥åŠ å¿«ä¼ è¾“
                b64_img = base64.b64encode(buffer.getvalue()).decode("utf-8")
                
                # è°ƒç”¨è§†è§‰æ¨¡å‹
                visual_payload = {
                    "model": os.getenv("VISUAL_MODEL", "Pro/Qwen/Qwen2.5-VL-7B-Instruct"),
                    "messages": [{"role": "user", "content": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"}],
                    "image": b64_img
                }
                
                # ä½¿ç”¨ç¡…åŸºæµåŠ¨çš„API
                VISUAL_API_URL = os.getenv("VISUAL_API_URL", "https://api.siliconflow.cn/v1/chat/completions")
                visual_response = await client.post(
                    VISUAL_API_URL,
                    json=visual_payload,
                    timeout=30.0,  # å¢åŠ è¶…æ—¶æ—¶é—´
                    headers={
                        "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
                        "Content-Type": "application/json"
                    }
                )
                try:
                    visual_json = visual_response.json()
                    if isinstance(visual_json, dict):
                        return (
                            visual_json.get("message", {}).get("content") or
                            visual_json.get("choices", [{}])[0].get("message", {}).get("content") or
                            "(è§†è§‰æ¨¡å‹æœªè¿”å›æœ‰æ•ˆæè¿°)"
                        ).strip()
                    else:
                        return f"(è§†è§‰æ¨¡å‹è¿”å›å¼‚å¸¸æ ¼å¼: {str(visual_json)})"
                except Exception as json_error:
                    return f"(è§†è§‰æ¨¡å‹JSONè§£æå¤±è´¥: {str(json_error)})"
                
            except Exception as e:
                if attempt == max_retries - 1:
                    return f"å›¾ç‰‡è¯†åˆ«å¤±è´¥ï¼ˆ{str(e)}ï¼‰"
                await asyncio.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•

    try:
        # è·å–HTTPå®¢æˆ·ç«¯é…ç½®ï¼ˆåŒ…å«Torä»£ç†è®¾ç½®ï¼‰
        client_config = get_http_client_config()
        client_config.update({"headers": headers, "cookies": cookies})
        
        async with httpx.AsyncClient(**client_config) as client:
            # Step 1: æŠ“ç½‘é¡µ
            response = await client.get(url)
            response.raise_for_status()
            
            # ç»Ÿä¸€ä½¿ç”¨UTF-8ç¼–ç å¤„ç†ï¼Œä¸æ•°æ®åº“ä¿æŒä¸€è‡´çš„ç¼–ç ï¼ˆæ•°æ®åº“ä½¿ç”¨utf8mb4ï¼Œæ˜¯UTF-8çš„è¶…é›†ï¼‰
            # è¿™æ ·å¯ä»¥é¿å…ç¼–ç è½¬æ¢é—®é¢˜ï¼Œç®€åŒ–å¤„ç†æµç¨‹
            response.encoding = "utf-8"  # å¼ºåˆ¶æŒ‡å®šç¼–ç ä¸ºutf-8
            
            try:
                soup = BeautifulSoup(response.text, "html.parser")
                
                for tag in soup(["script", "style"]):
                    tag.decompose()

                # Step 2: æå–å…¨ç½‘é¡µæ­£æ–‡å†…å®¹
                title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
                headings = [h.get_text(strip=True) for h in soup.find_all(['h1', 'h2'])]
                description = next((m.get("content") for m in soup.find_all("meta", attrs={"name": "description"})), "æ— æè¿°")
                
                # æå–ä¸»è¦å†…å®¹
                main_content = []
                for p in soup.find_all(['p', 'div', 'article']):
                    text = p.get_text(strip=True)
                    if text and len(text) > 20:  # åªä¿ç•™æœ‰æ„ä¹‰çš„æ–‡æœ¬
                        main_content.append(text)

                main_text = f"ã€æ ‡é¢˜ã€‘{title}\nã€æè¿°ã€‘{description}\nã€ç»“æ„ã€‘{headings}\n\n" + "\n".join(main_content)
            except Exception as e:
                logger.error(f"ç½‘é¡µè§£æå¤±è´¥ {url}: {e}")
                return f"{{\"error\": \"ç½‘é¡µè§£æå¤±è´¥: {str(e)}\", \"url\": \"{url}\"}}"

            # Step 3: æå–å›¾ç‰‡URLå¹¶å¤„ç†å›¾ç‰‡
            img_descriptions = []
            image_urls = []
            try:
                img_tags = soup.find_all("img", src=True) if 'soup' in locals() else []
                seen_urls = set()
                valid_imgs = []
                
                # æå–æ‰€æœ‰æœ‰æ•ˆçš„å›¾ç‰‡URL
                logger.info(f"æ‰¾åˆ° {len(img_tags)} ä¸ªimgæ ‡ç­¾")
                for img_tag in img_tags:
                    img_src = img_tag.get("src", "")
                    logger.debug(f"æ£€æŸ¥å›¾ç‰‡: {img_src}")
                    if not await is_valid_image_tag(img_tag):
                        logger.debug(f"å›¾ç‰‡æœªé€šè¿‡éªŒè¯: {img_src}")
                        continue
                    img_url = img_tag["src"]
                    img_url = normalize_image_url(url, img_url)
                    if not any(img_url.lower().endswith(ext) for ext in SUPPORTED_IMAGE_FORMATS):
                        logger.debug(f"å›¾ç‰‡æ ¼å¼ä¸æ”¯æŒ: {img_url}")
                        continue
                    if img_url in seen_urls:
                        continue
                    seen_urls.add(img_url)
                    image_urls.append(img_url)  # ä¿å­˜æ‰€æœ‰å›¾ç‰‡URLç”¨äºä¸‹è½½
                    valid_imgs.append(img_url)
                    logger.info(f"æ·»åŠ æœ‰æ•ˆå›¾ç‰‡: {img_url}")
                    if len(valid_imgs) >= 5:  # åªå¤„ç†å‰5å¼ ç”¨äºè§†è§‰åˆ†æ
                        break
                
                logger.info(f"æœ€ç»ˆæå–åˆ° {len(image_urls)} ä¸ªå›¾ç‰‡URLç”¨äºä¸‹è½½")
                
                # å¯¹å‰å‡ å¼ å›¾ç‰‡è¿›è¡Œè§†è§‰åˆ†æï¼ˆç”¨äºå†…å®¹ç†è§£ï¼‰
                for i, img_url in enumerate(valid_imgs):
                    try:
                        img_data = await download_image_with_retry(client, img_url)
                        if not img_data:
                            continue  # è·³è¿‡æ— æ•ˆå›¾ç‰‡
                        vision_caption = await get_image_description(client, img_data)
                        if vision_caption and not vision_caption.startswith("å›¾ç‰‡è¯†åˆ«å¤±è´¥"):
                            img_descriptions.append(f"ç¬¬{i+1}å¼ å›¾ï¼š{vision_caption}")
                    except Exception as e:
                        logger.warning(f"å¤„ç†å›¾ç‰‡ {img_url} æ—¶å‡ºé”™: {str(e)}")
                        continue
            except Exception as e:
                logger.warning(f"å›¾ç‰‡å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}")

            # Step 4: æ•´åˆå›¾æ–‡è¾“å…¥
            all_desc = "\n".join(img_descriptions) if img_descriptions else "æœªè¯†åˆ«å‡ºå›¾ç‰‡å†…å®¹"

            # æ ¹æ®æ˜¯å¦æœ‰å›¾ç‰‡æè¿°è°ƒæ•´æç¤ºè¯
            if img_descriptions:
                final_prompt = (
                    f"è¯·æ€»ç»“è¿™ä¸ªç½‘é¡µçš„å†…å®¹ï¼Œç»“åˆä»¥ä¸‹æ–‡æœ¬å’Œå›¾ç‰‡æè¿°ï¼š\n\n"
                    f"ğŸ“„ æ–‡æœ¬éƒ¨åˆ†ï¼š\n{main_text}\n\n"
                    f"ğŸ–¼ å›¾ç‰‡æè¿°ï¼š\n{all_desc}"
                )
            else:
                final_prompt = (
                    f"è¯·æ€»ç»“è¿™ä¸ªç½‘é¡µçš„å†…å®¹ï¼š\n\n"
                    f"ğŸ“„ æ–‡æœ¬å†…å®¹ï¼š\n{main_text}"
                )

            # Step 5: ä¸»æ¨¡å‹ç”Ÿæˆæ€»ç»“
            # ä»é…ç½®æ–‡ä»¶è·å–ç³»ç»Ÿæç¤ºè¯
            system_prompt = get_system_prompt()
            
            final_response = openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": final_prompt}
                ]
            )

            # æ–°å¢ï¼šä½¿ç”¨æ–°çš„å­˜å‚¨æ¨¡å—ä¿å­˜ç½‘é¡µå†…å®¹å’Œå›¾ç‰‡
            # ============= æ–°å¢æœ¬åœ°å­˜å‚¨ =============
            import json as _json
            from datetime import datetime as _dt

            # 1. ç”Ÿæˆ tech_topicï¼ˆç”¨æ ‡é¢˜æˆ–é¦–ä¸ª headingï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
            def clean_filename(s):
                return re.sub(r'[^\w\u4e00-\u9fa5]+', '_', s).strip('_')
            tech_topic = title or (headings[0] if headings else "ç½‘é¡µå†…å®¹")
            tech_topic_clean = clean_filename(tech_topic)
            # 2. æ—¶é—´æˆ³
            crawl_time = _dt.now().strftime('%Y-%m-%dT%H-%M-%S')
            crawl_time_human = _dt.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # è·å–å­˜å‚¨å®ä¾‹å¹¶ä¿å­˜ç½‘é¡µå†…å®¹ï¼ˆåŒ…æ‹¬å›¾ç‰‡ä¸‹è½½ï¼‰
            from utils.webpage_storage import get_storage_instance
            storage = get_storage_instance()
            
            # å…ˆå‡†å¤‡åŸºæœ¬å†…å®¹ç”¨äºå­˜å‚¨
            basic_content = {
                "title": title,
                "url": url,
                "main_text": main_text,
                "headings": headings,
                "img_descriptions": img_descriptions,
                "crawl_time": crawl_time_human
            }
            # 5. ç»„è£…å†…å®¹ï¼ˆçŸ¥è¯†åº“å¯¹æ¥æ ¼å¼ï¼‰
            # æ ¹æ®ç½‘é¡µå†…å®¹åˆ†æDFDç›¸å…³å…ƒç´ 
            try:
                if hasattr(final_response, 'choices') and final_response.choices:
                    content_analysis = final_response.choices[0].message.content or ""
                elif isinstance(final_response, str):
                    content_analysis = final_response
                else:
                    content_analysis = str(final_response)
            except Exception as e:
                content_analysis = f"[ERROR] å†…å®¹åˆ†æå¤±è´¥: {str(e)}"
                logger.error(f"å¤„ç†final_responseæ—¶å‡ºé”™: {e}, final_responseç±»å‹: {type(final_response)}")
            
            # ä½¿ç”¨é…ç½®åŒ–çš„æ ¼å¼å¤„ç†å™¨æ„å»ºçŸ¥è¯†åº“æ•°æ®
            # ä½¿ç”¨å½“å‰é»˜è®¤æ ¼å¼ç±»å‹ï¼ˆå¯é…ç½®ï¼‰
            format_type = "dfd"  # å¯ä»¥ä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è·å–
            
            # æ ¹æ®é…ç½®æå–çŸ¥è¯†åº“æ•°æ®
            extracted_data = format_processor.extract_knowledge(
                content_analysis, url, title
            )
            
            # å‡†å¤‡å…ƒæ•°æ®
            metadata = {
                "source_url": url,
                "title": title,
                "crawl_time": crawl_time,
                "crawl_time_human": crawl_time_human,
                "extraction_method": "åŸºäºé…ç½®æ–‡ä»¶çš„è‡ªåŠ¨æå–",
                "topic": tech_topic
            }
            
            # ç”ŸæˆJSONç»“æ„
            json_obj = format_processor.generate_json_structure(
                extracted_data, url, title
            )
            
            # ä¸ºäº†å…¼å®¹æ€§ï¼Œæå–å„ä¸ªç»„ä»¶çš„ç»Ÿè®¡ä¿¡æ¯
            dfd_concepts = extracted_data.get('dfd_concepts', [])
            dfd_rules = extracted_data.get('dfd_rules', [])
            dfd_patterns = extracted_data.get('dfd_patterns', [])
            dfd_cases = extracted_data.get('dfd_cases', [])
            dfd_nlp_mappings = extracted_data.get('dfd_nlp_mappings', [])
            
            # ç¡®ä¿ç»Ÿè®¡ä¿¡æ¯æ­£ç¡®
            if "statistics" not in json_obj:
                json_obj["statistics"] = {
                    "concepts_count": len(dfd_concepts),
                    "rules_count": len(dfd_rules),
                    "patterns_count": len(dfd_patterns),
                    "cases_count": len(dfd_cases),
                    "nlp_mappings_count": len(dfd_nlp_mappings)
                }
            
            # ä½¿ç”¨FormatProcessorç”ŸæˆMarkdownå†…å®¹
            markdown_content = format_processor.generate_markdown(
                extracted_data,
                {
                    'title': tech_topic,
                    'source_url': url,
                    'source_title': title,
                    'crawl_time': crawl_time_human,
                    'content_analysis': content_analysis
                }
            )
            md_lines = markdown_content.split('\n')
            # ä½¿ç”¨æ–°å­˜å‚¨æ¨¡å—ä¿å­˜ç½‘é¡µå†…å®¹å’Œå›¾ç‰‡
            saved_info = None
            try:
                saved_info = await storage.save_webpage(
                    url=url,
                    html_content=response.text,  # ä½¿ç”¨åŸå§‹HTMLå†…å®¹
                    title=title,
                    metadata=json_obj,  # å°†JSONå¯¹è±¡ä½œä¸ºå…ƒæ•°æ®
                    image_urls=image_urls,
                    client=client  # ä¼ é€’HTTPå®¢æˆ·ç«¯ç”¨äºä¸‹è½½å›¾ç‰‡
                )
                
                # è®°å½•ä¿å­˜ä¿¡æ¯
                if saved_info and saved_info.get('success'):
                    logger.info(f"ç½‘é¡µå†…å®¹å·²ä¿å­˜åˆ°: {saved_info['folder_path']}")
                    logger.info(f"ä¸‹è½½çš„å›¾ç‰‡: {saved_info['images_downloaded']}/{saved_info['total_images']}å¼ ")
                else:
                    logger.error(f"ä¿å­˜å¤±è´¥: {saved_info.get('error', 'æœªçŸ¥é”™è¯¯') if saved_info else 'ä¿å­˜ä¿¡æ¯ä¸ºç©º'}")
                
            except Exception as e:
                logger.error(f"ä½¿ç”¨æ–°å­˜å‚¨æ¨¡å—ä¿å­˜å¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°åŸæœ‰æ–¹å¼")
                saved_info = None
            
            # åŒæ—¶ä¿å­˜åˆ°åŸæœ‰çš„ç›®å½•ç»“æ„ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰
            json_dir = Path("shared_data/json_llm_ready")
            md_dir = Path("shared_data/markdown_llm_ready")
            json_dir.mkdir(parents=True, exist_ok=True)
            md_dir.mkdir(parents=True, exist_ok=True)
            
            legacy_json_path = json_dir / f"{tech_topic_clean}_{crawl_time}.json"
            legacy_md_path = md_dir / f"{tech_topic_clean}_{crawl_time}.md"
            
            with open(legacy_json_path, 'w', encoding='utf-8') as f:
                _json.dump(json_obj, f, ensure_ascii=False, indent=2)
            with open(legacy_md_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(md_lines))
            
            # è‡ªåŠ¨ä¿å­˜åˆ°çŸ¥è¯†åº“ç»“æ„åŒ–æ–‡ä»¶
            try:
                kb_result = await save_to_knowledge_base(_json.dumps(json_obj), tech_topic_clean)
                # Knowledge base save result logged
            except Exception as e:
                # Knowledge base save failed, error logged
                pass
            
            # ============= æ–°å¢æœ¬åœ°å­˜å‚¨ END =============
            
            # è¿”å›å¢å¼ºçš„ç»“æœï¼ŒåŒ…å«çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
            try:
                # å°è¯•è§£æå“åº”å†…å®¹
                if hasattr(final_response, 'choices') and final_response.choices:
                    result_summary = final_response.choices[0].message.content or "[ERROR] ç†è§£å¤±è´¥"
                elif isinstance(final_response, str):
                    result_summary = final_response
                else:
                    result_summary = str(final_response)
            except Exception as e:
                result_summary = f"[ERROR] å“åº”è§£æå¤±è´¥: {str(e)}"
            
            # æ·»åŠ çŸ¥è¯†åº“æå–ç»Ÿè®¡å’Œå­˜å‚¨ä¿¡æ¯
            try:
                # å°è¯•è·å–å­˜å‚¨ä¿¡æ¯
                if saved_info is not None and saved_info.get('success'):
                    storage_stats = f"\n\nğŸ—‚ï¸ **æ–°å­˜å‚¨ç»“æ„**:\n" \
                                  f"- ç½‘é¡µæ–‡ä»¶å¤¹: {saved_info['folder_path']}\n" \
                                  f"- HTMLæ–‡ä»¶: {saved_info['html_file']}\n" \
                                  f"- å…ƒæ•°æ®æ–‡ä»¶: {saved_info['metadata_file']}\n"
                    if saved_info['images_downloaded'] > 0:
                        storage_stats += f"- ä¸‹è½½çš„å›¾ç‰‡: {saved_info['images_downloaded']}/{saved_info['total_images']}å¼ \n"
                else:
                    storage_stats = "\n\nğŸ—‚ï¸ **å­˜å‚¨ä¿¡æ¯**: ä½¿ç”¨ä¼ ç»Ÿå­˜å‚¨æ–¹å¼\n"
            except Exception as e:
                storage_stats = f"\n\nğŸ—‚ï¸ **å­˜å‚¨ä¿¡æ¯**: å­˜å‚¨ä¿¡æ¯è·å–å¤±è´¥ - {str(e)}\n"
                
            kb_stats = f"\n\nğŸ“Š **çŸ¥è¯†åº“æå–ç»Ÿè®¡**:\n" \
                      f"- æ¦‚å¿µå®šä¹‰: {len(dfd_concepts)} ä¸ª\n" \
                      f"- è§„åˆ™æ¡ç›®: {len(dfd_rules)} ä¸ª\n" \
                      f"- æ¨¡å¼æ¨¡æ¿: {len(dfd_patterns)} ä¸ª\n" \
                      f"- æ¡ˆä¾‹ç¤ºä¾‹: {len(dfd_cases)} ä¸ª\n" \
                      f"- NLPæ˜ å°„: {len(dfd_nlp_mappings)} ä¸ª\n" + \
                      storage_stats + \
                      f"\nğŸ“ **å…¼å®¹æ€§æ–‡ä»¶ä½ç½®**:\n" \
                      f"- JSONæ•°æ®: {legacy_json_path}\n" \
                      f"- MarkdownæŠ¥å‘Š: {legacy_md_path}\n" \
                      f"- çŸ¥è¯†åº“æ–‡ä»¶: shared_data/knowledge_base/{tech_topic_clean}_*.json"
            
            # ç¼“å­˜URLå’Œå†…å®¹
            try:
                dedup_instance.add_url_cache(url, title)
                if dedup_instance.is_content_duplicate(result_summary):
                    # å†…å®¹é‡å¤ä½†URLä¸åŒï¼Œè®°å½•æ—¥å¿—
                    logger.info(f"æ£€æµ‹åˆ°å†…å®¹é‡å¤ä½†URLä¸åŒ: {url}")
                else:
                    dedup_instance.add_content_cache(result_summary, title, url)
            except Exception as cache_error:
                logger.warning(f"ç¼“å­˜å¤±è´¥: {str(cache_error)}")
            
            return result_summary + kb_stats

    except Exception as e:
        return f"[ERROR] å›¾æ–‡æå–å¤±è´¥ {str(e)}"

@mcp.tool()
def save_to_knowledge_base(json_data: str, base_filename: str = None, format_type: str = "dfd") -> str:
    """
    ä½¿ç”¨é€šç”¨æ ¼å¼å¤„ç†å™¨ä¿å­˜çŸ¥è¯†åº“æ•°æ®åˆ°ç‹¬ç«‹æ–‡ä»¶ä¸­ï¼Œæ”¯æŒå¤šç§æ ¼å¼ç±»å‹
    """
    try:
        import json as _json
        
        # è§£æè¾“å…¥çš„JSONæ•°æ®
        data = _json.loads(json_data) if isinstance(json_data, str) else json_data
        
        # åˆ›å»ºæŒ‡å®šæ ¼å¼ç±»å‹çš„æ ¼å¼å¤„ç†å™¨
        processor = FormatProcessor(format_type=format_type)
        
        # ä½¿ç”¨æ ¼å¼å¤„ç†å™¨ä¿å­˜æ•°æ®
        result = processor.save_knowledge_base(data, base_filename)
        
        if result["success"]:
            saved_files = result["saved_files"]
            summary_file = result["summary_file"]
            statistics = result["statistics"]
            
            # æ„å»ºæˆåŠŸæ¶ˆæ¯
            success_msg = f"[SUCCESS] {processor.get_format_name()}æ•°æ®å·²ä¿å­˜åˆ°{len(saved_files)}ä¸ªç‹¬ç«‹æ–‡ä»¶:\n"
            success_msg += "\n".join([f"- {file}" for file in saved_files])
            success_msg += f"\n\næ±‡æ€»æŠ¥å‘Š: {summary_file}\n\nç»Ÿè®¡ä¿¡æ¯:\n"
            
            # åŠ¨æ€ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            for key, value in statistics.items():
                if key.endswith('_count'):
                    category_name = key.replace('_count', '').replace('dfd_', '')
                    success_msg += f"- {category_name}: {value} ä¸ª\n"
            
            return success_msg
        else:
            return f"[ERROR] ä¿å­˜çŸ¥è¯†åº“æ•°æ®å¤±è´¥: {result['error']}"
        
    except Exception as e:
        return f"[ERROR] ä¿å­˜çŸ¥è¯†åº“æ•°æ®å¤±è´¥: {str(e)}"

@mcp.tool()
async def search_and_scrape(keyword: str, top_k: int = 12) -> str:
    """
    æ ¹æ®å…³é”®è¯æœç´¢ç½‘é¡µï¼Œå¹¶æŠ“å–å‰å‡ ä¸ªç½‘é¡µçš„å›¾æ–‡ä¿¡æ¯ã€‚
    """
    try:
        # å°è¯•æœç´¢
        logger.info(f"å¼€å§‹æœç´¢å…³é”®è¯: {keyword}")
        links = search_web(keyword, max_results=top_k)
        if not links:
            logger.info("æœªæ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœ")
            return "âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç½‘é¡µå–µ~"

        logger.info(f"æ‰¾åˆ° {len(links)} ä¸ªæœç´¢ç»“æœï¼Œå¼€å§‹å¤„ç†...")
        # æŠ“å–å†…å®¹
        summaries = []
        for i, url in enumerate(links):
            try:
                logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1} ä¸ªé“¾æ¥: {url}")
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if i > 0:
                    await asyncio.sleep(1)
                
                summary = await scrape_webpage(url)
                summaries.append(f"ğŸ”— ç½‘é¡µ {i+1}: {url}\n{summary}\n")
                logger.info(f"ç¬¬ {i+1} ä¸ªé“¾æ¥å¤„ç†å®Œæˆ")
            except Exception as e:
                logger.warning(f"å¤„ç†ç½‘é¡µ {url} æ—¶å‡ºé”™: {str(e)}")
                summaries.append(f"ğŸ”— ç½‘é¡µ {i+1}: {url}\nâŒ å¤„ç†å¤±è´¥å–µ~ {str(e)}\n")

        if not summaries:
            return "âš ï¸ æ‰€æœ‰ç½‘é¡µå¤„ç†éƒ½å¤±è´¥äº†å–µ~"

        # æ·»åŠ å»é‡ç»Ÿè®¡ä¿¡æ¯
        try:
            dedup_instance = get_deduplication_instance()
            stats = dedup_instance.get_stats()
            dedup_stats = f"\n\nğŸ“Š **å»é‡ç»Ÿè®¡ä¿¡æ¯**:\n" \
                         f"- URLç¼“å­˜æ•°é‡: {stats.get('url_count', 0)} ä¸ª\n" \
                         f"- å†…å®¹ç¼“å­˜æ•°é‡: {stats.get('content_count', 0)} ä¸ª\n" \
                         f"- è·³è¿‡é‡å¤URL: {stats.get('url_duplicates', 0)} æ¬¡\n" \
                         f"- æ£€æµ‹é‡å¤å†…å®¹: {stats.get('content_duplicates', 0)} æ¬¡"
            return "\n\n".join(summaries) + dedup_stats
        except Exception as e:
            logger.warning(f"è·å–å»é‡ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return "\n\n".join(summaries)
        
    except Exception as e:
        logger.error(f"æœç´¢æˆ–æŠ“å–è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        return f"[ERROR] æœç´¢æˆ–æŠ“å–å¤±è´¥ {str(e)}"

@mcp.tool()
def manage_web_deduplication(action: str = "stats", days: int = 7) -> str:
    """
    ç®¡ç†ç½‘é¡µå»é‡ç³»ç»Ÿ
    
    Args:
        action: æ“ä½œç±»å‹ ("stats" - æŸ¥çœ‹ç»Ÿè®¡, "clean" - æ¸…ç†ç¼“å­˜, "reset" - é‡ç½®æ‰€æœ‰)
        days: æ¸…ç†å¤šå°‘å¤©å‰çš„ç¼“å­˜ (ä»…åœ¨action="clean"æ—¶æœ‰æ•ˆ)
    """
    try:
        dedup_instance = get_deduplication_instance()
        
        if action == "stats":
            stats = dedup_instance.get_stats()
            return json.dumps({
                "status": "success",
                "action": "ç»Ÿè®¡ä¿¡æ¯",
                "data": {
                    "URLç¼“å­˜æ•°é‡": f"{stats.get('url_count', 0)} ä¸ª",
                    "å†…å®¹ç¼“å­˜æ•°é‡": f"{stats.get('content_count', 0)} ä¸ª",
                    "è·³è¿‡é‡å¤URLæ¬¡æ•°": f"{stats.get('url_duplicates', 0)} æ¬¡",
                    "æ£€æµ‹é‡å¤å†…å®¹æ¬¡æ•°": f"{stats.get('content_duplicates', 0)} æ¬¡",
                    "æ•°æ®åº“æ–‡ä»¶": "web_cache.db"
                }
            }, ensure_ascii=False, indent=2)
            
        elif action == "clean":
            cleaned_count = dedup_instance.clean_old_cache(days)
            return json.dumps({
                "status": "success",
                "action": "æ¸…ç†ç¼“å­˜",
                "message": f"å·²æ¸…ç† {days} å¤©å‰çš„ç¼“å­˜",
                "cleaned_count": cleaned_count
            }, ensure_ascii=False, indent=2)
            
        elif action == "reset":
            dedup_instance.reset_cache()
            return json.dumps({
                "status": "success",
                "action": "é‡ç½®ç¼“å­˜",
                "message": "æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º"
            }, ensure_ascii=False, indent=2)
            
        else:
            return json.dumps({
                "status": "error",
                "message": "æ— æ•ˆçš„æ“ä½œç±»å‹ï¼Œæ”¯æŒ: stats, clean, reset"
            }, ensure_ascii=False, indent=2)
            
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"å»é‡ç³»ç»Ÿç®¡ç†å¤±è´¥: {str(e)}"
        }, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    # åˆå§‹åŒ–æ•°æ®åº“
    # asyncio.run(init_db()) # åˆ é™¤æ‰€æœ‰æ•°æ®åº“ç›¸å…³çš„è°ƒç”¨å’Œé€»è¾‘
    
    # å¯åŠ¨MCPæœåŠ¡å™¨
    # MCP server starting...
    
    # å¦‚æœå¯ç”¨äº†Torï¼Œè‡ªåŠ¨å¯åŠ¨Torä»£ç†
    if USE_TOR and tor_manager:
        # Tor proxy enabled, auto-starting...
        success = tor_manager.start_tor()
        if success:
            # SUCCESS: Tor proxy auto-start successful
            pass
        else:
            # ERROR: Tor proxy auto-start failed, using normal network connection
            pass
    
    try:
        mcp.run(transport='stdio')
    finally:
        # ç¨‹åºé€€å‡ºæ—¶æ¸…ç†Torèµ„æº
        if USE_TOR and tor_manager:
            # Cleaning up Tor resources...
            tor_manager.cleanup()
    # åœ¨ç¨‹åºé€€å‡ºå‰å…³é—­æ•°æ®åº“è¿æ¥æ± 
    # asyncio.run(close_pool()) # åˆ é™¤æ‰€æœ‰æ•°æ®åº“ç›¸å…³çš„è°ƒç”¨å’Œé€»è¾‘