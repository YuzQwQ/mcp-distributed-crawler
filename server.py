# -*- coding: utf-8 -*-
import base64
from io import BytesIO
from PIL import Image
import json
import os
from dotenv import load_dotenv
import httpx
from typing import Any, List, Dict
from mcp.server.fastmcp import FastMCP
from format_processor import FormatProcessor
try:
    from httpx_socks import AsyncProxyTransport
    SOCKS_AVAILABLE = True
except ImportError:
    SOCKS_AVAILABLE = False
    # Warning: httpx-socks not installed, SOCKS proxy functionality will be unavailable
from openai import OpenAI
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
import asyncio
from urllib.parse import urljoin, urlparse
import time
import requests  # æ·»åŠ requestsåº“ç”¨äºSerpAPIè¯·æ±‚
import datetime
import re
from pathlib import Path
import subprocess
import json
import requests

import random
import threading

# å¯¼å…¥MySQLæ•°æ®åº“å·¥å…·
# from mysql_db_utils import init_db, save_to_db, close_pool

load_dotenv()
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"), base_url=os.getenv("BASE_URL"))
model = os.getenv("MODEL")

mcp = FastMCP("WebScrapingServer")

# åˆå§‹åŒ–æ ¼å¼å¤„ç†å™¨
format_processor = FormatProcessor()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
BASE_URL = os.getenv("BASE_URL")
MODEL = os.getenv("MODEL")

SERPAPI_API_KEY = os.getenv("SERPAPI_API_KEY")  # SerpAPIçš„APIå¯†é’¥

# Torä»£ç†é…ç½®
USE_TOR = os.getenv("USE_TOR", "false").lower() == "true"
TOR_SOCKS_PORT = int(os.getenv("TOR_SOCKS_PORT", "9050"))
TOR_CONTROL_PORT = int(os.getenv("TOR_CONTROL_PORT", "9051"))
TOR_PASSWORD = os.getenv("TOR_PASSWORD", "")
TOR_EXECUTABLE_PATH = os.getenv("TOR_EXECUTABLE_PATH", "tor")  # Torå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„

# å…¨å±€è‡ªå®šä¹‰headerså’Œå¯é€‰Cookie
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Accept-Encoding": "gzip, deflate, br",
    "Referer": "https://www.zhihu.com/",
    "Connection": "keep-alive",
}
DEFAULT_COOKIES = os.getenv("SCRAPER_COOKIES", "")  # å¯åœ¨.envä¸­é…ç½®Cookieå­—ç¬¦ä¸²

def parse_cookies(cookie_str):
    cookies = {}
    for item in cookie_str.split(';'):
        if '=' in item:
            k, v = item.split('=', 1)
            cookies[k.strip()] = v.strip()
    return cookies


class TorManager:
    """Torä»£ç†ç®¡ç†å™¨ï¼ˆä½¿ç”¨subprocessè‡ªåŠ¨å¯åŠ¨ï¼‰"""
    
    def __init__(self):
        self.tor_process = None
        self.is_running = False
        self.lock = threading.Lock()
        self.log_file = None
    
    def start_tor(self):
        """ä½¿ç”¨subprocesså¯åŠ¨Torè¿›ç¨‹"""
        if self.is_running:
            return True
            
        try:
            with self.lock:
                if self.is_running:
                    return True
                    
                # Starting Tor process...
                
                # æ£€æŸ¥Torå¯æ‰§è¡Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                tor_cmd = TOR_EXECUTABLE_PATH
                if not self._check_tor_executable(tor_cmd):
                    # ERROR: Tor executable not found
                    return False
                
                # æ„å»ºTorå¯åŠ¨å‘½ä»¤
                cmd = [
                    tor_cmd,
                    "--SocksPort", str(TOR_SOCKS_PORT),
                    "--ControlPort", str(TOR_CONTROL_PORT),
                    "--DataDirectory", "./tor_data",
                    "--Log", "notice stdout"
                ]
                
                # å¦‚æœè®¾ç½®äº†å¯†ç ï¼Œæ·»åŠ å¯†ç é…ç½®
                if TOR_PASSWORD:
                    cmd.extend(["--HashedControlPassword", self._hash_password(TOR_PASSWORD)])
                
                # å¯åŠ¨Torè¿›ç¨‹ï¼Œé‡å®šå‘è¾“å‡ºåˆ°æ–‡ä»¶é¿å…ç¼–ç é—®é¢˜
                log_file = open('./tor_data/tor.log', 'w', encoding='utf-8', errors='ignore')
                self.tor_process = subprocess.Popen(
                    cmd,
                    stdout=log_file,
                    stderr=subprocess.STDOUT,
                    creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                )
                self.log_file = log_file
                
                # ç­‰å¾…Torå¯åŠ¨
                if self._wait_for_tor_ready():
                    self.is_running = True
                    # SUCCESS: Tor started, SOCKS proxy port ready
                    return True
                else:
                    # ERROR: Tor startup timeout
                    self.cleanup()
                    return False
                    
        except Exception as e:
            # ERROR: Failed to start Tor
            self.cleanup()
            return False
    
    def _check_tor_executable(self, tor_cmd):
        """æ£€æŸ¥Torå¯æ‰§è¡Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        try:
            result = subprocess.run([tor_cmd, "--version"], 
                                  capture_output=True, 
                                  timeout=5,
                                  creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0)
            return result.returncode == 0
        except:
            return False
    
    def _hash_password(self, password):
        """ç”ŸæˆTorå¯†ç å“ˆå¸Œï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            import hashlib
            import base64
            salt = os.urandom(8)
            key = hashlib.pbkdf2_hmac('sha1', password.encode(), salt, 1000, 20)
            return base64.b64encode(salt + key).decode()
        except:
            return None
    
    def _wait_for_tor_ready(self, timeout=30):
        """ç­‰å¾…Torå‡†å¤‡å°±ç»ª"""
        import socket
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                # å°è¯•è¿æ¥SOCKSç«¯å£
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(1)
                result = sock.connect_ex(('127.0.0.1', TOR_SOCKS_PORT))
                sock.close()
                
                if result == 0:
                    return True
                    
            except:
                pass
                
            time.sleep(1)
            
        return False
    
    def new_identity(self):
        """è¯·æ±‚æ–°çš„Torèº«ä»½ï¼ˆé€šè¿‡é‡å¯å®ç°ï¼‰"""
        if not self.is_running:
            return False
            
        try:
            # Changing Tor identity...
            self.cleanup()
            time.sleep(2)
            return self.start_tor()
        except Exception as e:
            # Failed to change Tor identity
            return False
    
    def get_proxy_config(self):
        """è·å–ä»£ç†é…ç½®"""
        if not self.is_running:
            return None
        return f"socks5://127.0.0.1:{TOR_SOCKS_PORT}"
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.tor_process:
                self.tor_process.terminate()
                try:
                    self.tor_process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    self.tor_process.kill()
                self.tor_process = None
                
            self.is_running = False
            
            # å…³é—­æ—¥å¿—æ–‡ä»¶
            if hasattr(self, 'log_file') and self.log_file:
                try:
                    self.log_file.close()
                    self.log_file = None
                except Exception as e:
                    # Error closing log file
                    pass
                    
            # Tor process stopped
        except Exception as e:
            # Error cleaning up Tor resources
            pass


# å…¨å±€Torç®¡ç†å™¨å®ä¾‹
tor_manager = TorManager() if USE_TOR else None


def get_http_client_config():
    """è·å–HTTPå®¢æˆ·ç«¯é…ç½®"""
    config = {
        "timeout": 30.0,
        "follow_redirects": True,
    }
    
    if USE_TOR and tor_manager and tor_manager.is_running and SOCKS_AVAILABLE:
        proxy_url = tor_manager.get_proxy_config()
        if proxy_url:
            try:
                # ä½¿ç”¨httpx-socksçš„AsyncProxyTransportæ¥æ”¯æŒSOCKS5ä»£ç†
                transport = AsyncProxyTransport.from_url(proxy_url)
                config["transport"] = transport
                print(f"ä½¿ç”¨Torä»£ç† (transport): {proxy_url}")
            except Exception as e:
                print(f"ä»£ç†é…ç½®é”™è¯¯: {e}")
                print("å°†ä½¿ç”¨æ™®é€šç½‘ç»œè¿æ¥")
    elif USE_TOR and tor_manager and tor_manager.is_running and not SOCKS_AVAILABLE:
        print("è­¦å‘Š: æ£€æµ‹åˆ°Torä»£ç†å·²å¯ç”¨ï¼Œä½†httpx-socksæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨SOCKSä»£ç†")
        print("è¯·è¿è¡Œ: pip install httpx-socks[asyncio]")
    
    return config


def search_web(keyword: str, max_results=12):
    """ä½¿ç”¨SerpAPIæœç´¢ç½‘é¡µ"""
    try:
        # æ£€æŸ¥APIå¯†é’¥
        if not SERPAPI_API_KEY:
            print("é”™è¯¯: æœªæ‰¾åˆ°SerpAPI APIå¯†é’¥ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®SERPAPI_API_KEY")
            return []

        # DFDä¸“ä¸šå…³é”®è¯æ‰©å±•ï¼ˆç²¾ç®€ç‰ˆï¼Œåªä¿ç•™æ ¸å¿ƒæœ¯è¯­ï¼‰
        dfd_keywords = ["æ•°æ®æµå›¾", "DFD", "Data Flow Diagram", "ç»“æ„åŒ–åˆ†æ"]
        
        # æ„å»ºå¢å¼ºçš„æœç´¢æŸ¥è¯¢
        enhanced_queries = [keyword]
        for dfd_term in dfd_keywords:
            if dfd_term.lower() not in keyword.lower():
                enhanced_queries.append(f"{keyword} {dfd_term}")
        
        all_results = []
        
        # å¯¹æ¯ä¸ªå¢å¼ºæŸ¥è¯¢è¿›è¡Œæœç´¢
        for query in enhanced_queries[:3]:  # é™åˆ¶æŸ¥è¯¢æ•°é‡é¿å…è¿‡å¤šè¯·æ±‚
            # è®¾ç½®æœç´¢å‚æ•°
            params = {
                "engine": "google",  # å¯é€‰ï¼šgoogle, bing, baidu
                "q": query,
                "api_key": SERPAPI_API_KEY,
                "num": max_results // len(enhanced_queries[:3]) + 2,  # åˆ†é…æœç´¢æ•°é‡
                "count": max_results // len(enhanced_queries[:3]) + 2,  # Bingå‚æ•°
                "hl": "zh-cn",  # è®¾ç½®è¯­è¨€ä¸ºä¸­æ–‡
                "gl": "cn",  # è®¾ç½®åœ°åŒºä¸ºä¸­å›½
            }

            print(f"ä½¿ç”¨SerpAPIæœç´¢: {query}")

            # å‘é€è¯·æ±‚åˆ°SerpAPI
            response = requests.get("https://serpapi.com/search", params=params)
            response.raise_for_status()

            # è§£æJSONå“åº”
            try:
                data = response.json()
                # ç¡®ä¿dataæ˜¯å­—å…¸ç±»å‹
                if not isinstance(data, dict):
                    print(f"[ERROR] SerpAPIå“åº”æ ¼å¼å¼‚å¸¸: {type(data)}")
                    continue
            except Exception as e:
                print(f"[ERROR] SerpAPI JSONè§£æå¤±è´¥: {e}")
                continue

            # å¤„ç†æœ‰æœºæœç´¢ç»“æœ
            if "organic_results" in data:
                for result in data["organic_results"]:
                    if "link" in result:
                        url = result["link"]
                        # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤
                        if url not in all_results:
                            # DFDç›¸å…³æ€§æ£€æŸ¥ï¼ˆæ›´ä¸¥æ ¼çš„åŒ¹é…ï¼‰
                            title = result.get("title", "").lower()
                            snippet = result.get("snippet", "").lower()
                            dfd_terms = ["æ•°æ®æµå›¾", "dfd", "data flow diagram", "ç»“æ„åŒ–åˆ†æ"]
                            
                            # åªæœ‰æ ‡é¢˜æˆ–æ‘˜è¦åŒ…å«DFDæ ¸å¿ƒæœ¯è¯­æ‰æ·»åŠ 
                            is_dfd_related = any(term in title or term in snippet for term in dfd_terms)
                            if is_dfd_related:
                                all_results.append(url)
                                print(f"æ·»åŠ æœç´¢ç»“æœ: {url} (DFDç›¸å…³)")
                            else:
                                print(f"è·³è¿‡éç›¸å…³ç»“æœ: {result.get('title', 'Unknown')}")
                                
                        if len(all_results) >= max_results:
                            break
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            if query != enhanced_queries[:3][-1]:  # ä¸æ˜¯æœ€åä¸€ä¸ªæŸ¥è¯¢
                time.sleep(0.5)
        
        print(f"æ€»å…±æ‰¾åˆ° {len(all_results)} ä¸ªå»é‡åçš„æœç´¢ç»“æœ")
        return all_results

    except Exception as e:
        print(f"SerpAPIæœç´¢å¤±è´¥: {str(e)}")
        return []


# çŸ¥ä¹åçˆ¬è™«ç›¸å…³ä»£ç å·²ç¦ç”¨
# def is_zhihu(url):
#     return 'zhihu.com' in urlparse(url).netloc

# def ensure_zhihu_cookie():
#     cookie_path = Path("cookies/zhihu.json")
#     # å¦‚æœCookieä¸å­˜åœ¨æˆ–å¤ªæ—§ï¼ˆå¦‚1å¤©ï¼‰ï¼Œè‡ªåŠ¨åˆ·æ–°
#     if not cookie_path.exists() or (cookie_path.stat().st_mtime < (time.time() - 86400)):
#         print("æ­£åœ¨è‡ªåŠ¨ç™»å½•çŸ¥ä¹è·å–Cookie...")
#         subprocess.run(["python", "login_and_save_cookie.py"], check=True)
#     with open(cookie_path, "r", encoding="utf-8") as f:
#         return json.load(f)

# def get_cookies_for_url(url):
#     if is_zhihu(url):
#         return ensure_zhihu_cookie()
#     # å¯æ‰©å±•å…¶ä»–ç«™ç‚¹
#     return None

def get_cookies_for_url(url):
    # çŸ¥ä¹åçˆ¬è™«åŠŸèƒ½å·²ç¦ç”¨
    return None


# æ³¨æ„ï¼šåŸæœ‰çš„ç¡¬ç¼–ç æå–å‡½æ•°å·²è¢«FormatProcessoræ›¿ä»£
# ç°åœ¨é€šè¿‡é…ç½®æ–‡ä»¶é©±åŠ¨ï¼Œæä¾›æ›´å¥½çš„å¯æ‰©å±•æ€§


@mcp.tool()
def start_tor_proxy() -> str:
    """å¯åŠ¨Torä»£ç†æœåŠ¡"""
    if not USE_TOR:
        return "Torä»£ç†åŠŸèƒ½æœªå¯ç”¨ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®USE_TOR=trueæ¥å¯ç”¨ã€‚"
    
    if not tor_manager:
        return "Torç®¡ç†å™¨æœªåˆå§‹åŒ–ã€‚"
    
    if tor_manager.is_running:
        return "Torä»£ç†å·²ç»åœ¨è¿è¡Œä¸­ã€‚"
    
    success = tor_manager.start_tor()
    if success:
        return f"[SUCCESS] Torä»£ç†å¯åŠ¨æˆåŠŸï¼SOCKSä»£ç†ç«¯å£: {TOR_SOCKS_PORT}"
    else:
        return "[ERROR] Torä»£ç†å¯åŠ¨å¤±è´¥ã€‚è¯·æ£€æŸ¥Toræ˜¯å¦å·²å®‰è£…å¹¶é…ç½®æ­£ç¡®ã€‚"


@mcp.tool()
def stop_tor_proxy() -> str:
    """åœæ­¢Torä»£ç†æœåŠ¡"""
    if not USE_TOR or not tor_manager:
        return "Torä»£ç†åŠŸèƒ½æœªå¯ç”¨ã€‚"
    
    if not tor_manager.is_running:
        return "Torä»£ç†æœªè¿è¡Œã€‚"
    
    tor_manager.cleanup()
    return "[SUCCESS] Torä»£ç†å·²åœæ­¢ã€‚"


@mcp.tool()
def change_tor_identity() -> str:
    """æ›´æ¢Torèº«ä»½ï¼ˆè·å–æ–°IPåœ°å€ï¼‰"""
    if not USE_TOR or not tor_manager:
        return "Torä»£ç†åŠŸèƒ½æœªå¯ç”¨ã€‚"
    
    if not tor_manager.is_running:
        return "Torä»£ç†æœªè¿è¡Œã€‚è¯·å…ˆå¯åŠ¨Torä»£ç†ã€‚"
    
    success = tor_manager.new_identity()
    if success:
        return "[SUCCESS] å·²æˆåŠŸæ›´æ¢Torèº«ä»½ï¼ŒIPåœ°å€å·²æ›´æ–°ã€‚"
    else:
        return "[ERROR] æ›´æ¢Torèº«ä»½å¤±è´¥ã€‚"


@mcp.tool()
def get_tor_status() -> str:
    """è·å–Torä»£ç†çŠ¶æ€"""
    if not USE_TOR:
        return "Torä»£ç†åŠŸèƒ½æœªå¯ç”¨ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®USE_TOR=trueæ¥å¯ç”¨ã€‚"
    
    if not tor_manager:
        return "Torç®¡ç†å™¨æœªåˆå§‹åŒ–ã€‚"
    
    if tor_manager.is_running:
        proxy_url = tor_manager.get_proxy_config()
        return f"Torä»£ç†æ­£åœ¨è¿è¡Œ\nä»£ç†åœ°å€: {proxy_url}\nSOCKSç«¯å£: {TOR_SOCKS_PORT}\næ§åˆ¶ç«¯å£: {TOR_CONTROL_PORT}"
    else:
        return "Torä»£ç†æœªè¿è¡Œã€‚"

@mcp.tool()
async def scrape_webpage(url: str, headers=None, cookies=None) -> str:
    """
    æŠ“å–ç½‘é¡µæ–‡æœ¬ + å›¾ç‰‡åˆ†æï¼ˆé€šè¿‡è§†è§‰æ¨¡å‹ï¼‰+ ä½¿ç”¨ä¸»æ¨¡å‹æ€»ç»“ã€‚
    """
    headers = headers or DEFAULT_HEADERS
    # çŸ¥ä¹åçˆ¬è™«åŠŸèƒ½å·²ç¦ç”¨ - è‡ªåŠ¨åˆ¤æ–­çŸ¥ä¹ç­‰ç«™ç‚¹ï¼Œè‡ªåŠ¨è·å–Cookie
    # cookies = cookies or get_cookies_for_url(url) or (parse_cookies(DEFAULT_COOKIES) if DEFAULT_COOKIES else None)
    cookies = cookies or (parse_cookies(DEFAULT_COOKIES) if DEFAULT_COOKIES else None)

    # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
    SUPPORTED_IMAGE_FORMATS = ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp', '.svg']
    MAX_IMAGE_SIZE = 5 * 1024 * 1024  # 5MB

    def normalize_image_url(base_url: str, img_url: str) -> str:
        if img_url.startswith(('http://', 'https://')):
            return img_url
        elif img_url.startswith('//'):
            return 'https:' + img_url
        elif img_url.startswith('/'):
            return urljoin(base_url, img_url)
        else:
            return urljoin(base_url, img_url)

    async def download_image_with_retry(client, img_url: str, max_retries: int = 3) -> bytes:
        for attempt in range(max_retries):
            try:
                response = await client.get(img_url, timeout=10.0)
                if response.status_code != 200:
                    # Image request failed
                    return None
                response.raise_for_status()
                return response.content
            except Exception as e:
                if attempt == max_retries - 1:
                    # Image download failed
                    return None
                await asyncio.sleep(1)

    async def is_valid_image_size(client, img_url: str) -> bool:
        try:
            async with client.stream('GET', img_url) as response:
                content_length = response.headers.get('content-length')
                if content_length and int(content_length) > MAX_IMAGE_SIZE:
                    return False
                return True
        except:
            return False

    async def is_valid_image_tag(img_tag) -> bool:
        # è¿‡æ»¤æå°å›¾ç‰‡ã€iconã€å¹¿å‘Šç­‰
        src = img_tag.get('src', '')
        if not src:
            return False
        # è¿‡æ»¤å¸¸è§icon/å¹¿å‘Šå…³é”®è¯
        lower_src = src.lower()
        if any(x in lower_src for x in ['logo', 'icon', 'avatar', 'ad', 'ads', 'spacer', 'blank', 'tracker']):
            return False
        # è¿‡æ»¤æå°å›¾ç‰‡ï¼ˆå¦‚å®½é«˜<32pxï¼‰
        try:
            width = int(img_tag.get('width', 0))
            height = int(img_tag.get('height', 0))
            if width and height and (width < 32 or height < 32):
                return False
        except Exception:
            pass
        return True

    async def get_image_description(client, image_data: bytes, max_retries: int = 2) -> str:
        for attempt in range(max_retries):
            try:
                # å¤„ç†å›¾ç‰‡æ•°æ®
                image = Image.open(BytesIO(image_data)).convert("RGB")
                buffer = BytesIO()
                image.save(buffer, format="JPEG", quality=85)  # é™ä½è´¨é‡ä»¥åŠ å¿«ä¼ è¾“
                b64_img = base64.b64encode(buffer.getvalue()).decode("utf-8")
                
                # è°ƒç”¨è§†è§‰æ¨¡å‹
                visual_payload = {
                    "model": os.getenv("VISUAL_MODEL", "Pro/Qwen/Qwen2.5-VL-7B-Instruct"),
                    "messages": [{"role": "user", "content": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"}],
                    "image": b64_img
                }
                
                # ä½¿ç”¨ç¡…åŸºæµåŠ¨çš„API
                VISUAL_API_URL = os.getenv("VISUAL_API_URL", "https://api.siliconflow.cn/v1/chat/completions")
                visual_response = await client.post(
                    VISUAL_API_URL,
                    json=visual_payload,
                    timeout=30.0,  # å¢åŠ è¶…æ—¶æ—¶é—´
                    headers={
                        "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
                        "Content-Type": "application/json"
                    }
                )
                try:
                    visual_json = visual_response.json()
                    if isinstance(visual_json, dict):
                        return (
                            visual_json.get("message", {}).get("content") or
                            visual_json.get("choices", [{}])[0].get("message", {}).get("content") or
                            "(è§†è§‰æ¨¡å‹æœªè¿”å›æœ‰æ•ˆæè¿°)"
                        ).strip()
                    else:
                        return f"(è§†è§‰æ¨¡å‹è¿”å›å¼‚å¸¸æ ¼å¼: {str(visual_json)})"
                except Exception as json_error:
                    return f"(è§†è§‰æ¨¡å‹JSONè§£æå¤±è´¥: {str(json_error)})"
                
            except Exception as e:
                if attempt == max_retries - 1:
                    return f"å›¾ç‰‡è¯†åˆ«å¤±è´¥ï¼ˆ{str(e)}ï¼‰"
                await asyncio.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•

    try:
        # è·å–HTTPå®¢æˆ·ç«¯é…ç½®ï¼ˆåŒ…å«Torä»£ç†è®¾ç½®ï¼‰
        client_config = get_http_client_config()
        client_config.update({"headers": headers, "cookies": cookies})
        
        async with httpx.AsyncClient(**client_config) as client:
            # Step 1: æŠ“ç½‘é¡µ
            response = await client.get(url)
            response.raise_for_status()
            
            # ç»Ÿä¸€ä½¿ç”¨UTF-8ç¼–ç å¤„ç†ï¼Œä¸æ•°æ®åº“ä¿æŒä¸€è‡´çš„ç¼–ç ï¼ˆæ•°æ®åº“ä½¿ç”¨utf8mb4ï¼Œæ˜¯UTF-8çš„è¶…é›†ï¼‰
            # è¿™æ ·å¯ä»¥é¿å…ç¼–ç è½¬æ¢é—®é¢˜ï¼Œç®€åŒ–å¤„ç†æµç¨‹
            response.encoding = "utf-8"  # å¼ºåˆ¶æŒ‡å®šç¼–ç ä¸ºutf-8
            
            try:
                soup = BeautifulSoup(response.text, "html.parser")
                
                for tag in soup(["script", "style"]):
                    tag.decompose()

                # Step 2: æå–å…¨ç½‘é¡µæ­£æ–‡å†…å®¹
                title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
                headings = [h.get_text(strip=True) for h in soup.find_all(['h1', 'h2'])]
                description = next((m.get("content") for m in soup.find_all("meta", attrs={"name": "description"})), "æ— æè¿°")
                
                # æå–ä¸»è¦å†…å®¹
                main_content = []
                for p in soup.find_all(['p', 'div', 'article']):
                    text = p.get_text(strip=True)
                    if text and len(text) > 20:  # åªä¿ç•™æœ‰æ„ä¹‰çš„æ–‡æœ¬
                        main_content.append(text)

                main_text = f"ã€æ ‡é¢˜ã€‘{title}\nã€æè¿°ã€‘{description}\nã€ç»“æ„ã€‘{headings}\n\n" + "\n".join(main_content)
            except Exception as e:
                print(f"[ERROR] ç½‘é¡µè§£æå¤±è´¥ {url}: {e}")
                return f"{{\"error\": \"ç½‘é¡µè§£æå¤±è´¥: {str(e)}\", \"url\": \"{url}\"}}"

            # Step 3: å°è¯•å¤„ç†å›¾ç‰‡
            img_descriptions = []
            try:
                img_tags = soup.find_all("img", src=True) if 'soup' in locals() else []
                seen_urls = set()
                valid_imgs = []
                for img_tag in img_tags:
                    if not await is_valid_image_tag(img_tag):
                        continue
                    img_url = img_tag["src"]
                    img_url = normalize_image_url(url, img_url)
                    if not any(img_url.lower().endswith(ext) for ext in SUPPORTED_IMAGE_FORMATS):
                        continue
                    if img_url in seen_urls:
                        continue
                    seen_urls.add(img_url)
                    valid_imgs.append(img_url)
                    if len(valid_imgs) >= 5:
                        break
                for i, img_url in enumerate(valid_imgs):
                    try:
                        img_data = await download_image_with_retry(client, img_url)
                        if not img_data:
                            continue  # è·³è¿‡æ— æ•ˆå›¾ç‰‡
                        vision_caption = await get_image_description(client, img_data)
                        if vision_caption and not vision_caption.startswith("å›¾ç‰‡è¯†åˆ«å¤±è´¥"):
                            img_descriptions.append(f"ç¬¬{i+1}å¼ å›¾ï¼š{vision_caption}")
                    except Exception as e:
                        print(f"å¤„ç†å›¾ç‰‡ {img_url} æ—¶å‡ºé”™: {str(e)}")
                        continue
            except Exception as e:
                print(f"å›¾ç‰‡å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}")

            # Step 4: æ•´åˆå›¾æ–‡è¾“å…¥
            all_desc = "\n".join(img_descriptions) if img_descriptions else "æœªè¯†åˆ«å‡ºå›¾ç‰‡å†…å®¹"

            # æ ¹æ®æ˜¯å¦æœ‰å›¾ç‰‡æè¿°è°ƒæ•´æç¤ºè¯
            if img_descriptions:
                final_prompt = (
                    f"è¯·æ€»ç»“è¿™ä¸ªç½‘é¡µçš„å†…å®¹ï¼Œç»“åˆä»¥ä¸‹æ–‡æœ¬å’Œå›¾ç‰‡æè¿°ï¼š\n\n"
                    f"ğŸ“„ æ–‡æœ¬éƒ¨åˆ†ï¼š\n{main_text}\n\n"
                    f"ğŸ–¼ å›¾ç‰‡æè¿°ï¼š\n{all_desc}"
                )
            else:
                final_prompt = (
                    f"è¯·æ€»ç»“è¿™ä¸ªç½‘é¡µçš„å†…å®¹ï¼š\n\n"
                    f"ğŸ“„ æ–‡æœ¬å†…å®¹ï¼š\n{main_text}"
                )

            # Step 5: ä¸»æ¨¡å‹ç”Ÿæˆæ€»ç»“
            dfd_system_prompt = (
                "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®æµå›¾(DFD)åˆ†æä¸“å®¶ã€‚ä½ çš„ä¸»è¦èŒè´£æ˜¯ä»ç½‘é¡µå†…å®¹ä¸­æå–å’Œåˆ†æä¸æ•°æ®æµå›¾ç»˜åˆ¶ç›¸å…³çš„çŸ¥è¯†ã€‚\n\n"
                "è¯·é‡ç‚¹å…³æ³¨ä»¥ä¸‹å†…å®¹ï¼š\n"
                "1. æ•°æ®æµå›¾çš„åŸºæœ¬æ¦‚å¿µã€å®šä¹‰å’Œä½œç”¨\n"
                "2. DFDçš„å››ä¸ªæ ¸å¿ƒå…ƒç´ ï¼šå¤–éƒ¨å®ä½“ã€å¤„ç†è¿‡ç¨‹ã€æ•°æ®å­˜å‚¨ã€æ•°æ®æµ\n"
                "3. DFDçš„ç»˜åˆ¶æ­¥éª¤ã€æ–¹æ³•å’ŒæŠ€å·§\n"
                "4. DFDçš„å±‚æ¬¡ç»“æ„ï¼ˆLevel 0ã€Level 1ç­‰ï¼‰\n"
                "5. DFDçš„ç¬¦å·è§„èŒƒå’Œå‘½åçº¦å®š\n"
                "6. DFDç»˜åˆ¶å·¥å…·å’Œè½¯ä»¶æ¨è\n"
                "7. å®é™…æ¡ˆä¾‹å’Œåº”ç”¨åœºæ™¯\n"
                "8. å¸¸è§é”™è¯¯å’Œæ³¨æ„äº‹é¡¹\n\n"
                "è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€æ€»ç»“å†…å®¹ï¼Œçªå‡ºDFDç›¸å…³çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹ã€‚"
            )
            
            final_response = openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": dfd_system_prompt},
                    {"role": "user", "content": final_prompt}
                ]
            )

            # æ–°å¢ï¼šæœ¬åœ° JSON/Markdown æ–‡ä»¶å­˜å‚¨
            # ============= æ–°å¢æœ¬åœ°å­˜å‚¨ =============
            import json as _json
            from datetime import datetime as _dt

            # 1. ç”Ÿæˆ tech_topicï¼ˆç”¨æ ‡é¢˜æˆ–é¦–ä¸ª headingï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
            def clean_filename(s):
                return re.sub(r'[^\w\u4e00-\u9fa5]+', '_', s).strip('_')
            tech_topic = title or (headings[0] if headings else "ç½‘é¡µå†…å®¹")
            tech_topic_clean = clean_filename(tech_topic)
            # 2. æ—¶é—´æˆ³
            crawl_time = _dt.now().strftime('%Y-%m-%dT%H-%M-%S')
            crawl_time_human = _dt.now().strftime('%Y-%m-%d %H:%M:%S')
            # 3. ç›®å½•
            json_dir = Path("shared_data/json_llm_ready")
            md_dir = Path("shared_data/markdown_llm_ready")
            json_dir.mkdir(parents=True, exist_ok=True)
            md_dir.mkdir(parents=True, exist_ok=True)
            # 4. æ–‡ä»¶å
            json_path = json_dir / f"{tech_topic_clean}_{crawl_time}.json"
            md_path = md_dir / f"{tech_topic_clean}_{crawl_time}.md"
            # 5. ç»„è£…å†…å®¹ï¼ˆçŸ¥è¯†åº“å¯¹æ¥æ ¼å¼ï¼‰
            # æ ¹æ®ç½‘é¡µå†…å®¹åˆ†æDFDç›¸å…³å…ƒç´ 
            try:
                if hasattr(final_response, 'choices') and final_response.choices:
                    content_analysis = final_response.choices[0].message.content or ""
                elif isinstance(final_response, str):
                    content_analysis = final_response
                else:
                    content_analysis = str(final_response)
            except Exception as e:
                content_analysis = f"[ERROR] å†…å®¹åˆ†æå¤±è´¥: {str(e)}"
                print(f"å¤„ç†final_responseæ—¶å‡ºé”™: {e}, final_responseç±»å‹: {type(final_response)}")
            
            # ä½¿ç”¨é…ç½®åŒ–çš„æ ¼å¼å¤„ç†å™¨æ„å»ºçŸ¥è¯†åº“æ•°æ®
            # ä½¿ç”¨å½“å‰é»˜è®¤æ ¼å¼ç±»å‹ï¼ˆå¯é…ç½®ï¼‰
            format_type = "dfd"  # å¯ä»¥ä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è·å–
            
            # æ ¹æ®é…ç½®æå–çŸ¥è¯†åº“æ•°æ®
            extracted_data = format_processor.extract_knowledge(
                content_analysis, url, title
            )
            
            # å‡†å¤‡å…ƒæ•°æ®
            metadata = {
                "source_url": url,
                "title": title,
                "crawl_time": crawl_time,
                "crawl_time_human": crawl_time_human,
                "extraction_method": "åŸºäºé…ç½®æ–‡ä»¶çš„è‡ªåŠ¨æå–",
                "topic": tech_topic
            }
            
            # ç”ŸæˆJSONç»“æ„
            json_obj = format_processor.generate_json_structure(
                extracted_data, url, title
            )
            
            # ä¸ºäº†å…¼å®¹æ€§ï¼Œæå–å„ä¸ªç»„ä»¶çš„ç»Ÿè®¡ä¿¡æ¯
            dfd_concepts = extracted_data.get('dfd_concepts', [])
            dfd_rules = extracted_data.get('dfd_rules', [])
            dfd_patterns = extracted_data.get('dfd_patterns', [])
            dfd_cases = extracted_data.get('dfd_cases', [])
            dfd_nlp_mappings = extracted_data.get('dfd_nlp_mappings', [])
            
            # ç¡®ä¿ç»Ÿè®¡ä¿¡æ¯æ­£ç¡®
            if "statistics" not in json_obj:
                json_obj["statistics"] = {
                    "concepts_count": len(dfd_concepts),
                    "rules_count": len(dfd_rules),
                    "patterns_count": len(dfd_patterns),
                    "cases_count": len(dfd_cases),
                    "nlp_mappings_count": len(dfd_nlp_mappings)
                }
            
            # ä½¿ç”¨FormatProcessorç”ŸæˆMarkdownå†…å®¹
            markdown_content = format_processor.generate_markdown(
                extracted_data,
                {
                    'title': tech_topic,
                    'source_url': url,
                    'source_title': title,
                    'crawl_time': crawl_time_human,
                    'content_analysis': content_analysis
                }
            )
            md_lines = markdown_content.split('\n')
            # å†™å…¥æ–‡ä»¶
            with open(json_path, 'w', encoding='utf-8') as f:
                _json.dump(json_obj, f, ensure_ascii=False, indent=2)
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(md_lines))
            
            # è‡ªåŠ¨ä¿å­˜åˆ°çŸ¥è¯†åº“ç»“æ„åŒ–æ–‡ä»¶
            try:
                kb_result = await save_to_knowledge_base(_json.dumps(json_obj), tech_topic_clean)
                # Knowledge base save result logged
            except Exception as e:
                # Knowledge base save failed, error logged
                pass
            
            # ============= æ–°å¢æœ¬åœ°å­˜å‚¨ END =============
            
            # è¿”å›å¢å¼ºçš„ç»“æœï¼ŒåŒ…å«çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
            try:
                # å°è¯•è§£æå“åº”å†…å®¹
                if hasattr(final_response, 'choices') and final_response.choices:
                    result_summary = final_response.choices[0].message.content or "[ERROR] ç†è§£å¤±è´¥"
                elif isinstance(final_response, str):
                    result_summary = final_response
                else:
                    result_summary = str(final_response)
            except Exception as e:
                result_summary = f"[ERROR] å“åº”è§£æå¤±è´¥: {str(e)}"
            
            # æ·»åŠ çŸ¥è¯†åº“æå–ç»Ÿè®¡
            kb_stats = f"\n\nğŸ“Š **çŸ¥è¯†åº“æå–ç»Ÿè®¡**:\n" \
                      f"- æ¦‚å¿µå®šä¹‰: {len(dfd_concepts)} ä¸ª\n" \
                      f"- è§„åˆ™æ¡ç›®: {len(dfd_rules)} ä¸ª\n" \
                      f"- æ¨¡å¼æ¨¡æ¿: {len(dfd_patterns)} ä¸ª\n" \
                      f"- æ¡ˆä¾‹ç¤ºä¾‹: {len(dfd_cases)} ä¸ª\n" \
                      f"- NLPæ˜ å°„: {len(dfd_nlp_mappings)} ä¸ª\n\n" \
                      f"ğŸ“ **æ–‡ä»¶ä¿å­˜ä½ç½®**:\n" \
                      f"- JSONæ•°æ®: {json_path}\n" \
                      f"- MarkdownæŠ¥å‘Š: {md_path}\n" \
                      f"- çŸ¥è¯†åº“æ–‡ä»¶: shared_data/knowledge_base/{tech_topic_clean}_*.json"
            
            return result_summary + kb_stats

    except Exception as e:
        return f"[ERROR] å›¾æ–‡æå–å¤±è´¥ {str(e)}"

@mcp.tool()
async def save_to_knowledge_base(json_data: str, base_filename: str = None) -> str:
    """
    å°†æå–çš„DFDçŸ¥è¯†åº“æ•°æ®ä¿å­˜åˆ°5ä¸ªç‹¬ç«‹çš„JSONæ–‡ä»¶ä¸­ï¼Œæ¨¡æ‹Ÿæ•°æ®åº“è¡¨ç»“æ„
    """
    try:
        import json as _json
        from datetime import datetime as _dt
        
        # è§£æè¾“å…¥çš„JSONæ•°æ®
        data = _json.loads(json_data) if isinstance(json_data, str) else json_data
        
        # ç”Ÿæˆæ–‡ä»¶åå‰ç¼€
        if not base_filename:
            timestamp = _dt.now().strftime('%Y%m%d_%H%M%S')
            base_filename = f"dfd_knowledge_{timestamp}"
        
        # åˆ›å»ºçŸ¥è¯†åº“ç›®å½•
        kb_dir = Path("shared_data/knowledge_base")
        kb_dir.mkdir(parents=True, exist_ok=True)
        
        saved_files = []
        
        # 1. ä¿å­˜ dfd_concepts
        if 'dfd_concepts' in data and data['dfd_concepts']:
            concepts_file = kb_dir / f"{base_filename}_concepts.json"
            concepts_data = {
                "table_name": "dfd_concepts",
                "description": "DFDå…ƒç´ å®šä¹‰è¡¨ï¼ˆæ¦‚å¿µåº“ï¼‰",
                "schema": {
                    "id": "text (ä¸»é”®)",
                    "type": "text (å…ƒç´ ç±»å‹)",
                    "description": "text (å…ƒç´ æè¿°)",
                    "symbol": "text (å›¾å½¢ç¬¦å·)",
                    "rules": "jsonb (å…ƒç´ è§„åˆ™æ•°ç»„)"
                },
                "data": data['dfd_concepts'],
                "metadata": data.get('metadata', {})
            }
            with open(concepts_file, 'w', encoding='utf-8') as f:
                _json.dump(concepts_data, f, ensure_ascii=False, indent=2)
            saved_files.append(str(concepts_file))
        
        # 2. ä¿å­˜ dfd_rules
        if 'dfd_rules' in data and data['dfd_rules']:
            rules_file = kb_dir / f"{base_filename}_rules.json"
            rules_data = {
                "table_name": "dfd_rules",
                "description": "DFDè§„åˆ™åº“ï¼ˆåˆ†ä¸ºå±‚æ¬¡è§„åˆ™ã€è¿æ¥è§„åˆ™ã€å‘½åè§„åˆ™ï¼‰",
                "schema": {
                    "id": "text (ä¸»é”®)",
                    "category": "text (è§„åˆ™åˆ†ç±»)",
                    "description": "text (è§„åˆ™è¯´æ˜)",
                    "condition": "text (æ¡ä»¶è¯­æ³•)",
                    "validation": "text (éªŒè¯è¡¨è¾¾å¼)"
                },
                "data": data['dfd_rules'],
                "metadata": data.get('metadata', {})
            }
            with open(rules_file, 'w', encoding='utf-8') as f:
                _json.dump(rules_data, f, ensure_ascii=False, indent=2)
            saved_files.append(str(rules_file))
        
        # 3. ä¿å­˜ dfd_patterns
        if 'dfd_patterns' in data and data['dfd_patterns']:
            patterns_file = kb_dir / f"{base_filename}_patterns.json"
            patterns_data = {
                "table_name": "dfd_patterns",
                "description": "DFDæ¨¡æ¿åº“ï¼ˆæ¨¡å¼åº“ï¼‰",
                "schema": {
                    "id": "serial (è‡ªå¢ID)",
                    "system": "text (ç³»ç»Ÿåç§°)",
                    "level": "int (DFDå±‚çº§)",
                    "processes": "jsonb (åŠ å·¥åˆ—è¡¨)",
                    "entities": "jsonb (å¤–éƒ¨å®ä½“åˆ—è¡¨)",
                    "data_stores": "jsonb (æ•°æ®å­˜å‚¨åˆ—è¡¨)",
                    "flows": "jsonb (æ•°æ®æµæ•°ç»„)"
                },
                "data": data['dfd_patterns'],
                "metadata": data.get('metadata', {})
            }
            with open(patterns_file, 'w', encoding='utf-8') as f:
                _json.dump(patterns_data, f, ensure_ascii=False, indent=2)
            saved_files.append(str(patterns_file))
        
        # 4. ä¿å­˜ dfd_cases
        if 'dfd_cases' in data and data['dfd_cases']:
            cases_file = kb_dir / f"{base_filename}_cases.json"
            cases_data = {
                "table_name": "dfd_cases",
                "description": "DFDé”™è¯¯/ç¤ºä¾‹æ¡ˆä¾‹åº“",
                "schema": {
                    "id": "text (æ¡ˆä¾‹ID)",
                    "type": "text (error_case æˆ– best_practice)",
                    "description": "text (æè¿°)",
                    "incorrect": "jsonb (é”™è¯¯ç»“æ„)",
                    "correct": "jsonb (æ­£ç¡®ç»“æ„)",
                    "explanation": "text (è¯´æ˜è§£é‡Š)"
                },
                "data": data['dfd_cases'],
                "metadata": data.get('metadata', {})
            }
            with open(cases_file, 'w', encoding='utf-8') as f:
                _json.dump(cases_data, f, ensure_ascii=False, indent=2)
            saved_files.append(str(cases_file))
        
        # 5. ä¿å­˜ dfd_nlp_mappings
        if 'dfd_nlp_mappings' in data and data['dfd_nlp_mappings']:
            nlp_file = kb_dir / f"{base_filename}_nlp_mappings.json"
            nlp_data = {
                "table_name": "dfd_nlp_mappings",
                "description": "è‡ªç„¶è¯­è¨€æ˜ å°„è§„åˆ™åº“",
                "schema": {
                    "id": "serial (è‡ªå¢ID)",
                    "pattern": "text (åŒ¹é…å¥å¼)",
                    "element_type": "text (æ˜ å°„å‡ºçš„DFDå…ƒç´ ç±»å‹)",
                    "name_template": "text (åç§°æ¨¡æ¿)",
                    "flow_template": "text (æ•°æ®æµæ¨¡æ¿)",
                    "action_mappings": "jsonb (åŠ¨ä½œè½¬æ¢)"
                },
                "data": data['dfd_nlp_mappings'],
                "metadata": data.get('metadata', {})
            }
            with open(nlp_file, 'w', encoding='utf-8') as f:
                _json.dump(nlp_data, f, ensure_ascii=False, indent=2)
            saved_files.append(str(nlp_file))
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary_file = kb_dir / f"{base_filename}_summary.json"
        summary_data = {
            "extraction_summary": {
                "source_url": data.get('metadata', {}).get('source_url', ''),
                "extraction_time": _dt.now().isoformat(),
                "total_files_saved": len(saved_files),
                "statistics": data.get('statistics', {})
            },
            "saved_files": saved_files,
            "knowledge_base_structure": {
                "dfd_concepts": "å…ƒç´ å®šä¹‰è¡¨ï¼ˆæ¦‚å¿µåº“ï¼‰",
                "dfd_rules": "è§„åˆ™åº“ï¼ˆå±‚æ¬¡ã€è¿æ¥ã€å‘½åè§„åˆ™ï¼‰",
                "dfd_patterns": "æ¨¡æ¿åº“ï¼ˆæ¨¡å¼åº“ï¼‰",
                "dfd_cases": "é”™è¯¯/ç¤ºä¾‹æ¡ˆä¾‹åº“",
                "dfd_nlp_mappings": "è‡ªç„¶è¯­è¨€æ˜ å°„è§„åˆ™åº“"
            }
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            _json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        return f"[SUCCESS] DFDçŸ¥è¯†åº“æ•°æ®å·²ä¿å­˜åˆ°5ä¸ªç‹¬ç«‹æ–‡ä»¶:\n" + "\n".join([f"- {file}" for file in saved_files]) + f"\n\næ±‡æ€»æŠ¥å‘Š: {summary_file}\n\nç»Ÿè®¡ä¿¡æ¯:\n- æ¦‚å¿µå®šä¹‰: {data.get('statistics', {}).get('concepts_count', 0)} ä¸ª\n- è§„åˆ™æ¡ç›®: {data.get('statistics', {}).get('rules_count', 0)} ä¸ª\n- æ¨¡å¼æ¨¡æ¿: {data.get('statistics', {}).get('patterns_count', 0)} ä¸ª\n- æ¡ˆä¾‹ç¤ºä¾‹: {data.get('statistics', {}).get('cases_count', 0)} ä¸ª\n- NLPæ˜ å°„: {data.get('statistics', {}).get('nlp_mappings_count', 0)} ä¸ª"
        
    except Exception as e:
        return f"[ERROR] ä¿å­˜çŸ¥è¯†åº“æ•°æ®å¤±è´¥: {str(e)}"

@mcp.tool()
async def search_and_scrape(keyword: str, top_k: int = 12) -> str:
    """
    æ ¹æ®å…³é”®è¯æœç´¢ç½‘é¡µï¼Œå¹¶æŠ“å–å‰å‡ ä¸ªç½‘é¡µçš„å›¾æ–‡ä¿¡æ¯ã€‚
    """
    try:
        # å°è¯•æœç´¢
        print(f"å¼€å§‹æœç´¢å…³é”®è¯: {keyword}")
        links = search_web(keyword, max_results=top_k)
        if not links:
            print("æœªæ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœ")
            return "âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç½‘é¡µå–µ~"

        print(f"æ‰¾åˆ° {len(links)} ä¸ªæœç´¢ç»“æœï¼Œå¼€å§‹å¤„ç†...")
        # æŠ“å–å†…å®¹
        summaries = []
        for i, url in enumerate(links):
            try:
                print(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1} ä¸ªé“¾æ¥: {url}")
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if i > 0:
                    await asyncio.sleep(1)
                
                summary = await scrape_webpage(url)
                summaries.append(f"ğŸ”— ç½‘é¡µ {i+1}: {url}\n{summary}\n")
                print(f"ç¬¬ {i+1} ä¸ªé“¾æ¥å¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"å¤„ç†ç½‘é¡µ {url} æ—¶å‡ºé”™: {str(e)}")
                summaries.append(f"ğŸ”— ç½‘é¡µ {i+1}: {url}\nâŒ å¤„ç†å¤±è´¥å–µ~ {str(e)}\n")

        if not summaries:
            return "âš ï¸ æ‰€æœ‰ç½‘é¡µå¤„ç†éƒ½å¤±è´¥äº†å–µ~"

        return "\n\n".join(summaries)
        
    except Exception as e:
        print(f"æœç´¢æˆ–æŠ“å–è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        return f"[ERROR] æœç´¢æˆ–æŠ“å–å¤±è´¥ {str(e)}"

if __name__ == "__main__":
    # åˆå§‹åŒ–æ•°æ®åº“
    # asyncio.run(init_db()) # åˆ é™¤æ‰€æœ‰æ•°æ®åº“ç›¸å…³çš„è°ƒç”¨å’Œé€»è¾‘
    
    # å¯åŠ¨MCPæœåŠ¡å™¨
    # MCP server starting...
    
    # å¦‚æœå¯ç”¨äº†Torï¼Œè‡ªåŠ¨å¯åŠ¨Torä»£ç†
    if USE_TOR and tor_manager:
        # Tor proxy enabled, auto-starting...
        success = tor_manager.start_tor()
        if success:
            # SUCCESS: Tor proxy auto-start successful
            pass
        else:
            # ERROR: Tor proxy auto-start failed, using normal network connection
            pass
    
    try:
        mcp.run(transport='stdio')
    finally:
        # ç¨‹åºé€€å‡ºæ—¶æ¸…ç†Torèµ„æº
        if USE_TOR and tor_manager:
            # Cleaning up Tor resources...
            tor_manager.cleanup()
    # åœ¨ç¨‹åºé€€å‡ºå‰å…³é—­æ•°æ®åº“è¿æ¥æ± 
    # asyncio.run(close_pool()) # åˆ é™¤æ‰€æœ‰æ•°æ®åº“ç›¸å…³çš„è°ƒç”¨å’Œé€»è¾‘