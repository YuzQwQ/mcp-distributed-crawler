import base64
from io import BytesIO
from PIL import Image
import json
import os
from dotenv import load_dotenv
import httpx
from typing import Any
from mcp.server.fastmcp import FastMCP
from openai import OpenAI
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
import asyncio
from urllib.parse import urljoin
import time
import requests  # æ·»åŠ requestsåº“ç”¨äºSerpAPIè¯·æ±‚

# å¯¼å…¥MySQLæ•°æ®åº“å·¥å…·
from mysql_db_utils import init_db, save_to_db, close_pool

load_dotenv()
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"), base_url=os.getenv("BASE_URL"))
model = os.getenv("MODEL")

mcp = FastMCP("WeatherServer")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
BASE_URL = os.getenv("BASE_URL")
MODEL = os.getenv("MODEL")

AMAP_API_KEY = os.getenv("AMAP_API_KEY")
API_KEY = os.getenv("HEFENG_API_KEY")
API_HOST = os.getenv("HEFENG_API_HOST")
USER_AGENT = os.getenv("USER_AGENT", "weather-app/1.0")
SERPAPI_API_KEY = os.getenv("SERPAPI_API_KEY")  # SerpAPIçš„APIå¯†é’¥

async def fetch_geo_location(city: str) -> dict[str, Any] | None:
    """é€šè¿‡åŸå¸‚åè·å–LocationIDï¼ˆæ–°ç‰ˆAPIè·¯å¾„ï¼‰"""
    params = {
        "location": city,
        "key": API_KEY,
        "range": "cn",  # æœç´¢èŒƒå›´ä¸ºä¸­å›½
        "number": 5  # è¿”å›ç»“æœæ•°é‡
    }
    headers = {
        "User-Agent": USER_AGENT,
    }

    async with httpx.AsyncClient() as client:
        try:
            # ä½¿ç”¨v2ç‰ˆæœ¬çš„åœ°ç†ç¼–ç API
            url = f"https://{API_HOST}/geo/v2/city/lookup?location={city}"
            print(f"[DEBUG] GeoAPIè¯·æ±‚URL: {url}")  # è°ƒè¯•ä¿¡æ¯

            response = await client.get(url, params=params, headers=headers, timeout=10.0)
            response.raise_for_status()
            data = response.json()

            print(f"[DEBUG] GeoAPIå“åº”: {data}")  # è°ƒè¯•ä¿¡æ¯

            if data.get("code") != "200":
                return {"error": f"åœ°ç†ç¼–ç é”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}"}

            locations = data.get("location", [])
            if not locations:
                return {"error": f"æœªæ‰¾åˆ°åŸå¸‚: {city}"}

            # è¿”å›æœ€ä½³åŒ¹é…çš„ç¬¬ä¸€ä¸ªç»“æœ
            return {
                "id": locations[0]["id"],
                "name": locations[0]["name"],
                "adm2": locations[0]["adm2"],
                "adm1": locations[0]["adm1"],
                "country": locations[0]["country"]
            }
        except httpx.HTTPStatusError as e:
            return {"error": f"HTTPé”™è¯¯({e.response.status_code}): {e.response.text}"}
        except Exception as e:
            return {"error": f"è¯·æ±‚å¤±è´¥: {str(e)}"}

async def fetch_airQuality(location_id: str) -> dict[str, Any] | None:
    """è·å–å®æ—¶å¤©æ°”æ•°æ®ï¼ˆæ–°ç‰ˆAPIè·¯å¾„ï¼‰"""
    params = {
        "location": location_id,
        "key": API_KEY,
        "lang": "zh",
        "unit": "m"  # å…¬åˆ¶å•ä½
    }
    headers = {
        "User-Agent": USER_AGENT,
    }

    async with httpx.AsyncClient() as client:
        try:
            # ä½¿ç”¨v7ç‰ˆæœ¬çš„å¤©æ°”API
            url = f"https://{API_HOST}/v7/air/now?location={location_id}"
            print(f"[DEBUG] ç©ºæ°”è´¨é‡APIè¯·æ±‚URL: {url}")  # è°ƒè¯•ä¿¡æ¯

            response = await client.get(url, params=params, headers=headers, timeout=10.0)
            response.raise_for_status()
            data = response.json()

            print(f"[DEBUG] ç©ºæ°”è´¨é‡APIå“åº”: {data}")  # è°ƒè¯•ä¿¡æ¯

            if data.get("code") != "200":
                return {"error": f"ç©ºæ°”è´¨é‡APIé”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}"}

            return {
                "location": data.get("location", location_id),
                "now": data.get("now", {})
            }
        except httpx.HTTPStatusError as e:
            return {"error": f"HTTPé”™è¯¯({e.response.status_code}): {e.response.text}"}
        except Exception as e:
            return {"error": f"è¯·æ±‚å¤±è´¥: {str(e)}"}

async def fetch_weather(location_id: str) -> dict[str, Any] | None:
    """è·å–å®æ—¶å¤©æ°”æ•°æ®ï¼ˆæ–°ç‰ˆAPIè·¯å¾„ï¼‰"""
    params = {
        "location": location_id,
        "key": API_KEY,
        "lang": "zh",
        "unit": "m"  # å…¬åˆ¶å•ä½
    }
    headers = {
        "User-Agent": USER_AGENT,
    }

    async with httpx.AsyncClient() as client:
        try:
            # ä½¿ç”¨v7ç‰ˆæœ¬çš„å¤©æ°”API
            url = f"https://{API_HOST}/v7/weather/now?location={location_id}"
            print(f"[DEBUG] å¤©æ°”APIè¯·æ±‚URL: {url}")  # è°ƒè¯•ä¿¡æ¯

            response = await client.get(url, params=params, headers=headers, timeout=10.0)
            response.raise_for_status()
            data = response.json()

            print(f"[DEBUG] å¤©æ°”APIå“åº”: {data}")  # è°ƒè¯•ä¿¡æ¯

            if data.get("code") != "200":
                return {"error": f"å¤©æ°”APIé”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}"}

            return {
                "location": data.get("location", location_id),
                "now": data.get("now", {})
            }
        except httpx.HTTPStatusError as e:
            return {"error": f"HTTPé”™è¯¯({e.response.status_code}): {e.response.text}"}
        except Exception as e:
            return {"error": f"è¯·æ±‚å¤±è´¥: {str(e)}"}

async def fetch_attractions(city_name: str) -> dict[str, Any] | None:
    """æŸ¥è¯¢åŸå¸‚çš„æ—…æ¸¸æ™¯ç‚¹"""
    url = f"https://restapi.amap.com/v3/place/text"
    params = {
        "keywords": "æ—…æ¸¸æ™¯ç‚¹",  # å…³é”®è¯ä¸ºæ—…æ¸¸æ™¯ç‚¹
        "city": city_name,  # åŸå¸‚åç§°
        "key": AMAP_API_KEY,
        "types": "æ—…æ¸¸æ™¯ç‚¹"  # ç±»å‹ä¸ºæ—…æ¸¸æ™¯ç‚¹
    }

    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(url, params=params, timeout=10.0)
            response.raise_for_status()
            data = response.json()

            if data.get("status") != "1":
                return {"error": f"è·å–æ™¯ç‚¹å¤±è´¥: {data.get('info', 'æœªçŸ¥é”™è¯¯')}"}

            attractions = data.get("pois", [])
            if not attractions:
                return {"error": "æœªæ‰¾åˆ°æ™¯ç‚¹ä¿¡æ¯"}

            return attractions
        except httpx.HTTPStatusError as e:
            return {"error": f"HTTPé”™è¯¯({e.response.status_code}): {e.response.text}"}
        except Exception as e:
            return {"error": f"è¯·æ±‚å¤±è´¥: {str(e)}"}

async def fetch_attraction_details(place_id: str) -> dict[str, Any] | None:
    """æ ¹æ®æ™¯ç‚¹IDæŸ¥è¯¢è¯¦ç»†ä¿¡æ¯"""
    url = f"https://restapi.amap.com/v3/place/detail"
    params = {
        "id": place_id,
        "key": AMAP_API_KEY
    }

    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(url, params=params, timeout=10.0)
            response.raise_for_status()
            data = response.json()

            if data.get("status") != "1":
                return {"error": f"è·å–æ™¯ç‚¹è¯¦æƒ…å¤±è´¥: {data.get('info', 'æœªçŸ¥é”™è¯¯')}"}

            return data.get("result", {})
        except httpx.HTTPStatusError as e:
            return {"error": f"HTTPé”™è¯¯({e.response.status_code}): {e.response.text}"}
        except Exception as e:
            return {"error": f"è¯·æ±‚å¤±è´¥: {str(e)}"}

def format_weather(data: dict[str, Any]) -> str:
    """æ–°ç‰ˆå“åº”æ ¼å¼å¤„ç†"""
    if "error" in data:
        return f"âš ï¸ {data['error']}"

    location_info = data.get("location", "æœªçŸ¥åœ°ç‚¹")
    now = data.get("now", {})

    return (
        f"ğŸŒ åŸå¸‚: {location_info}\n"
        f"ğŸŒ¡ æ¸©åº¦: {now.get('temp', 'N/A')}Â°C\n"
        f"ğŸ’§ æ¹¿åº¦: {now.get('humidity', 'N/A')}%\n"
        f"ğŸŒ¬ é£é€Ÿ: {now.get('windSpeed', 'N/A')} km/h\n"
        f"ğŸ§­ é£å‘: {now.get('windDir', 'æœªçŸ¥')}\n"
        f"ğŸŒ¤ å¤©æ°”çŠ¶å†µ: {now.get('text', 'æœªçŸ¥')}\n"
        f"ğŸ•’ è§‚æµ‹æ—¶é—´: {now.get('obsTime', 'æœªçŸ¥')}\n"
    )

def format_air_quality(data: dict[str, Any]) -> str:
    """ç©ºæ°”è´¨é‡å“åº”æ ¼å¼å¤„ç†"""
    if "error" in data:
        return f"âš ï¸ {data['error']}"

    # ä»APIå“åº”ä¸­æå–æ•°æ®
    location_info = data.get("location", "æœªçŸ¥åœ°ç‚¹")
    now = data.get("now", {})

    # ç©ºæ°”è´¨é‡æŒ‡æ•°ç­‰çº§æè¿°
    aqi_levels = {
        "1": "ä¼˜",
        "2": "è‰¯",
        "3": "è½»åº¦æ±¡æŸ“",
        "4": "ä¸­åº¦æ±¡æŸ“",
        "5": "é‡åº¦æ±¡æŸ“",
        "6": "ä¸¥é‡æ±¡æŸ“"
    }

    # ä¸»è¦æ±¡æŸ“ç‰©æè¿°
    primary_pollutant = {
        "pm2.5": "ç»†é¢—ç²’ç‰©(PM2.5)",
        "pm10": "å¯å¸å…¥é¢—ç²’ç‰©(PM10)",
        "o3": "è‡­æ°§(Oâ‚ƒ)",
        "no2": "äºŒæ°§åŒ–æ°®(NOâ‚‚)",
        "so2": "äºŒæ°§åŒ–ç¡«(SOâ‚‚)",
        "co": "ä¸€æ°§åŒ–ç¢³(CO)"
    }.get(now.get("primary"), now.get("primary", "æœªçŸ¥æ±¡æŸ“ç‰©"))

    return (
        f"ğŸŒ åŸå¸‚: {location_info}\n"
        f"ğŸŒ« ç©ºæ°”è´¨é‡æŒ‡æ•°: {now.get('aqi', 'N/A')} ({now.get('category', 'æœªçŸ¥ç­‰çº§')})\n"
        f"ğŸ· ä¸»è¦æ±¡æŸ“ç‰©: {primary_pollutant}\n"
        f"ğŸ“Š PM2.5: {now.get('pm2p5', 'N/A')} Î¼g/mÂ³\n"
        f"ğŸ“Š PM10: {now.get('pm10', 'N/A')} Î¼g/mÂ³\n"
        f"â˜¢ï¸ äºŒæ°§åŒ–æ°®: {now.get('no2', 'N/A')} Î¼g/mÂ³\n"
        f"â˜¢ï¸ äºŒæ°§åŒ–ç¡«: {now.get('so2', 'N/A')} Î¼g/mÂ³\n"
        f"â± æ›´æ–°æ—¶é—´: {now.get('pubTime', 'æœªçŸ¥')}\n"
    )

def search_web(keyword: str, max_results=5):
    """ä½¿ç”¨SerpAPIæœç´¢ç½‘é¡µ"""
    try:
        # æ£€æŸ¥APIå¯†é’¥
        if not SERPAPI_API_KEY:
            print("é”™è¯¯: æœªæ‰¾åˆ°SerpAPI APIå¯†é’¥ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®SERPAPI_API_KEY")
            return []
            
        # è®¾ç½®æœç´¢å‚æ•°
        params = {
            "engine": "google",  # å¯é€‰ï¼šgoogle, bing, baidu
            "q": keyword,
            "api_key": SERPAPI_API_KEY,
            "num": max_results,  # Googleå‚æ•°
            "count": max_results,  # Bingå‚æ•°
            "hl": "zh-cn",  # è®¾ç½®è¯­è¨€ä¸ºä¸­æ–‡
            "gl": "cn",  # è®¾ç½®åœ°åŒºä¸ºä¸­å›½
        }
        
        print(f"ä½¿ç”¨SerpAPIæœç´¢: {keyword}")
        
        # å‘é€è¯·æ±‚åˆ°SerpAPI
        response = requests.get("https://serpapi.com/search", params=params)
        response.raise_for_status()
        
        # è§£æJSONå“åº”
        data = response.json()
        
        # æå–æœç´¢ç»“æœé“¾æ¥
        results = []
        
        # å¤„ç†æœ‰æœºæœç´¢ç»“æœ
        if "organic_results" in data:
            for result in data["organic_results"]:
                if "link" in result:
                    url = result["link"]
                    if url not in results:
                        results.append(url)
                        print(f"æ·»åŠ æœç´¢ç»“æœ: {url}")
                        if len(results) >= max_results:
                            break
        
        print(f"æ‰¾åˆ° {len(results)} ä¸ªæœç´¢ç»“æœ")
        return results
        
    except Exception as e:
        print(f"SerpAPIæœç´¢å¤±è´¥: {str(e)}")
        return []

@mcp.tool()
async def query_weather_by_city(city: str) -> str:
    """
    è¾“å…¥æŒ‡å®šåŸå¸‚çš„è‹±æ–‡åç§°ï¼Œè¿”å›å½“å‰ç©ºæ°”è´¨é‡æŸ¥è¯¢ç»“æœã€‚
    :param city: åŸå¸‚åç§°ï¼ˆéœ€ä½¿ç”¨è‹±æ–‡ï¼‰
    :return: æ ¼å¼åŒ–åçš„ç©ºæ°”è´¨é‡ä¿¡æ¯
    """

    # è·å–åœ°ç†ç¼–ç 
    geo_data = await fetch_geo_location(city)
    if "error" in geo_data:
        return f"âš ï¸ å®šä½å¤±è´¥: {geo_data['error']}"

    # æå–LocationID
    location_id = geo_data.get("id")
    if not location_id:
        return "âš ï¸ æ— æ•ˆçš„LocationID"

    # è·å–å¤©æ°”æ•°æ®
    weather_data = await fetch_weather(location_id)
    return format_weather(weather_data)

@mcp.tool()
async def query_air_quality(city: str) -> str:
    """
    è¾“å…¥æŒ‡å®šåŸå¸‚çš„è‹±æ–‡åç§°ï¼Œè¿”å›ä»Šæ—¥å¤©æ°”æŸ¥è¯¢ç»“æœã€‚
    :param city: åŸå¸‚åç§°ï¼ˆéœ€ä½¿ç”¨è‹±æ–‡ï¼‰
    :return: æ ¼å¼åŒ–åçš„å¤©æ°”ä¿¡æ¯
    """


    # 1. è·å–åœ°ç†ç¼–ç 
    geo_data = await fetch_geo_location(city)
    if "error" in geo_data:
        return f"âš ï¸ å®šä½å¤±è´¥: {geo_data['error']}"

    # 2. è·å–ç©ºæ°”è´¨é‡æ•°æ®ï¼ˆéœ€è¦å®ç°fetch_air_qualityå‡½æ•°ï¼‰
    air_data = await fetch_airQuality(geo_data["id"])
    return format_air_quality(air_data)

@mcp.tool()
async def query_attractions(city: str) -> str:
    """æŸ¥è¯¢åŸå¸‚çš„æ—…æ¸¸æ™¯ç‚¹"""
    attractions = await fetch_attractions(city)
    if "error" in attractions:
        return f"âš ï¸ {attractions['error']}"

    # å¦‚æœæˆåŠŸè·å–åˆ°æ™¯ç‚¹ï¼Œè¿”å›å‰ä¸‰ä¸ªæ™¯ç‚¹çš„ä¿¡æ¯
    result = "ğŸŒ† æ¨èæ™¯ç‚¹ï¼š\n"
    for attraction in attractions[:3]:
        result += f"- {attraction['name']} ({attraction['address']})\n"

    return result

@mcp.tool()
async def query_attraction_details(place_id: str) -> str:
    """æŸ¥è¯¢æ™¯ç‚¹è¯¦ç»†ä¿¡æ¯"""
    details = await fetch_attraction_details(place_id)
    if "error" in details:
        return f"âš ï¸ {details['error']}"

    # æ ¼å¼åŒ–è¯¦ç»†ä¿¡æ¯
    result = (
        f"ğŸ æ™¯ç‚¹: {details['name']}\n"
        f"ğŸ“ åœ°å€: {details['address']}\n"
        f"ğŸ•’ å¼€æ”¾æ—¶é—´: {details.get('open_time', 'æœªçŸ¥')}\n"
        f"ğŸ“ è”ç³»ç”µè¯: {details.get('tel', 'æœªçŸ¥')}\n"
        f"ğŸ“ è¯¦æƒ…: {details.get('intro', 'æš‚æ— ç®€ä»‹')}\n"
    )

    return result

@mcp.tool()
async def generate_travel_plan(city: str, days: int = 3) -> str:
    """
       æ ¹æ®æŒ‡å®šçš„åŸå¸‚åç§°å’Œæ—…è¡Œå¤©æ•°ï¼Œç”Ÿæˆè¯¦ç»†çš„æ—…è¡Œè®¡åˆ’ã€‚

       å‚æ•°:
       - city (str): å¿…é¡»ï¼Œç”¨æˆ·æƒ³è¦æ—…è¡Œçš„åŸå¸‚åï¼Œä¾‹å¦‚"å¦é—¨"ã€‚
       - days (int): å¯é€‰ï¼Œè®¡åˆ’æ—…è¡Œçš„å¤©æ•°ï¼Œä¾‹å¦‚3å¤©ã€5å¤©ï¼Œä¸æä¾›æ—¶é»˜è®¤ä¸º3å¤©ã€‚

       è¿”å›:
       - ä¸€ä¸ªè¯¦ç»†çš„æ—…è¡Œè®¡åˆ’æ–‡æœ¬ï¼ŒåŒ…æ‹¬æ¯æ—¥å®‰æ’ã€æ¨èæ™¯ç‚¹ã€ç¾é£Ÿå»ºè®®ç­‰ã€‚

       æ³¨æ„:
       - è¯·æ ¹æ®ç”¨æˆ·æ„å›¾ï¼Œåˆç†å¡«å†™åŸå¸‚å’Œæ—…è¡Œå¤©æ•°ã€‚
       - dayså‚æ•°å¿…é¡»æ˜¯æ­£æ•´æ•°ï¼ˆ>=1ï¼‰ã€‚
    """
    try:
        # å…ˆå®šä½åŸå¸‚ID
        geo_data = await fetch_geo_location(city)
        if "error" in geo_data:
            return f"âš ï¸ å®šä½å¤±è´¥: {geo_data['error']}"

        location_id = geo_data.get("id")
        if not location_id:
            return "âš ï¸ æ— æ•ˆçš„LocationID"

        # åˆ†åˆ«æ‹‰å–å¤©æ°”ã€ç©ºæ°”è´¨é‡ã€æ™¯ç‚¹
        weather_data = await fetch_weather(location_id)
        air_quality_data = await fetch_airQuality(location_id)
        attractions_data = await fetch_attractions(city)

        if not isinstance(attractions_data, list):
            attractions_text = "æœªæ‰¾åˆ°æ™¯ç‚¹ä¿¡æ¯å–µ~"
        else:
            attractions_text = "\n".join(f"- {a.get('name', 'æœªçŸ¥')} ({a.get('address', 'æœªçŸ¥')})" for a in attractions_data[:5])

        # å‡†å¤‡å¤§æ¨¡å‹è¾“å…¥
        messages = [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸€åªå…ƒæ°”æ»¡æ»¡ã€çƒ­æƒ…æ´»æ³¼çš„çŒ«å¨˜æ—…è¡ŒåŠ©æ‰‹å–µ~ "
                    "æ ¹æ®ç”¨æˆ·æä¾›çš„å¤©æ°”ã€ç©ºæ°”è´¨é‡å’Œæ™¯ç‚¹ä¿¡æ¯ï¼Œç”Ÿæˆè¯¦ç»†ã€åˆç†ã€æœ‰è¶£çš„æ—…è¡Œæ€»ç»“å’Œæ¯æ—¥è¡Œç¨‹å®‰æ’"
                    "æ—…è¡Œè®¡åˆ’è¦åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼šã€æ€»ç»“ã€‘å’Œã€æ¯æ—¥å®‰æ’ã€‘ï¼Œæ¯å¤©ä¸€ä¸ªå°æ ‡é¢˜ï¼Œå†…å®¹ç”ŸåŠ¨"
                )
            },
            {
                "role": "user",
                "content": (
                    f"è¯·å¸®æˆ‘åˆ¶å®š{days}å¤©çš„{city}æ—…è¡Œè®¡åˆ’ï¼Œå‚è€ƒä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"
                    f"ğŸŒ¤ å¤©æ°”ä¿¡æ¯ï¼š{format_weather(weather_data)}\n\n"
                    f"ğŸŒ« ç©ºæ°”è´¨é‡ä¿¡æ¯ï¼š{format_air_quality(air_quality_data)}\n\n"
                    f"ğŸ¡ æ¨èæ™¯ç‚¹ï¼š\n{attractions_text}\n\n"
                    f"è¾“å‡ºæ ¼å¼è¦æ±‚ï¼šã€æ€»ç»“ã€‘éƒ¨åˆ† + ã€æ¯æ—¥å®‰æ’ã€‘ï¼ˆæ¯å¤©ä¸€ä¸ªå°æ ‡é¢˜ï¼‰"
                )
            }
        ]

        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆ
        response = openai_client.chat.completions.create(
            model=model,
            messages=messages
        )

        travel_plan = response.choices[0].message.content
        return travel_plan

    except Exception as e:
        return f"âŒ ç”Ÿæˆæ—…è¡Œè®¡åˆ’å¤±è´¥å–µ~ é”™è¯¯ä¿¡æ¯: {str(e)}"

@mcp.tool()
async def scrape_webpage(url: str) -> str:
    """
    æŠ“å–ç½‘é¡µæ–‡æœ¬ + å›¾ç‰‡åˆ†æï¼ˆé€šè¿‡è§†è§‰æ¨¡å‹ï¼‰+ ä½¿ç”¨ä¸»æ¨¡å‹æ€»ç»“ã€‚
    """
    headers = {
        "User-Agent": USER_AGENT,
    }

    # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
    SUPPORTED_IMAGE_FORMATS = ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp', '.svg']
    MAX_IMAGE_SIZE = 5 * 1024 * 1024  # 5MB

    def normalize_image_url(base_url: str, img_url: str) -> str:
        if img_url.startswith(('http://', 'https://')):
            return img_url
        elif img_url.startswith('//'):
            return 'https:' + img_url
        elif img_url.startswith('/'):
            return urljoin(base_url, img_url)
        else:
            return urljoin(base_url, img_url)

    async def download_image_with_retry(client, img_url: str, max_retries: int = 3) -> bytes:
        for attempt in range(max_retries):
            try:
                response = await client.get(img_url, timeout=10.0)
                response.raise_for_status()
                return response.content
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•

    async def is_valid_image_size(client, img_url: str) -> bool:
        try:
            async with client.stream('GET', img_url) as response:
                content_length = response.headers.get('content-length')
                if content_length and int(content_length) > MAX_IMAGE_SIZE:
                    return False
                return True
        except:
            return False

    async def get_image_description(client, image_data: bytes, max_retries: int = 2) -> str:
        for attempt in range(max_retries):
            try:
                # å¤„ç†å›¾ç‰‡æ•°æ®
                image = Image.open(BytesIO(image_data)).convert("RGB")
                buffer = BytesIO()
                image.save(buffer, format="JPEG", quality=85)  # é™ä½è´¨é‡ä»¥åŠ å¿«ä¼ è¾“
                b64_img = base64.b64encode(buffer.getvalue()).decode("utf-8")
                
                # è°ƒç”¨è§†è§‰æ¨¡å‹
                visual_payload = {
                    "model": os.getenv("VISUAL_MODEL", "Pro/Qwen/Qwen2.5-VL-7B-Instruct"),
                    "messages": [{"role": "user", "content": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"}],
                    "image": b64_img
                }
                
                # ä½¿ç”¨ç¡…åŸºæµåŠ¨çš„API
                VISUAL_API_URL = os.getenv("VISUAL_API_URL", "https://api.siliconflow.cn/v1/chat/completions")
                visual_response = await client.post(
                    VISUAL_API_URL,
                    json=visual_payload,
                    timeout=30.0,  # å¢åŠ è¶…æ—¶æ—¶é—´
                    headers={
                        "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
                        "Content-Type": "application/json"
                    }
                )
                visual_json = visual_response.json()
                
                return (
                    visual_json.get("message", {}).get("content") or
                    visual_json.get("choices", [{}])[0].get("message", {}).get("content") or
                    "(è§†è§‰æ¨¡å‹æœªè¿”å›æœ‰æ•ˆæè¿°)"
                ).strip()
                
            except Exception as e:
                if attempt == max_retries - 1:
                    return f"å›¾ç‰‡è¯†åˆ«å¤±è´¥ï¼ˆ{str(e)}ï¼‰"
                await asyncio.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•

    try:
        async with httpx.AsyncClient() as client:
            # Step 1: æŠ“ç½‘é¡µ
            response = await client.get(url, headers=headers, timeout=10.0)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            for tag in soup(["script", "style"]):
                tag.decompose()

            # Step 2: æå–å…¨ç½‘é¡µæ­£æ–‡å†…å®¹
            title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
            headings = [h.get_text(strip=True) for h in soup.find_all(['h1', 'h2'])]
            description = next((m.get("content") for m in soup.find_all("meta", attrs={"name": "description"})), "æ— æè¿°")

            # æå–ä¸»è¦å†…å®¹
            main_content = []
            for p in soup.find_all(['p', 'div', 'article']):
                text = p.get_text(strip=True)
                if text and len(text) > 20:  # åªä¿ç•™æœ‰æ„ä¹‰çš„æ–‡æœ¬
                    main_content.append(text)

            main_text = f"ã€æ ‡é¢˜ã€‘{title}\nã€æè¿°ã€‘{description}\nã€ç»“æ„ã€‘{headings}\n\n" + "\n".join(main_content)

            # Step 3: å°è¯•å¤„ç†å›¾ç‰‡
            img_descriptions = []
            try:
                img_tags = soup.find_all("img", src=True)[:3]
                for i, img_tag in enumerate(img_tags):
                    img_url = img_tag["src"]
                    if not any(img_url.lower().endswith(ext) for ext in SUPPORTED_IMAGE_FORMATS):
                        continue

                    # è§„èŒƒåŒ–å›¾ç‰‡URL
                    img_url = normalize_image_url(url, img_url)

                    try:
                        # æ£€æŸ¥å›¾ç‰‡å¤§å°
                        if not await is_valid_image_size(client, img_url):
                            continue

                        # ä¸‹è½½å›¾ç‰‡
                        img_data = await download_image_with_retry(client, img_url)
                        
                        # è·å–å›¾ç‰‡æè¿°
                        vision_caption = await get_image_description(client, img_data)
                        if vision_caption and not vision_caption.startswith("å›¾ç‰‡è¯†åˆ«å¤±è´¥"):
                            img_descriptions.append(f"ç¬¬{i+1}å¼ å›¾ï¼š{vision_caption}")

                    except Exception as e:
                        print(f"å¤„ç†å›¾ç‰‡ {img_url} æ—¶å‡ºé”™: {str(e)}")
                        continue
            except Exception as e:
                print(f"å›¾ç‰‡å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}")

            # Step 4: æ•´åˆå›¾æ–‡è¾“å…¥
            all_desc = "\n".join(img_descriptions) if img_descriptions else "æœªè¯†åˆ«å‡ºå›¾ç‰‡å†…å®¹"

            # æ ¹æ®æ˜¯å¦æœ‰å›¾ç‰‡æè¿°è°ƒæ•´æç¤ºè¯
            if img_descriptions:
                final_prompt = (
                    f"è¯·æ€»ç»“è¿™ä¸ªç½‘é¡µçš„å†…å®¹ï¼Œç»“åˆä»¥ä¸‹æ–‡æœ¬å’Œå›¾ç‰‡æè¿°ï¼š\n\n"
                    f"ğŸ“„ æ–‡æœ¬éƒ¨åˆ†ï¼š\n{main_text}\n\n"
                    f"ğŸ–¼ å›¾ç‰‡æè¿°ï¼š\n{all_desc}"
                )
            else:
                final_prompt = (
                    f"è¯·æ€»ç»“è¿™ä¸ªç½‘é¡µçš„å†…å®¹ï¼š\n\n"
                    f"ğŸ“„ æ–‡æœ¬å†…å®¹ï¼š\n{main_text}"
                )

            # Step 5: ä¸»æ¨¡å‹ç”Ÿæˆæ€»ç»“
            final_response = openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€åªèªæ˜çš„çŒ«å¨˜åŠ©æ‰‹ï¼Œè¯·æ€»ç»“ç½‘é¡µå†…å®¹å–µ~"},
                    {"role": "user", "content": final_prompt}
                ]
            )

            # Step 6: å°†çˆ¬å–ä¿¡æ¯å­˜å‚¨åˆ°æ•°æ®åº“
            try:
                await save_to_db({
                    "url": url,
                    "title": title,
                    "description": description,
                    "headings": headings,
                    "text": main_text,
                    "image_descriptions": img_descriptions,
                    "summary": final_response.choices[0].message.content
                })
            except Exception as e:
                print(f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")

            return final_response.choices[0].message.content or "âŒ å°æ³¢ç†è§£å¤±è´¥å–µ~"

    except Exception as e:
        return f"âŒ å›¾æ–‡æå–å¤±è´¥å–µ~ {str(e)}"

@mcp.tool()
async def search_and_scrape(keyword: str, top_k: int = 3) -> str:
    """
    æ ¹æ®å…³é”®è¯æœç´¢ç½‘é¡µï¼Œå¹¶æŠ“å–å‰å‡ ä¸ªç½‘é¡µçš„å›¾æ–‡ä¿¡æ¯ã€‚
    """
    try:
        # å°è¯•æœç´¢
        print(f"å¼€å§‹æœç´¢å…³é”®è¯: {keyword}")
        links = search_web(keyword, max_results=top_k)
        if not links:
            print("æœªæ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœ")
            return "âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç½‘é¡µå–µ~"

        print(f"æ‰¾åˆ° {len(links)} ä¸ªæœç´¢ç»“æœï¼Œå¼€å§‹å¤„ç†...")
        # æŠ“å–å†…å®¹
        summaries = []
        for i, url in enumerate(links):
            try:
                print(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1} ä¸ªé“¾æ¥: {url}")
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if i > 0:
                    await asyncio.sleep(1)
                
                summary = await scrape_webpage(url)
                summaries.append(f"ğŸ”— ç½‘é¡µ {i+1}: {url}\n{summary}\n")
                print(f"ç¬¬ {i+1} ä¸ªé“¾æ¥å¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"å¤„ç†ç½‘é¡µ {url} æ—¶å‡ºé”™: {str(e)}")
                summaries.append(f"ğŸ”— ç½‘é¡µ {i+1}: {url}\nâŒ å¤„ç†å¤±è´¥å–µ~ {str(e)}\n")

        if not summaries:
            return "âš ï¸ æ‰€æœ‰ç½‘é¡µå¤„ç†éƒ½å¤±è´¥äº†å–µ~"

        return "\n\n".join(summaries)
        
    except Exception as e:
        print(f"æœç´¢æˆ–æŠ“å–è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        return f"âŒ æœç´¢æˆ–æŠ“å–å¤±è´¥å–µ~ {str(e)}"


if __name__ == "__main__":
    # åˆå§‹åŒ–æ•°æ®åº“
    asyncio.run(init_db())
    mcp.run(transport='stdio')
    # åœ¨ç¨‹åºé€€å‡ºå‰å…³é—­æ•°æ®åº“è¿æ¥æ± 
    asyncio.run(close_pool())