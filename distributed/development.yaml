crawler:
  delay_between_requests: 0.5
  follow_redirects: true
  max_concurrent_requests: 10
  max_depth: 3
  max_redirects: 5
  request_timeout: 30
  respect_robots_txt: true
  retry_attempts: 3
  retry_delay: 1
  user_agent: DistributedCrawler/1.0
database:
  connection_timeout: 30
  database: crawler_dev
  host: localhost
  max_connections: 20
  min_connections: 5
  password: dev_password
  pool_recycle: 3600
  port: 5432
  username: crawler
environment: development
logging:
  backup_count: 5
  console_output: true
  file_path: logs/crawler.log
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  json_format: false
  level: DEBUG
  max_file_size: 104857600
monitoring:
  alert_email: null
  alert_webhook_url: null
  enabled: true
  health_check_path: /health
  log_slow_queries: true
  metrics_path: /metrics
  metrics_port: 8080
  retention_days: 7
  slow_query_threshold: 1000
redis:
  db: 0
  health_check_interval: 30
  host: localhost
  max_connections: 100
  password: null
  port: 6379
  retry_on_timeout: true
  socket_connect_timeout: 5
  socket_timeout: 5
scheduler:
  enable_priority_queue: true
  health_check_interval: 30
  load_balancing_enabled: true
  max_task_per_worker: 100
  priority_levels: 5
  scheduling_algorithm: round_robin
  task_timeout: 3600
security:
  api_key: null
  api_key_required: false
  cors_origins: null
  enable_cors: false
  enable_rate_limiting: true
  rate_limit_per_minute: 60
  ssl_cert_path: null
  ssl_enabled: false
  ssl_key_path: null
worker:
  auto_restart: true
  graceful_shutdown_timeout: 30
  heartbeat_interval: 10
  max_cpu_usage: 80
  max_memory_usage: 80
  max_workers: 2
  restart_on_memory_leak: true
  worker_timeout: 300
