#!/usr/bin/env python3
"""
ç®€å•çš„äººæ€§åŒ–è®¿é—®æ§åˆ¶æµ‹è¯•
"""
import asyncio
import logging
import time
from distributed.access_controller import AccessController, GentleCrawlerMixin
from distributed.worker_node import BaseCrawler

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

class TestCrawler(BaseCrawler):
    """æµ‹è¯•çˆ¬è™«ç±»"""
    
    def __init__(self, task_id: str):
        super().__init__(task_id)
        self.access_controller = AccessController()
    
    async def crawl(self, task):
        """æµ‹è¯•çˆ¬å–æ–¹æ³•"""
        url = task.get("url", "https://example.com")
        
        # ä½¿ç”¨äººæ€§åŒ–è®¿é—®æ§åˆ¶
        await self.access_controller.wait_before_request(url)
        
        return {
            "url": url,
            "status": "success",
            "timestamp": time.time()
        }

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ äººæ€§åŒ–è®¿é—®æ§åˆ¶æµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    # æµ‹è¯•è®¿é—®æ§åˆ¶å™¨
    controller = AccessController()
    
    print("\nğŸ§ª æµ‹è¯•åŸŸåæå–:")
    test_urls = [
        "https://example.com/page1",
        "https://google.com/search",
        "https://github.com/user/repo"
    ]
    
    for url in test_urls:
        domain = controller._extract_domain(url)
        print(f"  {url} -> {domain}")
    
    print("\nğŸ§ª æµ‹è¯•å»¶è¿Ÿè®¡ç®—:")
    for url in test_urls:
        domain = controller._extract_domain(url)
        delay = controller._calculate_delay(domain, url)
        print(f"  {domain} -> å»¶è¿Ÿ: {delay:.2f}ç§’")
    
    print("\nğŸ§ª æµ‹è¯•çˆ¬è™«é›†æˆ:")
    crawler = TestCrawler("test_task")
    
    test_tasks = [
        {"url": "https://example.com/api1"},
        {"url": "https://google.com/search"},
        {"url": "https://github.com/repo"}
    ]
    
    start_time = time.time()
    for i, task in enumerate(test_tasks):
        task_start = time.time()
        print(f"\nä»»åŠ¡ {i+1}: {task['url']}")
        
        result = await crawler.crawl(task)
        elapsed = time.time() - task_start
        
        print(f"  âœ… å®Œæˆ! è€—æ—¶: {elapsed:.2f}ç§’")
    
    total_time = time.time() - start_time
    print(f"\næ€»è€—æ—¶: {total_time:.2f}ç§’")
    
    print("\n" + "=" * 50)
    print("âœ… æµ‹è¯•å®Œæˆ!")
    print("\näººæ€§åŒ–è®¿é—®æ§åˆ¶å·²å¯ç”¨:")
    print("  â€¢ æ™ºèƒ½å»¶è¿Ÿ: 1-3ç§’éšæœºå»¶è¿Ÿ")
    print("  â€¢ åŸŸåæ„ŸçŸ¥: ä¸åŒåŸŸåç‹¬ç«‹æ§åˆ¶")
    print("  â€¢ é˜²è¿‡è½½: é˜²æ­¢å¯¹å•ä¸€åŸŸåè¿‡åº¦è¯·æ±‚")

if __name__ == "__main__":
    asyncio.run(main())