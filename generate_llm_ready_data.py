#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”ŸæˆDFDå¤„ç†è¿‡ç¨‹å‘½åè§„èŒƒçš„LLMå°±ç»ªæ•°æ®ï¼ˆç¬¬ä¸‰é˜¶æ®µï¼‰
"""

import json
import os
from datetime import datetime
from pathlib import Path

def load_knowledge_base_files(base_filename):
    """åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶"""
    kb_dir = Path("shared_data/knowledge_base")
    
    # å®šä¹‰æ–‡ä»¶ç±»å‹
    file_types = ['concepts', 'rules', 'patterns', 'cases', 'nlp_mappings']
    
    data = {}
    metadata = None
    
    for file_type in file_types:
        file_path = kb_dir / f"{base_filename}_{file_type}.json"
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                content = json.load(f)
                data[f"dfd_{file_type}"] = content.get('data', [])
                if metadata is None:
                    metadata = content.get('metadata', {})
    
    return data, metadata

def generate_llm_ready_json(data, metadata, base_filename):
    """ç”ŸæˆLLMå°±ç»ªçš„JSONæ•°æ®"""
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    statistics = {}
    for key, value in data.items():
        if key.startswith('dfd_'):
            stat_key = key.replace('dfd_', '') + '_count'
            statistics[stat_key] = len(value) if isinstance(value, list) else 0
    
    # æ„å»ºLLMå°±ç»ªæ•°æ®ç»“æ„
    llm_ready_data = {
        "metadata": metadata,
        "statistics": statistics
    }
    
    # æ·»åŠ æ‰€æœ‰æ•°æ®ç±»åˆ«
    llm_ready_data.update(data)
    
    return llm_ready_data

def save_llm_ready_data(data, base_filename):
    """ä¿å­˜LLMå°±ç»ªæ•°æ®"""
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("shared_data/json_llm_ready")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime('%Y-%m-%dT%H-%M-%S')
    filename = f"{base_filename}_{timestamp}.json"
    file_path = output_dir / filename
    
    # ä¿å­˜æ–‡ä»¶
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return str(file_path)

def main():
    try:
        base_filename = "DFD_å¤„ç†è¿‡ç¨‹å‘½åè§„èŒƒ"
        
        print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆ {base_filename} çš„LLMå°±ç»ªæ•°æ®...")
        
        # åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶
        data, metadata = load_knowledge_base_files(base_filename)
        
        if not data:
            print("âŒ æœªæ‰¾åˆ°çŸ¥è¯†åº“æ–‡ä»¶")
            return
        
        print(f"âœ… æˆåŠŸåŠ è½½çŸ¥è¯†åº“æ•°æ®")
        print(f"ğŸ“‹ æ•°æ®ç±»åˆ«: {list(data.keys())}")
        
        # ç”ŸæˆLLMå°±ç»ªæ•°æ®
        llm_ready_data = generate_llm_ready_json(data, metadata, base_filename)
        
        print(f"âœ… æˆåŠŸç”ŸæˆLLMå°±ç»ªæ•°æ®ç»“æ„")
        
        # ä¿å­˜LLMå°±ç»ªæ•°æ®
        saved_path = save_llm_ready_data(llm_ready_data, base_filename)
        
        print(f"âœ… LLMå°±ç»ªæ•°æ®å·²ä¿å­˜åˆ°: {saved_path}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        for key, value in llm_ready_data['statistics'].items():
            category_name = key.replace('_count', '')
            print(f"  - {category_name}: {value} ä¸ª")
        
        print("\nğŸ‰ ç¬¬ä¸‰é˜¶æ®µLLMå°±ç»ªæ•°æ®ç”Ÿæˆå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()